{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edd0fe7a-31af-4ea9-89da-184176ba93fa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-06-02 16:47:03.317607: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-02 16:47:04.385214: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-06-02 16:47:04.385343: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-06-02 16:47:04.385355: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import BertPreTrainedModel, BertModel\n",
    "from transformers import AutoConfig, AutoTokenizer\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm, trange\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ec39f10-9c8b-49d4-a7df-7724f98e765f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_data = pd.read_csv('../data/forauto.csv')\n",
    "alltags = all_data[['Alltags','label']]\n",
    "train = alltags[:-180581]\n",
    "test = alltags[-180581:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6dad7aeb-2dd5-48b5-af3b-89feda34c3d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test = test.drop(['label'], axis=1)\n",
    "test = test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64d2dfc5-0444-4c3e-90d4-211fe1c19dff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data, validation = train_test_split(train, test_size=0.25, random_state=2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f082f220-0bd2-429c-8819-1ecdc13f045c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_OUT_DIR = '/bertmodels/bert_regressor'\n",
    "## Model Configurations\n",
    "MAX_LEN_TRAIN = 205\n",
    "MAX_LEN_VALID = 205\n",
    "MAX_LEN_TEST = 205\n",
    "BATCH_SIZE = 64\n",
    "LR = 4e-5\n",
    "NUM_EPOCHS = 5\n",
    "NUM_THREADS = 1  ## Number of threads for collecting dataset\n",
    "MODEL_NAME = 'bert-base-uncased'\n",
    "\n",
    "if not os.path.isdir(MODEL_OUT_DIR):\n",
    "    os.makedirs(MODEL_OUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d349cab8-c8e2-406f-8258-f6c3a001550a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Excerpt_Dataset(Dataset):\n",
    "\n",
    "    def __init__(self, data, maxlen, tokenizer): \n",
    "        #Store the contents of the file in a pandas dataframe\n",
    "        self.df = data.reset_index()\n",
    "        #Initialize the tokenizer for the desired transformer model\n",
    "        self.tokenizer = tokenizer\n",
    "        #Maximum length of the tokens list to keep all the sequences of fixed size\n",
    "        self.maxlen = maxlen\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):    \n",
    "        #Select the sentence and label at the specified index in the data frame\n",
    "        excerpt = self.df.loc[index, 'Alltags']\n",
    "        try:\n",
    "            target = self.df.loc[index, 'label']\n",
    "        except:\n",
    "            target = 0.0\n",
    "        # identifier = self.df.loc[index, 'id']\n",
    "        #Preprocess the text to be suitable for the transformer\n",
    "        tokens = self.tokenizer.tokenize(excerpt) \n",
    "        tokens = ['[CLS]'] + tokens + ['[SEP]'] \n",
    "        if len(tokens) < self.maxlen:\n",
    "            tokens = tokens + ['[PAD]' for _ in range(self.maxlen - len(tokens))] \n",
    "        else:\n",
    "            tokens = tokens[:self.maxlen-1] + ['[SEP]'] \n",
    "        #Obtain the indices of the tokens in the BERT Vocabulary\n",
    "        input_ids = self.tokenizer.convert_tokens_to_ids(tokens) \n",
    "        input_ids = torch.tensor(input_ids) \n",
    "        #Obtain the attention mask i.e a tensor containing 1s for no padded tokens and 0s for padded ones\n",
    "        attention_mask = (input_ids != 0).long()\n",
    "        \n",
    "        target = torch.tensor(target, dtype=torch.float32)\n",
    "        \n",
    "        return input_ids, attention_mask, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffb96e1f-23f5-4828-9daf-0efa977d302f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BertRegresser(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.bert = BertModel(config)\n",
    "        #The output layer that takes the [CLS] representation and gives an output\n",
    "        self.cls_layer1 = nn.Linear(config.hidden_size,128)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.ff1 = nn.Linear(128,128)\n",
    "        self.tanh1 = nn.Tanh()\n",
    "        self.ff2 = nn.Linear(128,1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        #Feed the input to Bert model to obtain contextualized representations\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        #Obtain the representations of [CLS] heads\n",
    "        logits = outputs.last_hidden_state[:,0,:]\n",
    "        output = self.cls_layer1(logits)\n",
    "        output = self.relu1(output)\n",
    "        output = self.ff1(output)\n",
    "        output = self.tanh1(output)\n",
    "        output = self.ff2(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4906dcc9-89ff-44ea-95a6-413275541b0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(model, criterion, optimizer, train_loader, val_loader, epochs, device):\n",
    "    best_acc = 0\n",
    "    for epoch in trange(epochs, desc=\"Epoch\"):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for i, (input_ids, attention_mask, target) in enumerate(iterable=train_loader):\n",
    "            optimizer.zero_grad()  \n",
    "            \n",
    "            input_ids, attention_mask, target = input_ids.to(device), attention_mask.to(device), target.to(device)\n",
    "            \n",
    "            output = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            \n",
    "            loss = criterion(output, target.type_as(output))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        print(f\"Training loss is {train_loss/len(train_loader)}\")\n",
    "        val_loss = evaluate(model=model, criterion=criterion, dataloader=val_loader, device=device)\n",
    "        print(\"Epoch {} complete! Validation Loss : {}\".format(epoch, val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "685ea2ac-9a04-4143-adf2-8a3195cdee0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate(model, criterion, dataloader, device):\n",
    "    model.eval()\n",
    "    mean_acc, mean_loss, count = 0, 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input_ids, attention_mask, target in (dataloader):\n",
    "            \n",
    "            input_ids, attention_mask, target = input_ids.to(device), attention_mask.to(device), target.to(device)\n",
    "            output = model(input_ids, attention_mask)\n",
    "            \n",
    "            mean_loss += criterion(output, target.type_as(output)).item()\n",
    "#             mean_err += get_rmse(output, target)\n",
    "            count += 1\n",
    "            \n",
    "    return mean_loss/count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff4d6199-29b8-4325-af75-80989cd4c3aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_rmse(output, target):\n",
    "    err = torch.sqrt(metrics.mean_squared_error(target, output))\n",
    "    return err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39f507f0-9745-42b7-85cb-83724f012113",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict(model, dataloader, device):\n",
    "    predicted_label = []\n",
    "    actual_label = []\n",
    "    with torch.no_grad():\n",
    "        for input_ids, attention_mask, target in (dataloader):\n",
    "            \n",
    "            input_ids, attention_mask, target = input_ids.to(device), attention_mask.to(device), target.to(device)\n",
    "            output = model(input_ids, attention_mask)\n",
    "                        \n",
    "            predicted_label += output\n",
    "            actual_label += target\n",
    "            \n",
    "    return predicted_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b08cff8-1ceb-4872-b044-f4e1c313d512",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)/main/tokenizer.json: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 466k/466k [00:00<00:00, 706kB/s]\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertRegresser: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertRegresser from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertRegresser from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertRegresser were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['ff1.weight', 'cls_layer1.bias', 'ff2.bias', 'ff1.bias', 'ff2.weight', 'cls_layer1.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "## Configuration loaded from AutoConfig \n",
    "config = AutoConfig.from_pretrained(MODEL_NAME)\n",
    "## Tokenizer loaded from AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "## Creating the model from the desired transformer model\n",
    "model = BertRegresser.from_pretrained(MODEL_NAME, config=config)\n",
    "## GPU or CPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "## Putting model to device\n",
    "model = model.to(device)\n",
    "## Takes as the input the logits of the positive class and computes the binary cross-entropy \n",
    "# criterion = nn.BCEWithLogitsLoss()\n",
    "criterion = nn.MSELoss()\n",
    "## Optimizer\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf12fabe-2aba-4e15-91ec-ff380b164dca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Training Dataset\n",
    "train_set = Excerpt_Dataset(data=train_data, maxlen=MAX_LEN_TRAIN, tokenizer=tokenizer)\n",
    "valid_set = Excerpt_Dataset(data=validation, maxlen=MAX_LEN_VALID, tokenizer=tokenizer)\n",
    "test_set = Excerpt_Dataset(data=test, maxlen=MAX_LEN_TEST, tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "## Data Loaders\n",
    "train_loader = DataLoader(dataset=train_set, batch_size=BATCH_SIZE, num_workers=NUM_THREADS)\n",
    "valid_loader = DataLoader(dataset=valid_set, batch_size=BATCH_SIZE, num_workers=NUM_THREADS)\n",
    "test_loader = DataLoader(dataset=test_set, batch_size=BATCH_SIZE, num_workers=NUM_THREADS)\n",
    "\n",
    "# print(len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eefdf37f-72bc-4d83-847a-dc7d713edef8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|                                                                                                                                                                  | 0/5 [00:00<?, ?it/s]/usr/local/lib/python3.8/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (566 > 512). Running this sequence through the model will result in indexing errors\n",
      "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([25])) that is different to the input size (torch.Size([25, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss is 7.134079149753139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (566 > 512). Running this sequence through the model will result in indexing errors\n",
      "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([52])) that is different to the input size (torch.Size([52, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "Epoch:  20%|██████████████████████████████                                                                                                                        | 1/5 [32:43<2:10:54, 1963.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 complete! Validation Loss : 6.118054050097314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (566 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss is 6.116273149639984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (566 > 512). Running this sequence through the model will result in indexing errors\n",
      "Epoch:  40%|███████████████████████████████████████████████████████████▏                                                                                        | 2/5 [1:05:56<1:39:02, 1980.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 complete! Validation Loss : 6.118256978054142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (566 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss is 6.116362091817382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (566 > 512). Running this sequence through the model will result in indexing errors\n",
      "Epoch:  60%|████████████████████████████████████████████████████████████████████████████████████████▊                                                           | 3/5 [1:38:43<1:05:48, 1974.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 complete! Validation Loss : 6.118283386605868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (566 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss is 6.116357835759536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (566 > 512). Running this sequence through the model will result in indexing errors\n",
      "Epoch:  80%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                              | 4/5 [2:11:51<32:59, 1979.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 complete! Validation Loss : 6.118293620034478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (566 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss is 6.116354460825407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (566 > 512). Running this sequence through the model will result in indexing errors\n",
      "Epoch: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [2:44:54<00:00, 1978.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 complete! Validation Loss : 6.118297070913778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train(model=model, \n",
    "      criterion=criterion,\n",
    "      optimizer=optimizer, \n",
    "      train_loader=train_loader,\n",
    "      val_loader=valid_loader,\n",
    "      epochs = 5,\n",
    "     device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "335eef80-4a69-4da6-9466-0ddb1d66eb45",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (618 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "output = predict(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ff7c298f-44d8-47a7-b60f-3d02cff4c72d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1 output.unique()                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2 </span>                                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">AttributeError: </span><span style=\"color: #008000; text-decoration-color: #008000\">'list'</span> object has no attribute <span style=\"color: #008000; text-decoration-color: #008000\">'unique'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m1\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1 output.unique()                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2 \u001b[0m                                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mAttributeError: \u001b[0m\u001b[32m'list'\u001b[0m object has no attribute \u001b[32m'unique'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# output.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccca3305-a07c-4d91-9e6c-fa45b22c42e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea755175-70c1-476a-97c8-b6f5c3287e8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcf6e91-27d1-49d0-b1e8-70edc1957117",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14668554-5aad-4179-a14a-8bd5e07e65dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "## 直接读取原始数据，并且保留原来的文本和图像，并把tag和title拼接\n",
    "import random\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgbm\n",
    "from datetime import datetime\n",
    "from time import gmtime, strftime\n",
    "from scipy import stats\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import torch\n",
    "import lightgbm\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.metrics import f1_score, mean_absolute_error, mean_squared_error\n",
    "random_seed = 2020\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7b18d15e-7d18-4dbe-b3f4-e9aa7970fb3d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Uid</th>\n",
       "      <th>Uid_count</th>\n",
       "      <th>Category</th>\n",
       "      <th>Subcategory</th>\n",
       "      <th>Concept</th>\n",
       "      <th>Title_len</th>\n",
       "      <th>Title_number</th>\n",
       "      <th>Alltags_len</th>\n",
       "      <th>Alltags_number</th>\n",
       "      <th>img_length</th>\n",
       "      <th>...</th>\n",
       "      <th>title_fe_290</th>\n",
       "      <th>title_fe_291</th>\n",
       "      <th>title_fe_292</th>\n",
       "      <th>title_fe_293</th>\n",
       "      <th>title_fe_294</th>\n",
       "      <th>title_fe_295</th>\n",
       "      <th>title_fe_296</th>\n",
       "      <th>title_fe_297</th>\n",
       "      <th>title_fe_298</th>\n",
       "      <th>title_fe_299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21894</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>65</td>\n",
       "      <td>75</td>\n",
       "      <td>33</td>\n",
       "      <td>6</td>\n",
       "      <td>128</td>\n",
       "      <td>12</td>\n",
       "      <td>333</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.055784</td>\n",
       "      <td>0.197610</td>\n",
       "      <td>-0.166620</td>\n",
       "      <td>-0.007064</td>\n",
       "      <td>0.308736</td>\n",
       "      <td>0.138135</td>\n",
       "      <td>0.149598</td>\n",
       "      <td>0.184107</td>\n",
       "      <td>-0.049617</td>\n",
       "      <td>0.011684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53866</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>75</td>\n",
       "      <td>139</td>\n",
       "      <td>56</td>\n",
       "      <td>13</td>\n",
       "      <td>800</td>\n",
       "      <td>65</td>\n",
       "      <td>500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.296695</td>\n",
       "      <td>0.087271</td>\n",
       "      <td>0.126468</td>\n",
       "      <td>-0.103056</td>\n",
       "      <td>0.147320</td>\n",
       "      <td>0.193802</td>\n",
       "      <td>0.035001</td>\n",
       "      <td>-0.045986</td>\n",
       "      <td>0.080715</td>\n",
       "      <td>0.078193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26948</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>42</td>\n",
       "      <td>480</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>188</td>\n",
       "      <td>23</td>\n",
       "      <td>500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.168270</td>\n",
       "      <td>0.003270</td>\n",
       "      <td>0.171345</td>\n",
       "      <td>0.108478</td>\n",
       "      <td>0.349160</td>\n",
       "      <td>-0.396890</td>\n",
       "      <td>-0.205695</td>\n",
       "      <td>0.068778</td>\n",
       "      <td>-0.128545</td>\n",
       "      <td>-0.080683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>355</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>68</td>\n",
       "      <td>225</td>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "      <td>61</td>\n",
       "      <td>9</td>\n",
       "      <td>500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.109710</td>\n",
       "      <td>0.117009</td>\n",
       "      <td>0.245160</td>\n",
       "      <td>-0.306995</td>\n",
       "      <td>0.387836</td>\n",
       "      <td>0.111045</td>\n",
       "      <td>0.157754</td>\n",
       "      <td>-0.086720</td>\n",
       "      <td>0.220020</td>\n",
       "      <td>-0.045470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>315</td>\n",
       "      <td>31</td>\n",
       "      <td>2</td>\n",
       "      <td>43</td>\n",
       "      <td>317</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>146</td>\n",
       "      <td>19</td>\n",
       "      <td>500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486189</th>\n",
       "      <td>46088</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>301</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>78</td>\n",
       "      <td>13</td>\n",
       "      <td>500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486190</th>\n",
       "      <td>12280</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>110</td>\n",
       "      <td>25</td>\n",
       "      <td>4</td>\n",
       "      <td>164</td>\n",
       "      <td>23</td>\n",
       "      <td>500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.134555</td>\n",
       "      <td>0.186670</td>\n",
       "      <td>0.330580</td>\n",
       "      <td>0.227405</td>\n",
       "      <td>0.155600</td>\n",
       "      <td>-0.088244</td>\n",
       "      <td>0.144885</td>\n",
       "      <td>-0.199950</td>\n",
       "      <td>0.033038</td>\n",
       "      <td>-0.065841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486191</th>\n",
       "      <td>43496</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>28</td>\n",
       "      <td>186</td>\n",
       "      <td>41</td>\n",
       "      <td>7</td>\n",
       "      <td>85</td>\n",
       "      <td>9</td>\n",
       "      <td>500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.156818</td>\n",
       "      <td>-0.107772</td>\n",
       "      <td>0.157098</td>\n",
       "      <td>-0.079490</td>\n",
       "      <td>0.042594</td>\n",
       "      <td>-0.034590</td>\n",
       "      <td>-0.349488</td>\n",
       "      <td>0.016242</td>\n",
       "      <td>-0.172004</td>\n",
       "      <td>-0.012735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486192</th>\n",
       "      <td>5492</td>\n",
       "      <td>17</td>\n",
       "      <td>7</td>\n",
       "      <td>27</td>\n",
       "      <td>54</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>305</td>\n",
       "      <td>35</td>\n",
       "      <td>500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.142362</td>\n",
       "      <td>-0.097873</td>\n",
       "      <td>-0.026855</td>\n",
       "      <td>0.195713</td>\n",
       "      <td>-0.101540</td>\n",
       "      <td>0.240027</td>\n",
       "      <td>-0.119417</td>\n",
       "      <td>-0.012023</td>\n",
       "      <td>0.021850</td>\n",
       "      <td>-0.081305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486193</th>\n",
       "      <td>17523</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>351</td>\n",
       "      <td>21</td>\n",
       "      <td>4</td>\n",
       "      <td>29</td>\n",
       "      <td>4</td>\n",
       "      <td>500</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.340258</td>\n",
       "      <td>0.249900</td>\n",
       "      <td>-0.260667</td>\n",
       "      <td>0.126590</td>\n",
       "      <td>0.352807</td>\n",
       "      <td>-0.505550</td>\n",
       "      <td>0.091707</td>\n",
       "      <td>-0.000797</td>\n",
       "      <td>-0.240234</td>\n",
       "      <td>0.046307</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>486194 rows × 688 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Uid  Uid_count  Category  Subcategory  Concept  Title_len  \\\n",
       "0       21894          4         5           65       75         33   \n",
       "1       53866         13         0           75      139         56   \n",
       "2       26948          1         3           42      480         12   \n",
       "3         355          1        10           68      225         18   \n",
       "4         315         31         2           43      317          8   \n",
       "...       ...        ...       ...          ...      ...        ...   \n",
       "486189  46088          2         0           35      301          6   \n",
       "486190  12280          7         0           60      110         25   \n",
       "486191  43496          9         4           28      186         41   \n",
       "486192   5492         17         7           27       54         15   \n",
       "486193  17523          4         7            4      351         21   \n",
       "\n",
       "        Title_number  Alltags_len  Alltags_number  img_length  ...  \\\n",
       "0                  6          128              12         333  ...   \n",
       "1                 13          800              65         500  ...   \n",
       "2                  2          188              23         500  ...   \n",
       "3                  3           61               9         500  ...   \n",
       "4                  1          146              19         500  ...   \n",
       "...              ...          ...             ...         ...  ...   \n",
       "486189             1           78              13         500  ...   \n",
       "486190             4          164              23         500  ...   \n",
       "486191             7           85               9         500  ...   \n",
       "486192             3          305              35         500  ...   \n",
       "486193             4           29               4         500  ...   \n",
       "\n",
       "        title_fe_290  title_fe_291  title_fe_292  title_fe_293  title_fe_294  \\\n",
       "0          -0.055784      0.197610     -0.166620     -0.007064      0.308736   \n",
       "1           0.296695      0.087271      0.126468     -0.103056      0.147320   \n",
       "2           0.168270      0.003270      0.171345      0.108478      0.349160   \n",
       "3           0.109710      0.117009      0.245160     -0.306995      0.387836   \n",
       "4           0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "...              ...           ...           ...           ...           ...   \n",
       "486189      0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "486190      0.134555      0.186670      0.330580      0.227405      0.155600   \n",
       "486191      0.156818     -0.107772      0.157098     -0.079490      0.042594   \n",
       "486192      0.142362     -0.097873     -0.026855      0.195713     -0.101540   \n",
       "486193     -0.340258      0.249900     -0.260667      0.126590      0.352807   \n",
       "\n",
       "        title_fe_295  title_fe_296  title_fe_297  title_fe_298  title_fe_299  \n",
       "0           0.138135      0.149598      0.184107     -0.049617      0.011684  \n",
       "1           0.193802      0.035001     -0.045986      0.080715      0.078193  \n",
       "2          -0.396890     -0.205695      0.068778     -0.128545     -0.080683  \n",
       "3           0.111045      0.157754     -0.086720      0.220020     -0.045470  \n",
       "4           0.000000      0.000000      0.000000      0.000000      0.000000  \n",
       "...              ...           ...           ...           ...           ...  \n",
       "486189      0.000000      0.000000      0.000000      0.000000      0.000000  \n",
       "486190     -0.088244      0.144885     -0.199950      0.033038     -0.065841  \n",
       "486191     -0.034590     -0.349488      0.016242     -0.172004     -0.012735  \n",
       "486192      0.240027     -0.119417     -0.012023      0.021850     -0.081305  \n",
       "486193     -0.505550      0.091707     -0.000797     -0.240234      0.046307  \n",
       "\n",
       "[486194 rows x 688 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all_data = pd.read_csv('../data/feature_data_530.csv')\n",
    "all_data = pd.read_csv('../data/forauto.csv')\n",
    "# train_all_data = all_data[:-180581]\n",
    "# submit_all_data = all_data[-180581:]\n",
    "# all_data\n",
    "# glove\n",
    "glove_tags = pd.read_csv('../data/alltags_feature.csv')\n",
    "glove_title = pd.read_csv('../data/title_feature.csv')\n",
    "vilt_flickr = pd.read_csv('../data/vilt_fea_flickr.csv')\n",
    "all_data = pd.concat([all_data, glove_tags, glove_title,vilt_flickr], axis=1)\n",
    "\n",
    "# feature_columns = ['img_file']\n",
    "# feature_columns = ['Pid', 'train_type', 'mean_label','img_file'] \n",
    "# feature_columns += ['user_fe_{}'.format(i) for i in range(399)]\n",
    "# feature_columns += ['loc_fe_{}'.format(i) for i in range(400)]\n",
    "all_data = all_data.drop(feature_columns, axis=1)\n",
    "train_all_data = all_data[:-180581]\n",
    "submit_all_data = all_data[-180581:]\n",
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "040cf3e2-f579-42d6-9acd-a63ad3ee184b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('float', [])        : 646 | ['svd_mode_t_0', 'svd_mode_t_1', 'svd_mode_t_2', 'svd_mode_t_3', 'svd_mode_t_4', ...]\n",
      "('int', [])          :  37 | ['Uid', 'Uid_count', 'Category', 'Subcategory', 'Concept', ...]\n",
      "('object', [])       :   3 | ['Category.1', 'Concept.1', 'Subcategory.1']\n",
      "('object', ['text']) :   2 | ['Alltags', 'Title']\n"
     ]
    }
   ],
   "source": [
    "label = 'label'\n",
    "from autogluon.tabular import FeatureMetadata\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "feature_metadata = FeatureMetadata.from_df(train_all_data)\n",
    "\n",
    "print(feature_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e1f49375-b01d-4a67-978e-338100a0ea5a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NN_TORCH': {},\n",
       " 'GBM': [{},\n",
       "  {'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}},\n",
       "  'GBMLarge'],\n",
       " 'CAT': {},\n",
       " 'XGB': {},\n",
       " 'AG_AUTOMM': {},\n",
       " 'VW': {}}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "excluded_model_types = ['GBM','CAT','VW','NN_TORCH']\n",
    "from autogluon.tabular.configs.hyperparameter_configs import get_hyperparameter_config\n",
    "hyperparameters = get_hyperparameter_config('multimodal')\n",
    "\n",
    "hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4ae01e53-c8d8-471c-9329-7099689d5e3f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20230527_063009/\"\n",
      "Warning: Training may take a very long time because `time_limit` was not specified and `train_data` is large (305613 samples, 1832.81 MB).\n",
      "\tConsider setting `time_limit` to ensure training finishes within an expected duration or experiment with a small portion of `train_data` to identify an ideal `presets` and `hyperparameters` configuration.\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20230527_063009/\"\n",
      "AutoGluon Version:  0.7.0\n",
      "Python Version:     3.8.10\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #78~20.04.1-Ubuntu SMP Wed Apr 19 11:26:48 UTC 2023\n",
      "Train Data Rows:    305613\n",
      "Train Data Columns: 687\n",
      "Label Column: label\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (16.56, 1.0, 6.40552, 2.47301)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    109811.72 MB\n",
      "\tTrain Data (Original)  Memory Usage: 1830.37 MB (1.7% of available memory)\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 4 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\t\tFitting RenameFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\t\tFitting TextSpecialFeatureGenerator...\n",
      "\t\t\tFitting BinnedFeatureGenerator...\n",
      "\t\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\t\tFitting TextNgramFeatureGenerator...\n",
      "\t\t\tFitting CountVectorizer for text features: ['Alltags', 'Title']\n",
      "\t\t\tCountVectorizer fit with vocabulary size = 10000\n",
      "\t\tWarning: Due to memory constraints, ngram feature count is being reduced. Allocate more memory to maximize model quality.\n",
      "\t\tReducing Vectorizer vocab size from 10000 to 2379 to avoid OOM error\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])        : 645 | ['svd_mode_t_0', 'svd_mode_t_1', 'svd_mode_t_2', 'svd_mode_t_3', 'svd_mode_t_4', ...]\n",
      "\t\t('int', [])          :  37 | ['Uid', 'Uid_count', 'Category', 'Subcategory', 'Concept', ...]\n",
      "\t\t('object', [])       :   3 | ['Category.1', 'Concept.1', 'Subcategory.1']\n",
      "\t\t('object', ['text']) :   2 | ['Alltags', 'Title']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])                    :    3 | ['Category.1', 'Concept.1', 'Subcategory.1']\n",
      "\t\t('category', ['text_as_category'])  :    2 | ['Alltags', 'Title']\n",
      "\t\t('float', [])                       :  645 | ['svd_mode_t_0', 'svd_mode_t_1', 'svd_mode_t_2', 'svd_mode_t_3', 'svd_mode_t_4', ...]\n",
      "\t\t('int', [])                         :   33 | ['Uid', 'Uid_count', 'Category', 'Subcategory', 'Concept', ...]\n",
      "\t\t('int', ['binned', 'text_special']) :   37 | ['Alltags.char_count', 'Alltags.word_count', 'Alltags.lower_ratio', 'Alltags.digit_ratio', 'Alltags.special_ratio', ...]\n",
      "\t\t('int', ['bool'])                   :    4 | ['img_model', 'Mediatype', 'ispro', 'Ispublic']\n",
      "\t\t('int', ['text_ngram'])             : 2380 | ['__nlp__.01', '__nlp__.02', '__nlp__.06', '__nlp__.07', '__nlp__.09', ...]\n",
      "\t\t('object', ['text'])                :    2 | ['Alltags_raw_text', 'Title_raw_text']\n",
      "\t523.3s = Fit runtime\n",
      "\t687 features in original data used to generate 3106 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 3229.69 MB (2.9% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 577.48s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Excluded Model Types: ['GBM']\n",
      "\tFound 'GBM' model in hyperparameters, but 'GBM' is present in `excluded_model_types` and will be removed.\n",
      "\tFound 'GBM' model in hyperparameters, but 'GBM' is present in `excluded_model_types` and will be removed.\n",
      "\tFound 'GBM' model in hyperparameters, but 'GBM' is present in `excluded_model_types` and will be removed.\n",
      "Fitting 5 L1 models ...\n",
      "Fitting model: CatBoost_BAG_L1 ...\n",
      "\tMemory not enough to fit CatBoostModel folds in parallel. Will do sequential fitting instead. \tConsider decreasing folds trained in parallel by passing num_folds_parallel to ag_args_ensemble when calling predictor.fit\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "Warning: less than 75% gpu memory available for training. Free: 903.1875 Total: 24259.6875\n",
      "Warning: less than 75% gpu memory available for training. Free: 903.1875 Total: 24259.6875\n",
      "\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "Warning: less than 75% gpu memory available for training. Free: 901.1875 Total: 24259.6875\n",
      "Warning: less than 75% gpu memory available for training. Free: 901.1875 Total: 24259.6875\n",
      "\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "Warning: less than 75% gpu memory available for training. Free: 901.1875 Total: 24259.6875\n",
      "Warning: less than 75% gpu memory available for training. Free: 901.1875 Total: 24259.6875\n",
      "\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "Warning: less than 75% gpu memory available for training. Free: 901.1875 Total: 24259.6875\n",
      "Warning: less than 75% gpu memory available for training. Free: 901.1875 Total: 24259.6875\n",
      "\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "Warning: less than 75% gpu memory available for training. Free: 901.1875 Total: 24259.6875\n",
      "Warning: less than 75% gpu memory available for training. Free: 901.1875 Total: 24259.6875\n",
      "\t-0.9663\t = Validation score   (-root_mean_squared_error)\n",
      "\t1886.83s\t = Training   runtime\n",
      "\t12.14s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ...\n",
      "\tMemory not enough to fit XGBoostModel folds in parallel. Will do sequential fitting instead. \tConsider decreasing folds trained in parallel by passing num_folds_parallel to ag_args_ensemble when calling predictor.fit\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "\t-0.9135\t = Validation score   (-root_mean_squared_error)\n",
      "\t3165.02s\t = Training   runtime\n",
      "\t58.72s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ...\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-1.0856\t = Validation score   (-root_mean_squared_error)\n",
      "\t1576.04s\t = Training   runtime\n",
      "\t15.06s\t = Validation runtime\n",
      "Fitting model: VowpalWabbit_BAG_L1 ...\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-1.5133\t = Validation score   (-root_mean_squared_error)\n",
      "\t4108.31s\t = Training   runtime\n",
      "\t1009.13s\t = Validation runtime\n",
      "Fitting model: MultiModalPredictor_BAG_L1 ...\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "INFO:lightning_fabric.utilities.seed:Global seed set to 0\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--electra-base-discriminator/snapshots/1b48ef100dac4676d84125a8a7b7ab7c51e00386/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"google/electra-base-discriminator\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--electra-base-discriminator/snapshots/1b48ef100dac4676d84125a8a7b7ab7c51e00386/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"google/electra-base-discriminator\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--google--electra-base-discriminator/snapshots/1b48ef100dac4676d84125a8a7b7ab7c51e00386/pytorch_model.bin\n",
      "Some weights of the model checkpoint at google/electra-base-discriminator were not used when initializing ElectraModel: ['discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of ElectraModel were initialized from the model checkpoint at google/electra-base-discriminator.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use ElectraModel for predictions without further training.\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--electra-base-discriminator/snapshots/1b48ef100dac4676d84125a8a7b7ab7c51e00386/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"google/electra-base-discriminator\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--google--electra-base-discriminator/snapshots/1b48ef100dac4676d84125a8a7b7ab7c51e00386/vocab.txt\n",
      "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--google--electra-base-discriminator/snapshots/1b48ef100dac4676d84125a8a7b7ab7c51e00386/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--google--electra-base-discriminator/snapshots/1b48ef100dac4676d84125a8a7b7ab7c51e00386/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--electra-base-discriminator/snapshots/1b48ef100dac4676d84125a8a7b7ab7c51e00386/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"google/electra-base-discriminator\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--electra-base-discriminator/snapshots/1b48ef100dac4676d84125a8a7b7ab7c51e00386/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"google/electra-base-discriminator\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--google--electra-base-discriminator/snapshots/1b48ef100dac4676d84125a8a7b7ab7c51e00386/vocab.txt\n",
      "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--google--electra-base-discriminator/snapshots/1b48ef100dac4676d84125a8a7b7ab7c51e00386/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--google--electra-base-discriminator/snapshots/1b48ef100dac4676d84125a8a7b7ab7c51e00386/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--electra-base-discriminator/snapshots/1b48ef100dac4676d84125a8a7b7ab7c51e00386/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"google/electra-base-discriminator\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "/usr/local/lib/python3.8/dist-packages/autogluon/multimodal/utils/environment.py:58: UserWarning: Interactive environment is detected. Currently, MultiModalPredictor does not support multi-gpu training under an interactive environment due to the limitation of ddp / ddp_spawn strategies in PT Lightning. Thus, we switch to single gpu training. For multi-gpu training, you need to execute MultiModalPredictor in a script.\n",
      "  warnings.warn(\n",
      "Configuration saved in /MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F1/automm_model/hf_text/config.json\n",
      "tokenizer config file saved in /MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F1/automm_model/hf_text/tokenizer_config.json\n",
      "Special tokens file saved in /MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F1/automm_model/hf_text/special_tokens_map.json\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Using 16bit None Automatic Mixed Precision (AMP)\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [2,3,4]\n",
      "INFO:pytorch_lightning.callbacks.model_summary:\n",
      "  | Name              | Type                | Params\n",
      "----------------------------------------------------------\n",
      "0 | model             | MultimodalFusionMLP | 109 M \n",
      "1 | validation_metric | MeanSquaredError    | 0     \n",
      "2 | loss_func         | MSELoss             | 0     \n",
      "----------------------------------------------------------\n",
      "109 M     Trainable params\n",
      "0         Non-trainable params\n",
      "109 M     Total params\n",
      "219.742   Total estimated model params size (MB)\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Epoch 0, global step 955: 'val_root_mean_squared_error' reached 0.62836 (best 0.62836), saving model to '/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F1/automm_model/epoch=0-step=955.ckpt' as top 3\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Epoch 0, global step 1911: 'val_root_mean_squared_error' reached 0.53122 (best 0.53122), saving model to '/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F1/automm_model/epoch=0-step=1911.ckpt' as top 3\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Epoch 1, global step 2866: 'val_root_mean_squared_error' reached 0.50328 (best 0.50328), saving model to '/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F1/automm_model/epoch=1-step=2866.ckpt' as top 3\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Epoch 1, global step 3822: 'val_root_mean_squared_error' reached 0.47121 (best 0.47121), saving model to '/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F1/automm_model/epoch=1-step=3822.ckpt' as top 3\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Epoch 2, global step 4777: 'val_root_mean_squared_error' reached 0.45560 (best 0.45560), saving model to '/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F1/automm_model/epoch=2-step=4777.ckpt' as top 3\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Epoch 2, global step 5733: 'val_root_mean_squared_error' reached 0.45797 (best 0.45560), saving model to '/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F1/automm_model/epoch=2-step=5733.ckpt' as top 3\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Epoch 3, global step 6688: 'val_root_mean_squared_error' reached 0.46334 (best 0.45560), saving model to '/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F1/automm_model/epoch=3-step=6688.ckpt' as top 3\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Epoch 3, global step 7644: 'val_root_mean_squared_error' reached 0.44323 (best 0.44323), saving model to '/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F1/automm_model/epoch=3-step=7644.ckpt' as top 3\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Epoch 4, global step 8599: 'val_root_mean_squared_error' reached 0.43619 (best 0.43619), saving model to '/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F1/automm_model/epoch=4-step=8599.ckpt' as top 3\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Epoch 4, global step 9555: 'val_root_mean_squared_error' reached 0.45233 (best 0.43619), saving model to '/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F1/automm_model/epoch=4-step=9555.ckpt' as top 3\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Epoch 5, global step 10510: 'val_root_mean_squared_error' reached 0.43218 (best 0.43218), saving model to '/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F1/automm_model/epoch=5-step=10510.ckpt' as top 3\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Epoch 5, global step 11466: 'val_root_mean_squared_error' reached 0.41856 (best 0.41856), saving model to '/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F1/automm_model/epoch=5-step=11466.ckpt' as top 3\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Epoch 6, global step 12421: 'val_root_mean_squared_error' reached 0.42071 (best 0.41856), saving model to '/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F1/automm_model/epoch=6-step=12421.ckpt' as top 3\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Epoch 6, global step 13377: 'val_root_mean_squared_error' reached 0.42391 (best 0.41856), saving model to '/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F1/automm_model/epoch=6-step=13377.ckpt' as top 3\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Epoch 7, global step 14332: 'val_root_mean_squared_error' reached 0.42018 (best 0.41856), saving model to '/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F1/automm_model/epoch=7-step=14332.ckpt' as top 3\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Epoch 7, global step 15288: 'val_root_mean_squared_error' was not in top 3\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Epoch 8, global step 16243: 'val_root_mean_squared_error' reached 0.41840 (best 0.41840), saving model to '/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F1/automm_model/epoch=8-step=16243.ckpt' as top 3\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Epoch 8, global step 17199: 'val_root_mean_squared_error' reached 0.41698 (best 0.41698), saving model to '/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F1/automm_model/epoch=8-step=17199.ckpt' as top 3\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Epoch 9, global step 18154: 'val_root_mean_squared_error' reached 0.41708 (best 0.41698), saving model to '/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F1/automm_model/epoch=9-step=18154.ckpt' as top 3\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Epoch 9, global step 19110: 'val_root_mean_squared_error' reached 0.41755 (best 0.41698), saving model to '/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F1/automm_model/epoch=9-step=19110.ckpt' as top 3\n",
      "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=10` reached.\n",
      "INFO:pytorch_lightning.utilities.rank_zero:You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "INFO:pytorch_lightning.utilities.rank_zero:You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "INFO:pytorch_lightning.utilities.rank_zero:You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "INFO:pytorch_lightning.utilities.rank_zero:You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Configuration saved in AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F1/automm_model/hf_text/config.json\n",
      "tokenizer config file saved in AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F1/automm_model/hf_text/tokenizer_config.json\n",
      "Special tokens file saved in AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F1/automm_model/hf_text/special_tokens_map.json\n",
      "INFO:lightning_fabric.utilities.seed:Global seed set to 0\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--electra-base-discriminator/snapshots/1b48ef100dac4676d84125a8a7b7ab7c51e00386/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"google/electra-base-discriminator\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--electra-base-discriminator/snapshots/1b48ef100dac4676d84125a8a7b7ab7c51e00386/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"google/electra-base-discriminator\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--google--electra-base-discriminator/snapshots/1b48ef100dac4676d84125a8a7b7ab7c51e00386/pytorch_model.bin\n",
      "Some weights of the model checkpoint at google/electra-base-discriminator were not used when initializing ElectraModel: ['discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of ElectraModel were initialized from the model checkpoint at google/electra-base-discriminator.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use ElectraModel for predictions without further training.\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--electra-base-discriminator/snapshots/1b48ef100dac4676d84125a8a7b7ab7c51e00386/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"google/electra-base-discriminator\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--google--electra-base-discriminator/snapshots/1b48ef100dac4676d84125a8a7b7ab7c51e00386/vocab.txt\n",
      "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--google--electra-base-discriminator/snapshots/1b48ef100dac4676d84125a8a7b7ab7c51e00386/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--google--electra-base-discriminator/snapshots/1b48ef100dac4676d84125a8a7b7ab7c51e00386/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--electra-base-discriminator/snapshots/1b48ef100dac4676d84125a8a7b7ab7c51e00386/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"google/electra-base-discriminator\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--electra-base-discriminator/snapshots/1b48ef100dac4676d84125a8a7b7ab7c51e00386/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"google/electra-base-discriminator\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--google--electra-base-discriminator/snapshots/1b48ef100dac4676d84125a8a7b7ab7c51e00386/vocab.txt\n",
      "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--google--electra-base-discriminator/snapshots/1b48ef100dac4676d84125a8a7b7ab7c51e00386/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--google--electra-base-discriminator/snapshots/1b48ef100dac4676d84125a8a7b7ab7c51e00386/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--electra-base-discriminator/snapshots/1b48ef100dac4676d84125a8a7b7ab7c51e00386/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"google/electra-base-discriminator\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "/usr/local/lib/python3.8/dist-packages/autogluon/multimodal/utils/environment.py:58: UserWarning: Interactive environment is detected. Currently, MultiModalPredictor does not support multi-gpu training under an interactive environment due to the limitation of ddp / ddp_spawn strategies in PT Lightning. Thus, we switch to single gpu training. For multi-gpu training, you need to execute MultiModalPredictor in a script.\n",
      "  warnings.warn(\n",
      "Configuration saved in /MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F2/automm_model/hf_text/config.json\n",
      "tokenizer config file saved in /MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F2/automm_model/hf_text/tokenizer_config.json\n",
      "Special tokens file saved in /MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F2/automm_model/hf_text/special_tokens_map.json\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Using 16bit None Automatic Mixed Precision (AMP)\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [2,3,4]\n",
      "INFO:pytorch_lightning.callbacks.model_summary:\n",
      "  | Name              | Type                | Params\n",
      "----------------------------------------------------------\n",
      "0 | model             | MultimodalFusionMLP | 109 M \n",
      "1 | validation_metric | MeanSquaredError    | 0     \n",
      "2 | loss_func         | MSELoss             | 0     \n",
      "----------------------------------------------------------\n",
      "109 M     Trainable params\n",
      "0         Non-trainable params\n",
      "109 M     Total params\n",
      "219.742   Total estimated model params size (MB)\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Epoch 0, global step 955: 'val_root_mean_squared_error' reached 0.63016 (best 0.63016), saving model to '/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F2/automm_model/epoch=0-step=955.ckpt' as top 3\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Epoch 0, global step 1911: 'val_root_mean_squared_error' reached 0.61521 (best 0.61521), saving model to '/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F2/automm_model/epoch=0-step=1911.ckpt' as top 3\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Epoch 1, global step 2866: 'val_root_mean_squared_error' reached 0.49417 (best 0.49417), saving model to '/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F2/automm_model/epoch=1-step=2866.ckpt' as top 3\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Epoch 1, global step 3822: 'val_root_mean_squared_error' reached 0.47233 (best 0.47233), saving model to '/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F2/automm_model/epoch=1-step=3822.ckpt' as top 3\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Epoch 2, global step 4777: 'val_root_mean_squared_error' reached 0.46879 (best 0.46879), saving model to '/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F2/automm_model/epoch=2-step=4777.ckpt' as top 3\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Epoch 2, global step 5733: 'val_root_mean_squared_error' reached 0.43997 (best 0.43997), saving model to '/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F2/automm_model/epoch=2-step=5733.ckpt' as top 3\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Epoch 3, global step 6688: 'val_root_mean_squared_error' reached 0.45487 (best 0.43997), saving model to '/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F2/automm_model/epoch=3-step=6688.ckpt' as top 3\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Epoch 3, global step 7644: 'val_root_mean_squared_error' reached 0.44044 (best 0.43997), saving model to '/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F2/automm_model/epoch=3-step=7644.ckpt' as top 3\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Epoch 4, global step 8599: 'val_root_mean_squared_error' reached 0.44635 (best 0.43997), saving model to '/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F2/automm_model/epoch=4-step=8599.ckpt' as top 3\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Epoch 4, global step 9555: 'val_root_mean_squared_error' reached 0.42895 (best 0.42895), saving model to '/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F2/automm_model/epoch=4-step=9555.ckpt' as top 3\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Epoch 5, global step 10510: 'val_root_mean_squared_error' reached 0.43370 (best 0.42895), saving model to '/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F2/automm_model/epoch=5-step=10510.ckpt' as top 3\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Epoch 5, global step 11466: 'val_root_mean_squared_error' reached 0.42607 (best 0.42607), saving model to '/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F2/automm_model/epoch=5-step=11466.ckpt' as top 3\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Epoch 6, global step 12421: 'val_root_mean_squared_error' reached 0.42690 (best 0.42607), saving model to '/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F2/automm_model/epoch=6-step=12421.ckpt' as top 3\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Epoch 6, global step 13377: 'val_root_mean_squared_error' reached 0.42889 (best 0.42607), saving model to '/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F2/automm_model/epoch=6-step=13377.ckpt' as top 3\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Epoch 7, global step 14332: 'val_root_mean_squared_error' reached 0.42470 (best 0.42470), saving model to '/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F2/automm_model/epoch=7-step=14332.ckpt' as top 3\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Epoch 7, global step 15288: 'val_root_mean_squared_error' reached 0.42260 (best 0.42260), saving model to '/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F2/automm_model/epoch=7-step=15288.ckpt' as top 3\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Epoch 8, global step 16243: 'val_root_mean_squared_error' reached 0.41836 (best 0.41836), saving model to '/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F2/automm_model/epoch=8-step=16243.ckpt' as top 3\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Epoch 8, global step 17199: 'val_root_mean_squared_error' reached 0.42007 (best 0.41836), saving model to '/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F2/automm_model/epoch=8-step=17199.ckpt' as top 3\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Epoch 9, global step 18154: 'val_root_mean_squared_error' reached 0.42117 (best 0.41836), saving model to '/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F2/automm_model/epoch=9-step=18154.ckpt' as top 3\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Epoch 9, global step 19110: 'val_root_mean_squared_error' reached 0.42033 (best 0.41836), saving model to '/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F2/automm_model/epoch=9-step=19110.ckpt' as top 3\n",
      "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=10` reached.\n",
      "INFO:pytorch_lightning.utilities.rank_zero:You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "INFO:pytorch_lightning.utilities.rank_zero:You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "INFO:pytorch_lightning.utilities.rank_zero:You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "INFO:pytorch_lightning.utilities.rank_zero:You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Configuration saved in AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F2/automm_model/hf_text/config.json\n",
      "tokenizer config file saved in AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F2/automm_model/hf_text/tokenizer_config.json\n",
      "Special tokens file saved in AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F2/automm_model/hf_text/special_tokens_map.json\n",
      "INFO:lightning_fabric.utilities.seed:Global seed set to 0\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--electra-base-discriminator/snapshots/1b48ef100dac4676d84125a8a7b7ab7c51e00386/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"google/electra-base-discriminator\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--electra-base-discriminator/snapshots/1b48ef100dac4676d84125a8a7b7ab7c51e00386/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"google/electra-base-discriminator\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--google--electra-base-discriminator/snapshots/1b48ef100dac4676d84125a8a7b7ab7c51e00386/pytorch_model.bin\n",
      "Some weights of the model checkpoint at google/electra-base-discriminator were not used when initializing ElectraModel: ['discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of ElectraModel were initialized from the model checkpoint at google/electra-base-discriminator.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use ElectraModel for predictions without further training.\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--electra-base-discriminator/snapshots/1b48ef100dac4676d84125a8a7b7ab7c51e00386/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"google/electra-base-discriminator\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--google--electra-base-discriminator/snapshots/1b48ef100dac4676d84125a8a7b7ab7c51e00386/vocab.txt\n",
      "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--google--electra-base-discriminator/snapshots/1b48ef100dac4676d84125a8a7b7ab7c51e00386/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--google--electra-base-discriminator/snapshots/1b48ef100dac4676d84125a8a7b7ab7c51e00386/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--electra-base-discriminator/snapshots/1b48ef100dac4676d84125a8a7b7ab7c51e00386/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"google/electra-base-discriminator\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--electra-base-discriminator/snapshots/1b48ef100dac4676d84125a8a7b7ab7c51e00386/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"google/electra-base-discriminator\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--google--electra-base-discriminator/snapshots/1b48ef100dac4676d84125a8a7b7ab7c51e00386/vocab.txt\n",
      "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--google--electra-base-discriminator/snapshots/1b48ef100dac4676d84125a8a7b7ab7c51e00386/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--google--electra-base-discriminator/snapshots/1b48ef100dac4676d84125a8a7b7ab7c51e00386/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--electra-base-discriminator/snapshots/1b48ef100dac4676d84125a8a7b7ab7c51e00386/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"google/electra-base-discriminator\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "/usr/local/lib/python3.8/dist-packages/autogluon/multimodal/utils/environment.py:58: UserWarning: Interactive environment is detected. Currently, MultiModalPredictor does not support multi-gpu training under an interactive environment due to the limitation of ddp / ddp_spawn strategies in PT Lightning. Thus, we switch to single gpu training. For multi-gpu training, you need to execute MultiModalPredictor in a script.\n",
      "  warnings.warn(\n",
      "Configuration saved in /MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F3/automm_model/hf_text/config.json\n",
      "tokenizer config file saved in /MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F3/automm_model/hf_text/tokenizer_config.json\n",
      "Special tokens file saved in /MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F3/automm_model/hf_text/special_tokens_map.json\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Using 16bit None Automatic Mixed Precision (AMP)\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [2,3,4]\n",
      "INFO:pytorch_lightning.callbacks.model_summary:\n",
      "  | Name              | Type                | Params\n",
      "----------------------------------------------------------\n",
      "0 | model             | MultimodalFusionMLP | 109 M \n",
      "1 | validation_metric | MeanSquaredError    | 0     \n",
      "2 | loss_func         | MSELoss             | 0     \n",
      "----------------------------------------------------------\n",
      "109 M     Trainable params\n",
      "0         Non-trainable params\n",
      "109 M     Total params\n",
      "219.742   Total estimated model params size (MB)\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Epoch 0, global step 955: 'val_root_mean_squared_error' reached 0.63262 (best 0.63262), saving model to '/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F3/automm_model/epoch=0-step=955.ckpt' as top 3\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Epoch 0, global step 1911: 'val_root_mean_squared_error' reached 0.56284 (best 0.56284), saving model to '/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F3/automm_model/epoch=0-step=1911.ckpt' as top 3\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Epoch 1, global step 2866: 'val_root_mean_squared_error' reached 0.54123 (best 0.54123), saving model to '/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F3/automm_model/epoch=1-step=2866.ckpt' as top 3\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Epoch 1, global step 3822: 'val_root_mean_squared_error' reached 0.48301 (best 0.48301), saving model to '/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F3/automm_model/epoch=1-step=3822.ckpt' as top 3\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Epoch 2, global step 4777: 'val_root_mean_squared_error' reached 0.48729 (best 0.48301), saving model to '/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F3/automm_model/epoch=2-step=4777.ckpt' as top 3\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Epoch 2, global step 5733: 'val_root_mean_squared_error' reached 0.45363 (best 0.45363), saving model to '/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F3/automm_model/epoch=2-step=5733.ckpt' as top 3\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Epoch 3, global step 6688: 'val_root_mean_squared_error' reached 0.44138 (best 0.44138), saving model to '/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F3/automm_model/epoch=3-step=6688.ckpt' as top 3\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Epoch 3, global step 7644: 'val_root_mean_squared_error' reached 0.44966 (best 0.44138), saving model to '/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F3/automm_model/epoch=3-step=7644.ckpt' as top 3\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Epoch 4, global step 8599: 'val_root_mean_squared_error' reached 0.43914 (best 0.43914), saving model to '/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F3/automm_model/epoch=4-step=8599.ckpt' as top 3\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Epoch 4, global step 9555: 'val_root_mean_squared_error' reached 0.42925 (best 0.42925), saving model to '/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F3/automm_model/epoch=4-step=9555.ckpt' as top 3\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Epoch 5, global step 10510: 'val_root_mean_squared_error' reached 0.43759 (best 0.42925), saving model to '/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F3/automm_model/epoch=5-step=10510.ckpt' as top 3\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Epoch 5, global step 11466: 'val_root_mean_squared_error' reached 0.42452 (best 0.42452), saving model to '/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F3/automm_model/epoch=5-step=11466.ckpt' as top 3\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Epoch 6, global step 12421: 'val_root_mean_squared_error' reached 0.42772 (best 0.42452), saving model to '/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F3/automm_model/epoch=6-step=12421.ckpt' as top 3\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Epoch 6, global step 13377: 'val_root_mean_squared_error' reached 0.41892 (best 0.41892), saving model to '/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F3/automm_model/epoch=6-step=13377.ckpt' as top 3\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Epoch 7, global step 14332: 'val_root_mean_squared_error' reached 0.41916 (best 0.41892), saving model to '/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F3/automm_model/epoch=7-step=14332.ckpt' as top 3\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Epoch 7, global step 15288: 'val_root_mean_squared_error' reached 0.41700 (best 0.41700), saving model to '/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F3/automm_model/epoch=7-step=15288.ckpt' as top 3\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Epoch 8, global step 16243: 'val_root_mean_squared_error' reached 0.41803 (best 0.41700), saving model to '/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F3/automm_model/epoch=8-step=16243.ckpt' as top 3\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Epoch 8, global step 17199: 'val_root_mean_squared_error' reached 0.41788 (best 0.41700), saving model to '/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F3/automm_model/epoch=8-step=17199.ckpt' as top 3\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Epoch 9, global step 18154: 'val_root_mean_squared_error' was not in top 3\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Epoch 9, global step 19110: 'val_root_mean_squared_error' reached 0.41777 (best 0.41700), saving model to '/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F3/automm_model/epoch=9-step=19110.ckpt' as top 3\n",
      "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=10` reached.\n",
      "INFO:pytorch_lightning.utilities.rank_zero:You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "INFO:pytorch_lightning.utilities.rank_zero:You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "INFO:pytorch_lightning.utilities.rank_zero:You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "INFO:pytorch_lightning.utilities.rank_zero:You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Configuration saved in AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F3/automm_model/hf_text/config.json\n",
      "tokenizer config file saved in AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F3/automm_model/hf_text/tokenizer_config.json\n",
      "Special tokens file saved in AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F3/automm_model/hf_text/special_tokens_map.json\n",
      "INFO:lightning_fabric.utilities.seed:Global seed set to 0\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--electra-base-discriminator/snapshots/1b48ef100dac4676d84125a8a7b7ab7c51e00386/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"google/electra-base-discriminator\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--electra-base-discriminator/snapshots/1b48ef100dac4676d84125a8a7b7ab7c51e00386/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"google/electra-base-discriminator\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--google--electra-base-discriminator/snapshots/1b48ef100dac4676d84125a8a7b7ab7c51e00386/pytorch_model.bin\n",
      "Some weights of the model checkpoint at google/electra-base-discriminator were not used when initializing ElectraModel: ['discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of ElectraModel were initialized from the model checkpoint at google/electra-base-discriminator.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use ElectraModel for predictions without further training.\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--electra-base-discriminator/snapshots/1b48ef100dac4676d84125a8a7b7ab7c51e00386/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"google/electra-base-discriminator\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--google--electra-base-discriminator/snapshots/1b48ef100dac4676d84125a8a7b7ab7c51e00386/vocab.txt\n",
      "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--google--electra-base-discriminator/snapshots/1b48ef100dac4676d84125a8a7b7ab7c51e00386/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--google--electra-base-discriminator/snapshots/1b48ef100dac4676d84125a8a7b7ab7c51e00386/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--electra-base-discriminator/snapshots/1b48ef100dac4676d84125a8a7b7ab7c51e00386/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"google/electra-base-discriminator\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--electra-base-discriminator/snapshots/1b48ef100dac4676d84125a8a7b7ab7c51e00386/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"google/electra-base-discriminator\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--google--electra-base-discriminator/snapshots/1b48ef100dac4676d84125a8a7b7ab7c51e00386/vocab.txt\n",
      "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--google--electra-base-discriminator/snapshots/1b48ef100dac4676d84125a8a7b7ab7c51e00386/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--google--electra-base-discriminator/snapshots/1b48ef100dac4676d84125a8a7b7ab7c51e00386/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--electra-base-discriminator/snapshots/1b48ef100dac4676d84125a8a7b7ab7c51e00386/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"google/electra-base-discriminator\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "/usr/local/lib/python3.8/dist-packages/autogluon/multimodal/utils/environment.py:58: UserWarning: Interactive environment is detected. Currently, MultiModalPredictor does not support multi-gpu training under an interactive environment due to the limitation of ddp / ddp_spawn strategies in PT Lightning. Thus, we switch to single gpu training. For multi-gpu training, you need to execute MultiModalPredictor in a script.\n",
      "  warnings.warn(\n",
      "Configuration saved in /MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F4/automm_model/hf_text/config.json\n",
      "tokenizer config file saved in /MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F4/automm_model/hf_text/tokenizer_config.json\n",
      "Special tokens file saved in /MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F4/automm_model/hf_text/special_tokens_map.json\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Using 16bit None Automatic Mixed Precision (AMP)\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [2,3,4]\n",
      "INFO:pytorch_lightning.callbacks.model_summary:\n",
      "  | Name              | Type                | Params\n",
      "----------------------------------------------------------\n",
      "0 | model             | MultimodalFusionMLP | 109 M \n",
      "1 | validation_metric | MeanSquaredError    | 0     \n",
      "2 | loss_func         | MSELoss             | 0     \n",
      "----------------------------------------------------------\n",
      "109 M     Trainable params\n",
      "0         Non-trainable params\n",
      "109 M     Total params\n",
      "219.742   Total estimated model params size (MB)\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Epoch 0, global step 955: 'val_root_mean_squared_error' reached 0.62250 (best 0.62250), saving model to '/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F4/automm_model/epoch=0-step=955.ckpt' as top 3\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Epoch 0, global step 1911: 'val_root_mean_squared_error' reached 0.56019 (best 0.56019), saving model to '/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F4/automm_model/epoch=0-step=1911.ckpt' as top 3\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Epoch 1, global step 2866: 'val_root_mean_squared_error' reached 0.49811 (best 0.49811), saving model to '/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F4/automm_model/epoch=1-step=2866.ckpt' as top 3\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Epoch 1, global step 3822: 'val_root_mean_squared_error' reached 0.46470 (best 0.46470), saving model to '/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F4/automm_model/epoch=1-step=3822.ckpt' as top 3\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Epoch 2, global step 4777: 'val_root_mean_squared_error' reached 0.46452 (best 0.46452), saving model to '/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F4/automm_model/epoch=2-step=4777.ckpt' as top 3\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Epoch 2, global step 5733: 'val_root_mean_squared_error' reached 0.44760 (best 0.44760), saving model to '/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F4/automm_model/epoch=2-step=5733.ckpt' as top 3\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Epoch 3, global step 6688: 'val_root_mean_squared_error' was not in top 3\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Epoch 3, global step 7644: 'val_root_mean_squared_error' reached 0.45221 (best 0.44760), saving model to '/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F4/automm_model/epoch=3-step=7644.ckpt' as top 3\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Epoch 4, global step 8599: 'val_root_mean_squared_error' reached 0.44174 (best 0.44174), saving model to '/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F4/automm_model/epoch=4-step=8599.ckpt' as top 3\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Epoch 4, global step 9555: 'val_root_mean_squared_error' reached 0.43461 (best 0.43461), saving model to '/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F4/automm_model/epoch=4-step=9555.ckpt' as top 3\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Epoch 5, global step 10510: 'val_root_mean_squared_error' reached 0.42986 (best 0.42986), saving model to '/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F4/automm_model/epoch=5-step=10510.ckpt' as top 3\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Epoch 5, global step 11466: 'val_root_mean_squared_error' reached 0.42494 (best 0.42494), saving model to '/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F4/automm_model/epoch=5-step=11466.ckpt' as top 3\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Epoch 6, global step 12421: 'val_root_mean_squared_error' reached 0.42787 (best 0.42494), saving model to '/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F4/automm_model/epoch=6-step=12421.ckpt' as top 3\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Epoch 6, global step 13377: 'val_root_mean_squared_error' reached 0.42872 (best 0.42494), saving model to '/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F4/automm_model/epoch=6-step=13377.ckpt' as top 3\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Epoch 7, global step 14332: 'val_root_mean_squared_error' reached 0.42085 (best 0.42085), saving model to '/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F4/automm_model/epoch=7-step=14332.ckpt' as top 3\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Epoch 7, global step 15288: 'val_root_mean_squared_error' reached 0.41816 (best 0.41816), saving model to '/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F4/automm_model/epoch=7-step=15288.ckpt' as top 3\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Epoch 8, global step 16243: 'val_root_mean_squared_error' reached 0.42164 (best 0.41816), saving model to '/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F4/automm_model/epoch=8-step=16243.ckpt' as top 3\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Epoch 8, global step 17199: 'val_root_mean_squared_error' reached 0.42005 (best 0.41816), saving model to '/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F4/automm_model/epoch=8-step=17199.ckpt' as top 3\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Epoch 9, global step 18154: 'val_root_mean_squared_error' reached 0.41847 (best 0.41816), saving model to '/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F4/automm_model/epoch=9-step=18154.ckpt' as top 3\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Epoch 9, global step 19110: 'val_root_mean_squared_error' reached 0.41868 (best 0.41816), saving model to '/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F4/automm_model/epoch=9-step=19110.ckpt' as top 3\n",
      "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=10` reached.\n",
      "INFO:pytorch_lightning.utilities.rank_zero:You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "INFO:pytorch_lightning.utilities.rank_zero:You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "INFO:pytorch_lightning.utilities.rank_zero:You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "INFO:pytorch_lightning.utilities.rank_zero:You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Configuration saved in AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F4/automm_model/hf_text/config.json\n",
      "tokenizer config file saved in AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F4/automm_model/hf_text/tokenizer_config.json\n",
      "Special tokens file saved in AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F4/automm_model/hf_text/special_tokens_map.json\n",
      "INFO:lightning_fabric.utilities.seed:Global seed set to 0\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--electra-base-discriminator/snapshots/1b48ef100dac4676d84125a8a7b7ab7c51e00386/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"google/electra-base-discriminator\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--electra-base-discriminator/snapshots/1b48ef100dac4676d84125a8a7b7ab7c51e00386/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"google/electra-base-discriminator\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--google--electra-base-discriminator/snapshots/1b48ef100dac4676d84125a8a7b7ab7c51e00386/pytorch_model.bin\n",
      "Some weights of the model checkpoint at google/electra-base-discriminator were not used when initializing ElectraModel: ['discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of ElectraModel were initialized from the model checkpoint at google/electra-base-discriminator.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use ElectraModel for predictions without further training.\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--electra-base-discriminator/snapshots/1b48ef100dac4676d84125a8a7b7ab7c51e00386/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"google/electra-base-discriminator\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--google--electra-base-discriminator/snapshots/1b48ef100dac4676d84125a8a7b7ab7c51e00386/vocab.txt\n",
      "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--google--electra-base-discriminator/snapshots/1b48ef100dac4676d84125a8a7b7ab7c51e00386/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--google--electra-base-discriminator/snapshots/1b48ef100dac4676d84125a8a7b7ab7c51e00386/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--electra-base-discriminator/snapshots/1b48ef100dac4676d84125a8a7b7ab7c51e00386/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"google/electra-base-discriminator\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--electra-base-discriminator/snapshots/1b48ef100dac4676d84125a8a7b7ab7c51e00386/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"google/electra-base-discriminator\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--google--electra-base-discriminator/snapshots/1b48ef100dac4676d84125a8a7b7ab7c51e00386/vocab.txt\n",
      "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--google--electra-base-discriminator/snapshots/1b48ef100dac4676d84125a8a7b7ab7c51e00386/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--google--electra-base-discriminator/snapshots/1b48ef100dac4676d84125a8a7b7ab7c51e00386/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--electra-base-discriminator/snapshots/1b48ef100dac4676d84125a8a7b7ab7c51e00386/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"google/electra-base-discriminator\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "/usr/local/lib/python3.8/dist-packages/autogluon/multimodal/utils/environment.py:58: UserWarning: Interactive environment is detected. Currently, MultiModalPredictor does not support multi-gpu training under an interactive environment due to the limitation of ddp / ddp_spawn strategies in PT Lightning. Thus, we switch to single gpu training. For multi-gpu training, you need to execute MultiModalPredictor in a script.\n",
      "  warnings.warn(\n",
      "Configuration saved in /MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F5/automm_model/hf_text/config.json\n",
      "tokenizer config file saved in /MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F5/automm_model/hf_text/tokenizer_config.json\n",
      "Special tokens file saved in /MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F5/automm_model/hf_text/special_tokens_map.json\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Using 16bit None Automatic Mixed Precision (AMP)\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [2,3,4]\n",
      "INFO:pytorch_lightning.callbacks.model_summary:\n",
      "  | Name              | Type                | Params\n",
      "----------------------------------------------------------\n",
      "0 | model             | MultimodalFusionMLP | 109 M \n",
      "1 | validation_metric | MeanSquaredError    | 0     \n",
      "2 | loss_func         | MSELoss             | 0     \n",
      "----------------------------------------------------------\n",
      "109 M     Trainable params\n",
      "0         Non-trainable params\n",
      "109 M     Total params\n",
      "219.742   Total estimated model params size (MB)\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Epoch 0, global step 955: 'val_root_mean_squared_error' reached 0.65761 (best 0.65761), saving model to '/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F5/automm_model/epoch=0-step=955.ckpt' as top 3\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Epoch 0, global step 1911: 'val_root_mean_squared_error' reached 0.55064 (best 0.55064), saving model to '/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F5/automm_model/epoch=0-step=1911.ckpt' as top 3\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Epoch 1, global step 2866: 'val_root_mean_squared_error' reached 0.51231 (best 0.51231), saving model to '/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F5/automm_model/epoch=1-step=2866.ckpt' as top 3\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Epoch 1, global step 3822: 'val_root_mean_squared_error' reached 0.47184 (best 0.47184), saving model to '/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F5/automm_model/epoch=1-step=3822.ckpt' as top 3\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Epoch 2, global step 4777: 'val_root_mean_squared_error' reached 0.46045 (best 0.46045), saving model to '/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F5/automm_model/epoch=2-step=4777.ckpt' as top 3\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Epoch 2, global step 5733: 'val_root_mean_squared_error' reached 0.44818 (best 0.44818), saving model to '/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F5/automm_model/epoch=2-step=5733.ckpt' as top 3\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Epoch 3, global step 6688: 'val_root_mean_squared_error' reached 0.45228 (best 0.44818), saving model to '/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F5/automm_model/epoch=3-step=6688.ckpt' as top 3\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Epoch 3, global step 7644: 'val_root_mean_squared_error' reached 0.44743 (best 0.44743), saving model to '/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F5/automm_model/epoch=3-step=7644.ckpt' as top 3\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Epoch 4, global step 8599: 'val_root_mean_squared_error' reached 0.45119 (best 0.44743), saving model to '/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F5/automm_model/epoch=4-step=8599.ckpt' as top 3\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Epoch 4, global step 9555: 'val_root_mean_squared_error' reached 0.43238 (best 0.43238), saving model to '/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F5/automm_model/epoch=4-step=9555.ckpt' as top 3\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Epoch 5, global step 10510: 'val_root_mean_squared_error' reached 0.42997 (best 0.42997), saving model to '/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F5/automm_model/epoch=5-step=10510.ckpt' as top 3\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Epoch 5, global step 11466: 'val_root_mean_squared_error' reached 0.43010 (best 0.42997), saving model to '/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F5/automm_model/epoch=5-step=11466.ckpt' as top 3\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Epoch 6, global step 12421: 'val_root_mean_squared_error' reached 0.42357 (best 0.42357), saving model to '/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F5/automm_model/epoch=6-step=12421.ckpt' as top 3\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Epoch 6, global step 13377: 'val_root_mean_squared_error' reached 0.42429 (best 0.42357), saving model to '/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F5/automm_model/epoch=6-step=13377.ckpt' as top 3\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Epoch 7, global step 14332: 'val_root_mean_squared_error' reached 0.42494 (best 0.42357), saving model to '/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F5/automm_model/epoch=7-step=14332.ckpt' as top 3\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Epoch 7, global step 15288: 'val_root_mean_squared_error' reached 0.42157 (best 0.42157), saving model to '/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F5/automm_model/epoch=7-step=15288.ckpt' as top 3\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Epoch 8, global step 16243: 'val_root_mean_squared_error' reached 0.42401 (best 0.42157), saving model to '/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F5/automm_model/epoch=8-step=16243.ckpt' as top 3\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Epoch 8, global step 17199: 'val_root_mean_squared_error' reached 0.41707 (best 0.41707), saving model to '/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F5/automm_model/epoch=8-step=17199.ckpt' as top 3\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Epoch 9, global step 18154: 'val_root_mean_squared_error' reached 0.41791 (best 0.41707), saving model to '/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F5/automm_model/epoch=9-step=18154.ckpt' as top 3\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Epoch 9, global step 19110: 'val_root_mean_squared_error' reached 0.41763 (best 0.41707), saving model to '/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F5/automm_model/epoch=9-step=19110.ckpt' as top 3\n",
      "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=10` reached.\n",
      "INFO:pytorch_lightning.utilities.rank_zero:You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "INFO:pytorch_lightning.utilities.rank_zero:You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "INFO:pytorch_lightning.utilities.rank_zero:You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "INFO:pytorch_lightning.utilities.rank_zero:You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Configuration saved in AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F5/automm_model/hf_text/config.json\n",
      "tokenizer config file saved in AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F5/automm_model/hf_text/tokenizer_config.json\n",
      "Special tokens file saved in AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F5/automm_model/hf_text/special_tokens_map.json\n",
      "\t-1.0323\t = Validation score   (-root_mean_squared_error)\n",
      "\t129699.84s\t = Training   runtime\n",
      "\t578.44s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\t-0.8744\t = Validation score   (-root_mean_squared_error)\n",
      "\t2.16s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Excluded Model Types: ['GBM']\n",
      "\tFound 'GBM' model in hyperparameters, but 'GBM' is present in `excluded_model_types` and will be removed.\n",
      "\tFound 'GBM' model in hyperparameters, but 'GBM' is present in `excluded_model_types` and will be removed.\n",
      "\tFound 'GBM' model in hyperparameters, but 'GBM' is present in `excluded_model_types` and will be removed.\n",
      "Fitting 3 L2 models ...\n",
      "Fitting model: CatBoost_BAG_L2 ...\n",
      "\tMemory not enough to fit CatBoostModel folds in parallel. Will do sequential fitting instead. \tConsider decreasing folds trained in parallel by passing num_folds_parallel to ag_args_ensemble when calling predictor.fit\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "Warning: less than 75% gpu memory available for training. Free: 15169.1875 Total: 24259.6875\n",
      "Warning: less than 75% gpu memory available for training. Free: 17399.1875 Total: 24259.6875\n",
      "Warning: less than 75% gpu memory available for training. Free: 17399.1875 Total: 24259.6875\n",
      "Warning: less than 75% gpu memory available for training. Free: 17399.1875 Total: 24259.6875\n",
      "\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "Warning: less than 75% gpu memory available for training. Free: 15169.1875 Total: 24259.6875\n",
      "Warning: less than 75% gpu memory available for training. Free: 17399.1875 Total: 24259.6875\n",
      "Warning: less than 75% gpu memory available for training. Free: 17399.1875 Total: 24259.6875\n",
      "Warning: less than 75% gpu memory available for training. Free: 17399.1875 Total: 24259.6875\n",
      "\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "Warning: less than 75% gpu memory available for training. Free: 15169.1875 Total: 24259.6875\n",
      "Warning: less than 75% gpu memory available for training. Free: 17399.1875 Total: 24259.6875\n",
      "Warning: less than 75% gpu memory available for training. Free: 17399.1875 Total: 24259.6875\n",
      "Warning: less than 75% gpu memory available for training. Free: 17399.1875 Total: 24259.6875\n",
      "\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "Warning: less than 75% gpu memory available for training. Free: 15169.1875 Total: 24259.6875\n",
      "Warning: less than 75% gpu memory available for training. Free: 17399.1875 Total: 24259.6875\n",
      "Warning: less than 75% gpu memory available for training. Free: 17399.1875 Total: 24259.6875\n",
      "Warning: less than 75% gpu memory available for training. Free: 17399.1875 Total: 24259.6875\n",
      "\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "Warning: less than 75% gpu memory available for training. Free: 15169.1875 Total: 24259.6875\n",
      "Warning: less than 75% gpu memory available for training. Free: 17399.1875 Total: 24259.6875\n",
      "Warning: less than 75% gpu memory available for training. Free: 17399.1875 Total: 24259.6875\n",
      "Warning: less than 75% gpu memory available for training. Free: 17399.1875 Total: 24259.6875\n",
      "\t-0.859\t = Validation score   (-root_mean_squared_error)\n",
      "\t240.65s\t = Training   runtime\n",
      "\t14.12s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L2 ...\n",
      "\tMemory not enough to fit XGBoostModel folds in parallel. Will do sequential fitting instead. \tConsider decreasing folds trained in parallel by passing num_folds_parallel to ag_args_ensemble when calling predictor.fit\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "\t-0.8532\t = Validation score   (-root_mean_squared_error)\n",
      "\t934.1s\t = Training   runtime\n",
      "\t40.78s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L2 ...\n",
      "\tMemory not enough to fit TabularNeuralNetTorchModel folds in parallel. Will do sequential fitting instead. \tConsider decreasing folds trained in parallel by passing num_folds_parallel to ag_args_ensemble when calling predictor.fit\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with SequentialLocalFoldFittingStrategy\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "<__array_function__ internals>:200: RuntimeWarning: invalid value encountered in cast\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "TabularNeuralNetTorchModel not yet able to use more than 1 GPU. 'num_gpus' is set to >1, but we will be using only 1 GPU.\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "<__array_function__ internals>:200: RuntimeWarning: invalid value encountered in cast\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "TabularNeuralNetTorchModel not yet able to use more than 1 GPU. 'num_gpus' is set to >1, but we will be using only 1 GPU.\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "<__array_function__ internals>:200: RuntimeWarning: invalid value encountered in cast\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "TabularNeuralNetTorchModel not yet able to use more than 1 GPU. 'num_gpus' is set to >1, but we will be using only 1 GPU.\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "<__array_function__ internals>:200: RuntimeWarning: invalid value encountered in cast\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "TabularNeuralNetTorchModel not yet able to use more than 1 GPU. 'num_gpus' is set to >1, but we will be using only 1 GPU.\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "<__array_function__ internals>:200: RuntimeWarning: invalid value encountered in cast\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "TabularNeuralNetTorchModel not yet able to use more than 1 GPU. 'num_gpus' is set to >1, but we will be using only 1 GPU.\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "\t-0.9218\t = Validation score   (-root_mean_squared_error)\n",
      "\t1268.87s\t = Training   runtime\n",
      "\t22.9s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ...\n",
      "\t-0.8529\t = Validation score   (-root_mean_squared_error)\n",
      "\t1.68s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 144647.13s ... Best model: \"WeightedEnsemble_L3\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20230527_063009/\")\n"
     ]
    }
   ],
   "source": [
    "from autogluon.tabular import TabularPredictor\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2,3,4'\n",
    "predictor = TabularPredictor(label=label).fit(\n",
    "    train_data=train_all_data,\n",
    "    hyperparameters=hyperparameters,\n",
    "    excluded_model_types=excluded_model_types,\n",
    "    feature_metadata=feature_metadata,\n",
    "    num_bag_folds=5, num_bag_sets=1, num_stack_levels=1,\n",
    "    # num_bag_folds=5, \n",
    "    ag_args_fit={'num_gpus': 2}\n",
    "    # hyperparameters = {'NN_TORCH': {'num_epochs': 5}, 'GBM': {'num_boost_round': 20}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e3709f-919c-4724-965e-dcb68ee80cbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3b0d86e7-cdea-4717-b9ef-a82b680756db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Load pretrained checkpoint: /MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230526_111730/models/MultiModalPredictor_BAG_L1/S1F1/automm_model/model.ckpt\n",
      "INFO:pytorch_lightning.utilities.rank_zero:You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Load pretrained checkpoint: /MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230526_111730/models/MultiModalPredictor_BAG_L1/S1F2/automm_model/model.ckpt\n",
      "INFO:pytorch_lightning.utilities.rank_zero:You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Load pretrained checkpoint: /MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230526_111730/models/MultiModalPredictor_BAG_L1/S1F3/automm_model/model.ckpt\n",
      "INFO:pytorch_lightning.utilities.rank_zero:You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Load pretrained checkpoint: /MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230526_111730/models/MultiModalPredictor_BAG_L1/S1F4/automm_model/model.ckpt\n",
      "INFO:pytorch_lightning.utilities.rank_zero:You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Load pretrained checkpoint: /MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230526_111730/models/MultiModalPredictor_BAG_L1/S1F5/automm_model/model.ckpt\n",
      "INFO:pytorch_lightning.utilities.rank_zero:You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        model  score_test  score_val  pred_time_test  pred_time_val      fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0         VowpalWabbit_BAG_L1   -6.474182  -2.170498     3142.466837    1201.360886   3773.821155              3142.466837             1201.360886        3773.821155            1       True          4\n",
      "1             CatBoost_BAG_L1   -6.505793  -1.015767        1.341930       0.663028    424.088711                 1.341930                0.663028         424.088711            1       True          1\n",
      "2  MultiModalPredictor_BAG_L1   -6.516147  -1.341388      181.519297      65.557616   9144.728142               181.519297               65.557616        9144.728142            1       True          5\n",
      "3       NeuralNetTorch_BAG_L1   -6.527236  -1.112051       28.855850      10.308606    762.461131                28.855850               10.308606         762.461131            1       True          3\n",
      "4         WeightedEnsemble_L2   -6.564738  -0.938834      115.792241     442.779285   1779.331267                 0.011613                0.004122           2.072291            2       True          6\n",
      "5              XGBoost_BAG_L2   -6.567089  -0.933674     3483.789063    1728.374298  14825.887018                44.022301               18.680633         130.078744            2       True          8\n",
      "6         WeightedEnsemble_L3   -6.569035  -0.933363     3516.401285    1739.563684  15284.953688                 0.007078                0.004108           1.836637            3       True         10\n",
      "7             CatBoost_BAG_L2   -6.571483  -0.936825     3441.354734    1710.264342  14765.273681                 1.587972                0.570677          69.465408            2       True          7\n",
      "8              XGBoost_BAG_L1   -6.584474  -0.944886       85.582848     431.803528    590.709133                85.582848              431.803528         590.709133            1       True          2\n",
      "9       NeuralNetTorch_BAG_L2   -6.595616  -0.971075     3470.783934    1720.308265  15083.572899                31.017172               10.614601         387.764626            2       True          9\n"
     ]
    }
   ],
   "source": [
    "leaderboard = predictor.leaderboard(submit_all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "44a841a5-aeae-4575-ab75-8e0fd5ae80bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CatBoost_BAG_L1',\n",
       " 'XGBoost_BAG_L1',\n",
       " 'NeuralNetTorch_BAG_L1',\n",
       " 'VowpalWabbit_BAG_L1',\n",
       " 'MultiModalPredictor_BAG_L1',\n",
       " 'WeightedEnsemble_L2',\n",
       " 'CatBoost_BAG_L2',\n",
       " 'XGBoost_BAG_L2',\n",
       " 'NeuralNetTorch_BAG_L2',\n",
       " 'WeightedEnsemble_L3']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_models = predictor.get_model_names()\n",
    "all_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "64b7a53e-8d34-4bb2-be85-34c10728fa45",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction from CatBoost_BAG_L1 model: 9.485945\n"
     ]
    }
   ],
   "source": [
    "i = 0  # index of model to use\n",
    "model_to_use = predictor.get_model_names()[i]\n",
    "model_pred0 = predictor.predict(submit_all_data.drop(columns=[label]), model=model_to_use)\n",
    "print(\"Prediction from %s model: %s\" % (model_to_use, model_pred0.iloc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "58bcc2b9-3fee-4cf2-800a-5c0f1d5fbde9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction from XGBoost_BAG_L1 model: 9.514761\n"
     ]
    }
   ],
   "source": [
    "i = 1  # index of model to use\n",
    "model_to_use = predictor.get_model_names()[i]\n",
    "model_pred1 = predictor.predict(submit_all_data.drop(columns=[label]), model=model_to_use)\n",
    "print(\"Prediction from %s model: %s\" % (model_to_use, model_pred1.iloc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "86343599-9a32-4fbc-b21c-465cff71e029",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction from NeuralNetTorch_BAG_L1 model: 8.669583\n"
     ]
    }
   ],
   "source": [
    "i = 2  # index of model to use\n",
    "model_to_use = predictor.get_model_names()[i]\n",
    "model_pred2 = predictor.predict(submit_all_data.drop(columns=[label]), model=model_to_use)\n",
    "print(\"Prediction from %s model: %s\" % (model_to_use, model_pred2.iloc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9b7c50df-af11-4bee-a9c0-09bc88598dc1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction from VowpalWabbit_BAG_L1 model: 8.654921\n"
     ]
    }
   ],
   "source": [
    "i = 3  # index of model to use\n",
    "model_to_use = predictor.get_model_names()[i]\n",
    "model_pred3 = predictor.predict(submit_all_data.drop(columns=[label]), model=model_to_use)\n",
    "print(\"Prediction from %s model: %s\" % (model_to_use, model_pred3.iloc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6003c09d-61fe-4809-9759-a9cf995c4afa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading configuration file /MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F1/automm_model/hf_text/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F1/automm_model/hf_text\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "Load pretrained checkpoint: /MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F1/automm_model/model.ckpt\n",
      "INFO:pytorch_lightning.utilities.rank_zero:You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (618 > 512). Running this sequence through the model will result in indexing errors\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading configuration file /MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F2/automm_model/hf_text/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F2/automm_model/hf_text\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "Load pretrained checkpoint: /MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F2/automm_model/model.ckpt\n",
      "INFO:pytorch_lightning.utilities.rank_zero:You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (618 > 512). Running this sequence through the model will result in indexing errors\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading configuration file /MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F3/automm_model/hf_text/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F3/automm_model/hf_text\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "Load pretrained checkpoint: /MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F3/automm_model/model.ckpt\n",
      "INFO:pytorch_lightning.utilities.rank_zero:You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (618 > 512). Running this sequence through the model will result in indexing errors\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading configuration file /MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F4/automm_model/hf_text/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F4/automm_model/hf_text\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "Load pretrained checkpoint: /MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F4/automm_model/model.ckpt\n",
      "INFO:pytorch_lightning.utilities.rank_zero:You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (618 > 512). Running this sequence through the model will result in indexing errors\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading configuration file /MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F5/automm_model/hf_text/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F5/automm_model/hf_text\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "Load pretrained checkpoint: /MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F5/automm_model/model.ckpt\n",
      "INFO:pytorch_lightning.utilities.rank_zero:You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (618 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction from MultiModalPredictor_BAG_L1 model: 9.721411\n"
     ]
    }
   ],
   "source": [
    "i = 4  # index of model to use\n",
    "model_to_use = predictor.get_model_names()[i]\n",
    "model_pred4 = predictor.predict(submit_all_data.drop(columns=[label]), model=model_to_use)\n",
    "print(\"Prediction from %s model: %s\" % (model_to_use, model_pred4.iloc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "26c775c7-f048-4c20-8148-e685344b63c2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading configuration file /MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F1/automm_model/hf_text/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F1/automm_model/hf_text\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "Load pretrained checkpoint: /MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F1/automm_model/model.ckpt\n",
      "INFO:pytorch_lightning.utilities.rank_zero:You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (618 > 512). Running this sequence through the model will result in indexing errors\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading configuration file /MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F2/automm_model/hf_text/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F2/automm_model/hf_text\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "Load pretrained checkpoint: /MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F2/automm_model/model.ckpt\n",
      "INFO:pytorch_lightning.utilities.rank_zero:You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (618 > 512). Running this sequence through the model will result in indexing errors\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading configuration file /MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F3/automm_model/hf_text/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F3/automm_model/hf_text\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "Load pretrained checkpoint: /MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F3/automm_model/model.ckpt\n",
      "INFO:pytorch_lightning.utilities.rank_zero:You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (618 > 512). Running this sequence through the model will result in indexing errors\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading configuration file /MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F4/automm_model/hf_text/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F4/automm_model/hf_text\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "Load pretrained checkpoint: /MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F4/automm_model/model.ckpt\n",
      "INFO:pytorch_lightning.utilities.rank_zero:You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (618 > 512). Running this sequence through the model will result in indexing errors\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading configuration file /MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F5/automm_model/hf_text/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F5/automm_model/hf_text\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "Load pretrained checkpoint: /MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F5/automm_model/model.ckpt\n",
      "INFO:pytorch_lightning.utilities.rank_zero:You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (618 > 512). Running this sequence through the model will result in indexing errors\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction from WeightedEnsemble_L2 model: 9.525716\n"
     ]
    }
   ],
   "source": [
    "i = 5  # index of model to use\n",
    "model_to_use = predictor.get_model_names()[i]\n",
    "model_pred5 = predictor.predict(submit_all_data.drop(columns=[label]), model=model_to_use)\n",
    "print(\"Prediction from %s model: %s\" % (model_to_use, model_pred5.iloc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "62ecde5d-56c2-4cac-8bef-98b6251441b3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading configuration file /MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F1/automm_model/hf_text/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F1/automm_model/hf_text\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "Load pretrained checkpoint: /MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F1/automm_model/model.ckpt\n",
      "INFO:pytorch_lightning.utilities.rank_zero:You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (618 > 512). Running this sequence through the model will result in indexing errors\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading configuration file /MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F2/automm_model/hf_text/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F2/automm_model/hf_text\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "Load pretrained checkpoint: /MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F2/automm_model/model.ckpt\n",
      "INFO:pytorch_lightning.utilities.rank_zero:You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (618 > 512). Running this sequence through the model will result in indexing errors\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading configuration file /MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F3/automm_model/hf_text/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F3/automm_model/hf_text\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "Load pretrained checkpoint: /MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F3/automm_model/model.ckpt\n",
      "INFO:pytorch_lightning.utilities.rank_zero:You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (618 > 512). Running this sequence through the model will result in indexing errors\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading configuration file /MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F4/automm_model/hf_text/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F4/automm_model/hf_text\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "Load pretrained checkpoint: /MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F4/automm_model/model.ckpt\n",
      "INFO:pytorch_lightning.utilities.rank_zero:You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (618 > 512). Running this sequence through the model will result in indexing errors\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading configuration file /MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F5/automm_model/hf_text/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F5/automm_model/hf_text\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "Load pretrained checkpoint: /MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F5/automm_model/model.ckpt\n",
      "INFO:pytorch_lightning.utilities.rank_zero:You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (618 > 512). Running this sequence through the model will result in indexing errors\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction from CatBoost_BAG_L2 model: 9.651156\n"
     ]
    }
   ],
   "source": [
    "i = 6  # index of model to use\n",
    "model_to_use = predictor.get_model_names()[i]\n",
    "model_pred6 = predictor.predict(submit_all_data.drop(columns=[label]), model=model_to_use)\n",
    "print(\"Prediction from %s model: %s\" % (model_to_use, model_pred6.iloc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9be3b73d-bef1-4af6-9fb2-1b9159025b1f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading configuration file /MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F1/automm_model/hf_text/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F1/automm_model/hf_text\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "Load pretrained checkpoint: /MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F1/automm_model/model.ckpt\n",
      "INFO:pytorch_lightning.utilities.rank_zero:You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (618 > 512). Running this sequence through the model will result in indexing errors\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading configuration file /MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F2/automm_model/hf_text/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F2/automm_model/hf_text\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "Load pretrained checkpoint: /MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F2/automm_model/model.ckpt\n",
      "INFO:pytorch_lightning.utilities.rank_zero:You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (618 > 512). Running this sequence through the model will result in indexing errors\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading configuration file /MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F3/automm_model/hf_text/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F3/automm_model/hf_text\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "Load pretrained checkpoint: /MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F3/automm_model/model.ckpt\n",
      "INFO:pytorch_lightning.utilities.rank_zero:You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (618 > 512). Running this sequence through the model will result in indexing errors\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading configuration file /MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F4/automm_model/hf_text/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F4/automm_model/hf_text\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "Load pretrained checkpoint: /MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F4/automm_model/model.ckpt\n",
      "INFO:pytorch_lightning.utilities.rank_zero:You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (618 > 512). Running this sequence through the model will result in indexing errors\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading configuration file /MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F5/automm_model/hf_text/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F5/automm_model/hf_text\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "Load pretrained checkpoint: /MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F5/automm_model/model.ckpt\n",
      "INFO:pytorch_lightning.utilities.rank_zero:You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (618 > 512). Running this sequence through the model will result in indexing errors\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction from XGBoost_BAG_L2 model: 9.550249\n"
     ]
    }
   ],
   "source": [
    "i = 7  # index of model to use\n",
    "model_to_use = predictor.get_model_names()[i]\n",
    "model_pred7 = predictor.predict(submit_all_data.drop(columns=[label]), model=model_to_use)\n",
    "print(\"Prediction from %s model: %s\" % (model_to_use, model_pred7.iloc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bc4e5f1d-8fea-490e-8cae-6200d2f97d5f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading configuration file /MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F1/automm_model/hf_text/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F1/automm_model/hf_text\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "Load pretrained checkpoint: /MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F1/automm_model/model.ckpt\n",
      "INFO:pytorch_lightning.utilities.rank_zero:You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (618 > 512). Running this sequence through the model will result in indexing errors\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading configuration file /MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F2/automm_model/hf_text/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F2/automm_model/hf_text\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "Load pretrained checkpoint: /MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F2/automm_model/model.ckpt\n",
      "INFO:pytorch_lightning.utilities.rank_zero:You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (618 > 512). Running this sequence through the model will result in indexing errors\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading configuration file /MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F3/automm_model/hf_text/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F3/automm_model/hf_text\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "Load pretrained checkpoint: /MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F3/automm_model/model.ckpt\n",
      "INFO:pytorch_lightning.utilities.rank_zero:You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (618 > 512). Running this sequence through the model will result in indexing errors\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading configuration file /MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F4/automm_model/hf_text/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F4/automm_model/hf_text\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "Load pretrained checkpoint: /MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F4/automm_model/model.ckpt\n",
      "INFO:pytorch_lightning.utilities.rank_zero:You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (618 > 512). Running this sequence through the model will result in indexing errors\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading configuration file /MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F5/automm_model/hf_text/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F5/automm_model/hf_text\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "Load pretrained checkpoint: /MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F5/automm_model/model.ckpt\n",
      "INFO:pytorch_lightning.utilities.rank_zero:You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (618 > 512). Running this sequence through the model will result in indexing errors\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction from NeuralNetTorch_BAG_L2 model: 9.559865\n"
     ]
    }
   ],
   "source": [
    "i = 8  # index of model to use\n",
    "model_to_use = predictor.get_model_names()[i]\n",
    "model_pred8 = predictor.predict(submit_all_data.drop(columns=[label]), model=model_to_use)\n",
    "print(\"Prediction from %s model: %s\" % (model_to_use, model_pred8.iloc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "28557309-b2bc-44b0-94bf-2017f2cd4a76",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading configuration file /MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F1/automm_model/hf_text/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F1/automm_model/hf_text\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "Load pretrained checkpoint: /MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F1/automm_model/model.ckpt\n",
      "INFO:pytorch_lightning.utilities.rank_zero:You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (618 > 512). Running this sequence through the model will result in indexing errors\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading configuration file /MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F2/automm_model/hf_text/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F2/automm_model/hf_text\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "Load pretrained checkpoint: /MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F2/automm_model/model.ckpt\n",
      "INFO:pytorch_lightning.utilities.rank_zero:You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (618 > 512). Running this sequence through the model will result in indexing errors\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading configuration file /MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F3/automm_model/hf_text/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F3/automm_model/hf_text\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "Load pretrained checkpoint: /MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F3/automm_model/model.ckpt\n",
      "INFO:pytorch_lightning.utilities.rank_zero:You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (618 > 512). Running this sequence through the model will result in indexing errors\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading configuration file /MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F4/automm_model/hf_text/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F4/automm_model/hf_text\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "Load pretrained checkpoint: /MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F4/automm_model/model.ckpt\n",
      "INFO:pytorch_lightning.utilities.rank_zero:You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (618 > 512). Running this sequence through the model will result in indexing errors\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading configuration file /MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F5/automm_model/hf_text/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F5/automm_model/hf_text\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "Load pretrained checkpoint: /MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F5/automm_model/model.ckpt\n",
      "INFO:pytorch_lightning.utilities.rank_zero:You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (618 > 512). Running this sequence through the model will result in indexing errors\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction from WeightedEnsemble_L3 model: 9.568596\n"
     ]
    }
   ],
   "source": [
    "i = 9  # index of model to use\n",
    "model_to_use = predictor.get_model_names()[i]\n",
    "model_pred9 = predictor.predict(submit_all_data.drop(columns=[label]), model=model_to_use)\n",
    "print(\"Prediction from %s model: %s\" % (model_to_use, model_pred9.iloc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7466e466-e54e-480d-8cee-335d7c142d12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f2c86cee-e64f-427d-8571-57e1470d5af7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_pred0.to_csv('../data/auto_5f_CAT1.csv',header=True, index=None)\n",
    "model_pred1.to_csv('../data/auto_5f_XGB1.csv',header=True, index=None)\n",
    "model_pred2.to_csv('../data/auto_5f_NNT1.csv',header=True, index=None)\n",
    "model_pred3.to_csv('../data/auto_5f_VW1.csv',header=True, index=None)\n",
    "model_pred4.to_csv('../data/auto_5f_MMP1.csv',header=True, index=None)\n",
    "model_pred5.to_csv('../data/auto_5f_Ensemble2.csv',header=True, index=None)\n",
    "model_pred6.to_csv('../data/auto_5f_CAT2.csv',header=True, index=None)\n",
    "model_pred7.to_csv('../data/auto_5f_XGB2.csv',header=True, index=None)\n",
    "model_pred8.to_csv('../data/auto_5f_NNT2.csv',header=True, index=None)\n",
    "model_pred9.to_csv('../data/auto_5f_Ensemble3.csv',header=True, index=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b85ee718-b57d-4580-8491-fa3f9f281bc7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading configuration file /MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F1/automm_model/hf_text/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F1/automm_model/hf_text\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "Load pretrained checkpoint: /MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F1/automm_model/model.ckpt\n",
      "INFO:pytorch_lightning.utilities.rank_zero:You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (618 > 512). Running this sequence through the model will result in indexing errors\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading configuration file /MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F2/automm_model/hf_text/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F2/automm_model/hf_text\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "Load pretrained checkpoint: /MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F2/automm_model/model.ckpt\n",
      "INFO:pytorch_lightning.utilities.rank_zero:You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (618 > 512). Running this sequence through the model will result in indexing errors\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading configuration file /MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F3/automm_model/hf_text/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F3/automm_model/hf_text\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "Load pretrained checkpoint: /MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F3/automm_model/model.ckpt\n",
      "INFO:pytorch_lightning.utilities.rank_zero:You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (618 > 512). Running this sequence through the model will result in indexing errors\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading configuration file /MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F4/automm_model/hf_text/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F4/automm_model/hf_text\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "Load pretrained checkpoint: /MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F4/automm_model/model.ckpt\n",
      "INFO:pytorch_lightning.utilities.rank_zero:You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (618 > 512). Running this sequence through the model will result in indexing errors\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading configuration file /MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F5/automm_model/hf_text/config.json\n",
      "Model config ElectraConfig {\n",
      "  \"_name_or_path\": \"/MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F5/automm_model/hf_text\",\n",
      "  \"architectures\": [\n",
      "    \"ElectraForPreTraining\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"embedding_size\": 768,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"electra\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"summary_activation\": \"gelu\",\n",
      "  \"summary_last_dropout\": 0.1,\n",
      "  \"summary_type\": \"first\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "Load pretrained checkpoint: /MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230527_063009/models/MultiModalPredictor_BAG_L1/S1F5/automm_model/model.ckpt\n",
      "INFO:pytorch_lightning.utilities.rank_zero:You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (618 > 512). Running this sequence through the model will result in indexing errors\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n",
      "/usr/local/lib/python3.8/dist-packages/pandas/core/arrays/base.py:513: RuntimeWarning: invalid value encountered in cast\n",
      "  result = np.asarray(self, dtype=dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        model  score_test  score_val  pred_time_test  pred_time_val       fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0             CatBoost_BAG_L1   -6.224059  -0.966294       12.424019      12.136761    1886.828896                12.424019               12.136761        1886.828896            1       True          1\n",
      "1         VowpalWabbit_BAG_L1   -6.246320  -1.513337     3073.269047    1009.125686    4108.309258              3073.269047             1009.125686        4108.309258            1       True          4\n",
      "2       NeuralNetTorch_BAG_L1   -6.270536  -1.085602       34.347759      15.062787    1576.037658                34.347759               15.062787        1576.037658            1       True          3\n",
      "3             CatBoost_BAG_L2   -6.337632  -0.859033     4371.714132    1687.604743  140676.680133                11.849617               14.120359         240.653024            2       True          7\n",
      "4         WeightedEnsemble_L3   -6.365669  -0.852927     4706.686147    1728.389094  141612.467402                 0.006105                0.003873           1.684033            3       True         10\n",
      "5              XGBoost_BAG_L2   -6.372240  -0.853228     4694.830426    1714.264862  141370.130345               334.965910               40.780478         934.103236            2       True          8\n",
      "6         WeightedEnsemble_L2   -6.413806  -0.874387     4359.882499    1673.488732  140438.187065                 0.017984                0.004347           2.159957            2       True          6\n",
      "7  MultiModalPredictor_BAG_L1   -6.452168  -1.032268      894.053077     578.441623  129699.835797               894.053077              578.441623      129699.835797            1       True          5\n",
      "8       NeuralNetTorch_BAG_L2   -6.463701  -0.921803     4420.245836    1696.379683  141704.896457                60.381320               22.895299        1268.869349            2       True          9\n",
      "9              XGBoost_BAG_L1   -6.528817  -0.913534      345.770614      58.717527    3165.015500               345.770614               58.717527        3165.015500            1       True          2\n"
     ]
    }
   ],
   "source": [
    "leaderboard = predictor.leaderboard(submit_all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ec1ec1-ba39-4357-8acd-1b48015186d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'CatBoost_BAG_L1',\n",
    "#  'XGBoost_BAG_L1',\n",
    "#  'NeuralNetTorch_BAG_L1',\n",
    "#  'VowpalWabbit_BAG_L1',\n",
    "#  'MultiModalPredictor_BAG_L1',\n",
    "#  'WeightedEnsemble_L2',\n",
    "#  'CatBoost_BAG_L2',\n",
    "#  'XGBoost_BAG_L2',\n",
    "#  'NeuralNetTorch_BAG_L2',\n",
    "#  'WeightedEnsemble_L3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b04bee8c-58db-4b93-b09d-1b87e9dea351",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Load pretrained checkpoint: /MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230526_111730/models/MultiModalPredictor_BAG_L1/S1F1/automm_model/model.ckpt\n",
      "INFO:pytorch_lightning.utilities.rank_zero:You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Load pretrained checkpoint: /MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230526_111730/models/MultiModalPredictor_BAG_L1/S1F2/automm_model/model.ckpt\n",
      "INFO:pytorch_lightning.utilities.rank_zero:You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Load pretrained checkpoint: /MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230526_111730/models/MultiModalPredictor_BAG_L1/S1F3/automm_model/model.ckpt\n",
      "INFO:pytorch_lightning.utilities.rank_zero:You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Load pretrained checkpoint: /MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230526_111730/models/MultiModalPredictor_BAG_L1/S1F4/automm_model/model.ckpt\n",
      "INFO:pytorch_lightning.utilities.rank_zero:You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Load pretrained checkpoint: /MMChallenge/SMP/HyFea-main/AutogluonModels/ag-20230526_111730/models/MultiModalPredictor_BAG_L1/S1F5/automm_model/model.ckpt\n",
      "INFO:pytorch_lightning.utilities.rank_zero:You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "305613     8.664865\n",
       "305614    10.856478\n",
       "305615     9.701542\n",
       "305616     6.663115\n",
       "305617     6.663115\n",
       "Name: label, dtype: float32"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = predictor.predict(submit_all_data.drop(columns=[label]))\n",
    "y_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "acb90e03-85ee-40d5-a3e2-847c598ea488",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred.to_csv('../data/auto_5fold_5models.csv',header=True, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebdb8de-f324-498a-94c8-5592191b06a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58f95623-cb97-4e12-8baa-931d2de07a44",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "305613     8.541639\n",
       "305614    10.987025\n",
       "305615    10.023292\n",
       "305616     6.948594\n",
       "305617     6.948594\n",
       "Name: label, dtype: float32"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = predictor.predict(submit_all_data.drop(columns=[label]))\n",
    "y_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "13e02358-a746-4d2b-98f1-7bad6f2b83b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.304041"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6fe6367e-1d55-455c-98ff-a3e44639c764",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred.to_csv('../data/auto_4models.csv',header=True, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec62c54-a9e5-49b2-a2f8-4d875385b218",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

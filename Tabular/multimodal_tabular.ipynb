{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc646ef1-738c-48dc-8e27-134c89e1417b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from scipy.stats import norm, skew\n",
    "from sklearn.model_selection import KFold\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn import model_selection\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "all_data = pd.read_csv('../data/forauto.csv')\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "447cd0a4-8787-4ba9-bd59-a8f1ca40c40c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Uid</th>\n",
       "      <th>Uid_count</th>\n",
       "      <th>Category</th>\n",
       "      <th>Subcategory</th>\n",
       "      <th>Concept</th>\n",
       "      <th>Title_len</th>\n",
       "      <th>Title_number</th>\n",
       "      <th>Alltags_len</th>\n",
       "      <th>Alltags_number</th>\n",
       "      <th>img_length</th>\n",
       "      <th>...</th>\n",
       "      <th>followerCount</th>\n",
       "      <th>followingCount</th>\n",
       "      <th>Ispublic</th>\n",
       "      <th>label</th>\n",
       "      <th>img_file</th>\n",
       "      <th>Category.1</th>\n",
       "      <th>Concept.1</th>\n",
       "      <th>Subcategory.1</th>\n",
       "      <th>Alltags</th>\n",
       "      <th>Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21894</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>65</td>\n",
       "      <td>75</td>\n",
       "      <td>33</td>\n",
       "      <td>6</td>\n",
       "      <td>128</td>\n",
       "      <td>12</td>\n",
       "      <td>333</td>\n",
       "      <td>...</td>\n",
       "      <td>2405</td>\n",
       "      <td>2729</td>\n",
       "      <td>1</td>\n",
       "      <td>11.18</td>\n",
       "      <td>train/59@N75/775.jpg</td>\n",
       "      <td>Fashion</td>\n",
       "      <td>glam</td>\n",
       "      <td>Fashion</td>\n",
       "      <td>rock punk transgender tranny electronicmusic e...</td>\n",
       "      <td>Luis Drayton - Edinburgh shoot #6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53866</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>75</td>\n",
       "      <td>139</td>\n",
       "      <td>56</td>\n",
       "      <td>13</td>\n",
       "      <td>800</td>\n",
       "      <td>65</td>\n",
       "      <td>500</td>\n",
       "      <td>...</td>\n",
       "      <td>8491</td>\n",
       "      <td>236</td>\n",
       "      <td>1</td>\n",
       "      <td>15.15</td>\n",
       "      <td>train/1@N18/1075.jpg</td>\n",
       "      <td>Travel&amp;Active&amp;Sports</td>\n",
       "      <td>fifa</td>\n",
       "      <td>Soccer</td>\n",
       "      <td>brazil rio brasil riodejaneiro by maria fifa m...</td>\n",
       "      <td>Arena da Barra - Arena HSBC - Arena do PAN   #...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26948</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>42</td>\n",
       "      <td>480</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>188</td>\n",
       "      <td>23</td>\n",
       "      <td>500</td>\n",
       "      <td>...</td>\n",
       "      <td>1550</td>\n",
       "      <td>1346</td>\n",
       "      <td>1</td>\n",
       "      <td>10.99</td>\n",
       "      <td>train/351@N64/4890.jpg</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>cinema</td>\n",
       "      <td>Movies</td>\n",
       "      <td>old cinema beauty marilyn photoshop movie joke...</td>\n",
       "      <td>MARILYN 2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>355</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>68</td>\n",
       "      <td>225</td>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "      <td>61</td>\n",
       "      <td>9</td>\n",
       "      <td>500</td>\n",
       "      <td>...</td>\n",
       "      <td>1233</td>\n",
       "      <td>1115</td>\n",
       "      <td>1</td>\n",
       "      <td>8.63</td>\n",
       "      <td>train/6@N59/6568.jpg</td>\n",
       "      <td>Holiday&amp;Celebrations</td>\n",
       "      <td>old</td>\n",
       "      <td>Birthday</td>\n",
       "      <td>pictures old family scans brothers sister 1958...</td>\n",
       "      <td>Knikkertijd - 1959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>315</td>\n",
       "      <td>31</td>\n",
       "      <td>2</td>\n",
       "      <td>43</td>\n",
       "      <td>317</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>146</td>\n",
       "      <td>19</td>\n",
       "      <td>500</td>\n",
       "      <td>...</td>\n",
       "      <td>5781</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>11.16</td>\n",
       "      <td>train/1617@N40/7079.jpg</td>\n",
       "      <td>Food</td>\n",
       "      <td>thirsty</td>\n",
       "      <td>Drinks</td>\n",
       "      <td>hot sahara animal animals desert bottles drink...</td>\n",
       "      <td>CAMELS01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486189</th>\n",
       "      <td>46088</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>301</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>78</td>\n",
       "      <td>13</td>\n",
       "      <td>500</td>\n",
       "      <td>...</td>\n",
       "      <td>489</td>\n",
       "      <td>612</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>test/49686@N77/1118324.jpg</td>\n",
       "      <td>Travel&amp;Active&amp;Sports</td>\n",
       "      <td>sea</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>natur nature frog sea lake fish outdoor macro ...</td>\n",
       "      <td>Quaak!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486190</th>\n",
       "      <td>12280</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>110</td>\n",
       "      <td>25</td>\n",
       "      <td>4</td>\n",
       "      <td>164</td>\n",
       "      <td>23</td>\n",
       "      <td>500</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>test/31147@N75/1118329.jpg</td>\n",
       "      <td>Travel&amp;Active&amp;Sports</td>\n",
       "      <td>cars</td>\n",
       "      <td>Cars</td>\n",
       "      <td>11 arisk okruh veternov 2016 10 jn preov veter...</td>\n",
       "      <td>arisk okruh veternov 2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486191</th>\n",
       "      <td>43496</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>28</td>\n",
       "      <td>186</td>\n",
       "      <td>41</td>\n",
       "      <td>7</td>\n",
       "      <td>85</td>\n",
       "      <td>9</td>\n",
       "      <td>500</td>\n",
       "      <td>...</td>\n",
       "      <td>702</td>\n",
       "      <td>113</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>test/5656@N26/1118336.jpg</td>\n",
       "      <td>Animal</td>\n",
       "      <td>cat</td>\n",
       "      <td>Cats</td>\n",
       "      <td>holland netherlands amsterdam cat nl ramses gu...</td>\n",
       "      <td>Bike &amp; Cat / Eerste Anjeliers Dwarsstraat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486192</th>\n",
       "      <td>5492</td>\n",
       "      <td>17</td>\n",
       "      <td>7</td>\n",
       "      <td>27</td>\n",
       "      <td>54</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>305</td>\n",
       "      <td>35</td>\n",
       "      <td>500</td>\n",
       "      <td>...</td>\n",
       "      <td>1092</td>\n",
       "      <td>389</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>test/28302@N36/1118349.jpg</td>\n",
       "      <td>Whether&amp;Season</td>\n",
       "      <td>white</td>\n",
       "      <td>Snowing</td>\n",
       "      <td>street people bw white black netherlands monoc...</td>\n",
       "      <td>The Stairs crew</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486193</th>\n",
       "      <td>17523</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>351</td>\n",
       "      <td>21</td>\n",
       "      <td>4</td>\n",
       "      <td>29</td>\n",
       "      <td>4</td>\n",
       "      <td>500</td>\n",
       "      <td>...</td>\n",
       "      <td>347</td>\n",
       "      <td>612</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>test/102192@N21/1118356.jpg</td>\n",
       "      <td>Whether&amp;Season</td>\n",
       "      <td>flower</td>\n",
       "      <td>Spring</td>\n",
       "      <td>flower nature fleur extrieure</td>\n",
       "      <td>qui veut souffler ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>486194 rows × 89 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Uid  Uid_count  Category  Subcategory  Concept  Title_len  \\\n",
       "0       21894          4         5           65       75         33   \n",
       "1       53866         13         0           75      139         56   \n",
       "2       26948          1         3           42      480         12   \n",
       "3         355          1        10           68      225         18   \n",
       "4         315         31         2           43      317          8   \n",
       "...       ...        ...       ...          ...      ...        ...   \n",
       "486189  46088          2         0           35      301          6   \n",
       "486190  12280          7         0           60      110         25   \n",
       "486191  43496          9         4           28      186         41   \n",
       "486192   5492         17         7           27       54         15   \n",
       "486193  17523          4         7            4      351         21   \n",
       "\n",
       "        Title_number  Alltags_len  Alltags_number  img_length  ...  \\\n",
       "0                  6          128              12         333  ...   \n",
       "1                 13          800              65         500  ...   \n",
       "2                  2          188              23         500  ...   \n",
       "3                  3           61               9         500  ...   \n",
       "4                  1          146              19         500  ...   \n",
       "...              ...          ...             ...         ...  ...   \n",
       "486189             1           78              13         500  ...   \n",
       "486190             4          164              23         500  ...   \n",
       "486191             7           85               9         500  ...   \n",
       "486192             3          305              35         500  ...   \n",
       "486193             4           29               4         500  ...   \n",
       "\n",
       "        followerCount  followingCount  Ispublic  label  \\\n",
       "0                2405            2729         1  11.18   \n",
       "1                8491             236         1  15.15   \n",
       "2                1550            1346         1  10.99   \n",
       "3                1233            1115         1   8.63   \n",
       "4                5781              13         1  11.16   \n",
       "...               ...             ...       ...    ...   \n",
       "486189            489             612         1   0.00   \n",
       "486190              0               0         1   0.00   \n",
       "486191            702             113         1   0.00   \n",
       "486192           1092             389         1   0.00   \n",
       "486193            347             612         1   0.00   \n",
       "\n",
       "                           img_file            Category.1  Concept.1  \\\n",
       "0              train/59@N75/775.jpg               Fashion       glam   \n",
       "1              train/1@N18/1075.jpg  Travel&Active&Sports       fifa   \n",
       "2            train/351@N64/4890.jpg         Entertainment     cinema   \n",
       "3              train/6@N59/6568.jpg  Holiday&Celebrations        old   \n",
       "4           train/1617@N40/7079.jpg                  Food    thirsty   \n",
       "...                             ...                   ...        ...   \n",
       "486189   test/49686@N77/1118324.jpg  Travel&Active&Sports        sea   \n",
       "486190   test/31147@N75/1118329.jpg  Travel&Active&Sports       cars   \n",
       "486191    test/5656@N26/1118336.jpg                Animal        cat   \n",
       "486192   test/28302@N36/1118349.jpg        Whether&Season      white   \n",
       "486193  test/102192@N21/1118356.jpg        Whether&Season     flower   \n",
       "\n",
       "        Subcategory.1                                            Alltags  \\\n",
       "0             Fashion  rock punk transgender tranny electronicmusic e...   \n",
       "1              Soccer  brazil rio brasil riodejaneiro by maria fifa m...   \n",
       "2              Movies  old cinema beauty marilyn photoshop movie joke...   \n",
       "3            Birthday  pictures old family scans brothers sister 1958...   \n",
       "4              Drinks  hot sahara animal animals desert bottles drink...   \n",
       "...               ...                                                ...   \n",
       "486189        Seattle  natur nature frog sea lake fish outdoor macro ...   \n",
       "486190           Cars  11 arisk okruh veternov 2016 10 jn preov veter...   \n",
       "486191           Cats  holland netherlands amsterdam cat nl ramses gu...   \n",
       "486192        Snowing  street people bw white black netherlands monoc...   \n",
       "486193         Spring                      flower nature fleur extrieure   \n",
       "\n",
       "                                                    Title  \n",
       "0                       Luis Drayton - Edinburgh shoot #6  \n",
       "1       Arena da Barra - Arena HSBC - Arena do PAN   #...  \n",
       "2                                            MARILYN 2015  \n",
       "3                                      Knikkertijd - 1959  \n",
       "4                                                CAMELS01  \n",
       "...                                                   ...  \n",
       "486189                                             Quaak!  \n",
       "486190                          arisk okruh veternov 2016  \n",
       "486191          Bike & Cat / Eerste Anjeliers Dwarsstraat  \n",
       "486192                                    The Stairs crew  \n",
       "486193                              qui veut souffler ...  \n",
       "\n",
       "[486194 rows x 89 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9ef3dcb-97ea-4f2b-86c2-0daa9dd2e9e2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Uid', 'Uid_count', 'Category', 'Subcategory', 'Concept', 'Title_len',\n",
       "       'Title_number', 'Alltags_len', 'Alltags_number', 'img_length',\n",
       "       'img_width', 'pixel', 'img_model', 'svd_mode_t_0', 'svd_mode_t_1',\n",
       "       'svd_mode_t_2', 'svd_mode_t_3', 'svd_mode_t_4', 'svd_mode_t_5',\n",
       "       'svd_mode_t_6', 'svd_mode_t_7', 'svd_mode_t_8', 'svd_mode_t_9',\n",
       "       'svd_mode_t_10', 'svd_mode_t_11', 'svd_mode_t_12', 'svd_mode_t_13',\n",
       "       'svd_mode_t_14', 'svd_mode_t_15', 'svd_mode_t_16', 'svd_mode_t_17',\n",
       "       'svd_mode_t_18', 'svd_mode_t_19', 'svd_mode_0', 'svd_mode_1',\n",
       "       'svd_mode_2', 'svd_mode_3', 'svd_mode_4', 'svd_mode_5', 'svd_mode_6',\n",
       "       'svd_mode_7', 'svd_mode_8', 'svd_mode_9', 'svd_mode_10', 'svd_mode_11',\n",
       "       'svd_mode_12', 'svd_mode_13', 'svd_mode_14', 'svd_mode_15',\n",
       "       'svd_mode_16', 'svd_mode_17', 'svd_mode_18', 'svd_mode_19', 'Mediatype',\n",
       "       'hour', 'day', 'weekday', 'week_hour', 'year_weekday', 'Longitude',\n",
       "       'Latitude', 'Geoaccuracy', 'photo_count', 'ispro', 'firstdate',\n",
       "       'firstweek', 'firstmonth', 'firstdatetaken', 'firstdatetakenweek',\n",
       "       'firstdatetakenmonth', 'totalViews', 'totalTags', 'totalGeotagged',\n",
       "       'totalFaves', 'totalInGroup', 'photoCount', 'meanView', 'meanTags',\n",
       "       'meanFaves', 'followerCount', 'followingCount', 'Ispublic', 'label',\n",
       "       'img_file', 'Category.1', 'Concept.1', 'Subcategory.1', 'Alltags',\n",
       "       'Title'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b94d4a79-96ab-45cf-a14e-14890a45a595",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "useless_cols = ['Category.1', 'Concept.1', 'Subcategory.1', 'img_file','firstdate','firstweek', 'firstmonth', \n",
    "                'firstdatetaken', 'firstdatetakenweek','firstdatetakenmonth']\n",
    "\n",
    "cate_cols = ['Uid', 'Category', 'Latitude', 'Geoaccuracy', 'Subcategory', 'Concept', 'Mediatype', \n",
    "             'hour', 'day', 'weekday', 'week_hour','Geoaccuracy', 'ispro' , 'Ispublic', 'img_model']\n",
    "# useless_cols += cate_cols\n",
    "all_data = all_data.drop(useless_cols,axis=1)\n",
    "\n",
    "columns = ['Title_len', 'Title_number', 'Alltags_len', 'Alltags_number', \n",
    "           'photo_count', 'totalTags', 'totalGeotagged', 'totalFaves',\n",
    "          'totalInGroup','photoCount','meanView', 'meanTags', 'meanFaves', 'followerCount','followingCount']\n",
    "\n",
    "\n",
    "for i in columns:\n",
    "    all_data[i] = np.log1p(all_data[i])\n",
    "    \n",
    "train = all_data[:-180581]\n",
    "test = all_data[-180581:]\n",
    "\n",
    "test = test.reset_index(drop=True)\n",
    "\n",
    "# y = train[['label']].values\n",
    "# X = train.drop(['label'],axis=1).values\n",
    "# df_test = test.drop(['label'],axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5b1172c-536c-4958-94c0-a5a10e61d47d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-06-05 15:55:57.598220: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-05 15:55:59.214146: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-06-05 15:55:59.214292: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-06-05 15:55:59.214304: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass, field\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    Trainer,\n",
    "    EvalPrediction,\n",
    "    set_seed\n",
    ")\n",
    "from transformers.training_args import TrainingArguments\n",
    "\n",
    "# from multimodal_transformers.data import load_data_from_folder\n",
    "from multimodal_transformers.model import TabularConfig\n",
    "from multimodal_transformers.model import AutoModelWithTabular\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "os.environ['COMET_MODE'] = 'DISABLED'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15454f60-40ed-4b4f-96b3-805acae71348",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_data_from_folder(\n",
    "    folder_path,\n",
    "    text_cols,\n",
    "    tokenizer,\n",
    "    label_col,\n",
    "    label_list=None,\n",
    "    categorical_cols=None,\n",
    "    numerical_cols=None,\n",
    "    sep_text_token_str=\" \",\n",
    "    categorical_encode_type=\"none\",\n",
    "    numerical_transformer_method=\"quantile_normal\",\n",
    "    empty_text_values=None,\n",
    "    replace_empty_text=None,\n",
    "    max_token_length=None,\n",
    "    debug=False,\n",
    "    debug_dataset_size=100,\n",
    "):\n",
    "    \"\"\"\n",
    "    Function to load tabular and text data from a specified folder\n",
    "\n",
    "    Loads train, test and/or validation text and tabular data from specified\n",
    "    folder path into TorchTextDataset class and does categorical and numerical\n",
    "    data preprocessing if specified. Inside the folder, there is expected to be\n",
    "    a train.csv, and test.csv (and if given val.csv) containing the training, testing,\n",
    "    and validation sets respectively\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): The path to the folder containing `train.csv`, and `test.csv` (and if given `val.csv`)\n",
    "        text_cols (:obj:`list` of :obj:`str`): The column names in the dataset that contain text\n",
    "            from which we want to load\n",
    "        tokenizer (:obj:`transformers.tokenization_utils.PreTrainedTokenizer`):\n",
    "            HuggingFace tokenizer used to tokenize the input texts as specifed by text_cols\n",
    "        label_col (str): The column name of the label, for classification the column should have\n",
    "            int values from 0 to n_classes-1 as the label for each class.\n",
    "            For regression the column can have any numerical value\n",
    "        label_list (:obj:`list` of :obj:`str`, optional): Used for classification;\n",
    "            the names of the classes indexed by the values in label_col.\n",
    "        categorical_cols (:obj:`list` of :obj:`str`, optional): The column names in the dataset that\n",
    "            contain categorical features. The features can be already prepared numerically, or\n",
    "            could be preprocessed by the method specified by categorical_encode_type\n",
    "        numerical_cols (:obj:`list` of :obj:`str`, optional): The column names in the dataset that contain numerical features.\n",
    "            These columns should contain only numeric values.\n",
    "        sep_text_token_str (str, optional): The string token that is used to separate between the\n",
    "            different text columns for a given data example. For Bert for example,\n",
    "            this could be the [SEP] token.\n",
    "        categorical_encode_type (str, optional): Given categorical_cols, this specifies\n",
    "            what method we want to preprocess our categorical features.\n",
    "            choices: [ 'ohe', 'binary', None]\n",
    "            see encode_features.CategoricalFeatures for more details\n",
    "        numerical_transformer_method (str, optional): Given numerical_cols, this specifies\n",
    "            what method we want to use for normalizing our numerical data.\n",
    "            choices: ['yeo_johnson', 'box_cox', 'quantile_normal', None]\n",
    "            see https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html\n",
    "            for more details\n",
    "        empty_text_values (:obj:`list` of :obj:`str`, optional): specifies what texts should be considered as\n",
    "            missing which would be replaced by replace_empty_text\n",
    "        replace_empty_text (str, optional): The value of the string that will replace the texts\n",
    "            that match with those in empty_text_values. If this argument is None then\n",
    "            the text that match with empty_text_values will be skipped\n",
    "        max_token_length (int, optional): The token length to pad or truncate to on the\n",
    "            input text\n",
    "        debug (bool, optional): Whether or not to load a smaller debug version of the dataset\n",
    "\n",
    "    Returns:\n",
    "        :obj:`tuple` of `tabular_torch_dataset.TorchTextDataset`:\n",
    "            This tuple contains the\n",
    "            training, validation and testing sets. The val dataset is :obj:`None` if\n",
    "            there is no `val.csv` in folder_path\n",
    "    \"\"\"\n",
    "    train_df = pd.read_csv(join(folder_path, \"train.csv\"), index_col=0)\n",
    "    test_df = pd.read_csv(join(folder_path, \"test.csv\"), index_col=0)\n",
    "    if exists(join(folder_path, \"val.csv\")):\n",
    "        val_df = pd.read_csv(join(folder_path, \"val.csv\"), index_col=0)\n",
    "    else:\n",
    "        val_df = None\n",
    "\n",
    "    return load_train_val_test_helper(\n",
    "        train_df,\n",
    "        val_df,\n",
    "        test_df,\n",
    "        text_cols,\n",
    "        tokenizer,\n",
    "        label_col,\n",
    "        label_list,\n",
    "        categorical_cols,\n",
    "        numerical_cols,\n",
    "        sep_text_token_str,\n",
    "        categorical_encode_type,\n",
    "        numerical_transformer_method,\n",
    "        empty_text_values,\n",
    "        replace_empty_text,\n",
    "        max_token_length,\n",
    "        debug,\n",
    "        debug_dataset_size,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a25e6c87-c8cf-4906-a3b1-8290c41866b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_train_val_test_helper(\n",
    "    train_df,\n",
    "    val_df,\n",
    "    test_df,\n",
    "    text_cols,\n",
    "    tokenizer,\n",
    "    label_col,\n",
    "    label_list=None,\n",
    "    categorical_cols=None,\n",
    "    numerical_cols=None,\n",
    "    sep_text_token_str=\" \",\n",
    "    categorical_encode_type=\"none\",\n",
    "    numerical_transformer_method=\"quantile_normal\",\n",
    "    empty_text_values=None,\n",
    "    replace_empty_text=None,\n",
    "    max_token_length=None,\n",
    "    debug=False,\n",
    "    debug_dataset_size=100,\n",
    "):\n",
    "    if categorical_encode_type == \"ohe\" or categorical_encode_type == \"binary\":\n",
    "        dfs = [df for df in [train_df, val_df, test_df] if df is not None]\n",
    "        data_df = pd.concat(dfs, axis=0)\n",
    "        cat_feat_processor = CategoricalFeatures(\n",
    "            data_df, categorical_cols, categorical_encode_type\n",
    "        )\n",
    "        vals = cat_feat_processor.fit_transform()\n",
    "        cat_df = pd.DataFrame(vals, columns=cat_feat_processor.feat_names)\n",
    "        data_df = pd.concat([data_df, cat_df], axis=1)\n",
    "        categorical_cols = cat_feat_processor.feat_names\n",
    "\n",
    "        len_train = len(train_df)\n",
    "        len_val = len(val_df) if val_df is not None else 0\n",
    "\n",
    "        train_df = data_df.iloc[:len_train]\n",
    "        if val_df is not None:\n",
    "            val_df = data_df.iloc[len_train : len_train + len_val]\n",
    "            len_train = len_train + len_val\n",
    "        test_df = data_df.iloc[len_train:]\n",
    "\n",
    "        categorical_encode_type = None\n",
    "\n",
    "    if numerical_transformer_method != \"none\":\n",
    "        if numerical_transformer_method == \"yeo_johnson\":\n",
    "            numerical_transformer = PowerTransformer(method=\"yeo-johnson\")\n",
    "        elif numerical_transformer_method == \"box_cox\":\n",
    "            numerical_transformer = PowerTransformer(method=\"box-cox\")\n",
    "        elif numerical_transformer_method == \"quantile_normal\":\n",
    "            numerical_transformer = QuantileTransformer(output_distribution=\"normal\")\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"preprocessing transformer method \"\n",
    "                f\"{numerical_transformer_method} not implemented\"\n",
    "            )\n",
    "        num_feats = load_num_feats(train_df, convert_to_func(numerical_cols))\n",
    "        numerical_transformer.fit(num_feats)\n",
    "    else:\n",
    "        numerical_transformer = None\n",
    "\n",
    "    train_dataset = load_data(\n",
    "        train_df,\n",
    "        text_cols,\n",
    "        tokenizer,\n",
    "        label_col,\n",
    "        label_list,\n",
    "        categorical_cols,\n",
    "        numerical_cols,\n",
    "        sep_text_token_str,\n",
    "        categorical_encode_type,\n",
    "        numerical_transformer,\n",
    "        empty_text_values,\n",
    "        replace_empty_text,\n",
    "        max_token_length,\n",
    "        debug,\n",
    "        debug_dataset_size,\n",
    "    )\n",
    "    test_dataset = load_data(\n",
    "        test_df,\n",
    "        text_cols,\n",
    "        tokenizer,\n",
    "        label_col,\n",
    "        label_list,\n",
    "        categorical_cols,\n",
    "        numerical_cols,\n",
    "        sep_text_token_str,\n",
    "        categorical_encode_type,\n",
    "        numerical_transformer,\n",
    "        empty_text_values,\n",
    "        replace_empty_text,\n",
    "        max_token_length,\n",
    "        debug,\n",
    "        debug_dataset_size,\n",
    "    )\n",
    "\n",
    "    if val_df is not None:\n",
    "        val_dataset = load_data(\n",
    "            val_df,\n",
    "            text_cols,\n",
    "            tokenizer,\n",
    "            label_col,\n",
    "            label_list,\n",
    "            categorical_cols,\n",
    "            numerical_cols,\n",
    "            sep_text_token_str,\n",
    "            categorical_encode_type,\n",
    "            numerical_transformer,\n",
    "            empty_text_values,\n",
    "            replace_empty_text,\n",
    "            max_token_length,\n",
    "            debug,\n",
    "            debug_dataset_size,\n",
    "        )\n",
    "    else:\n",
    "        val_dataset = None\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "189af0e7-f2c9-475d-8361-de215a1b9c14",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "244490 61123\n"
     ]
    }
   ],
   "source": [
    "def pandas_split_valid_test_dataset(pandas_dataset, valid_ratio=0.2,  shuffle=True):\n",
    "    \n",
    "    index = list(range(len(pandas_dataset)))\n",
    "    \n",
    "    length = len(pandas_dataset)\n",
    "    len_valid = int(length * valid_ratio + 0.6)\n",
    "    # len_test = int(length * test_ratio + 0.6)\n",
    "\n",
    "    train_data = pandas_dataset.loc[index[:-len_valid]]\n",
    "    valid_data = pandas_dataset.loc[index[-len_valid:]]\n",
    "    # test_data = pandas_dataset.loc[index[-len_test:]]\n",
    "    return train_data, valid_data\n",
    "\n",
    "train_df, valid_df = pandas_split_valid_test_dataset(train, valid_ratio=0.2, shuffle=True)\n",
    "print(len(train_df), len(valid_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70d9ef5d-d9bd-4a3a-8916-e2fbb6126cdb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "valid_df = valid_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03d5d4b1-8d64-4a7f-b67b-ca59b8013f99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_df, val_df = np.split(train.sample(frac=1), [int(.8*len(data_df)), int(.9 * len(data_df))])\n",
    "# print('Num examples train-val-test')\n",
    "# print(len(train_df), len(val_df), len(test_df))\n",
    "# train_df.to_csv('train.csv')\n",
    "# valid_df.to_csv('val.csv')\n",
    "# test.to_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06f4769f-b203-4b6d-ad56-5294b2cbeeaa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelArguments:\n",
    "  # \"\"\"\n",
    "  # Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n",
    "  # \"\"\"\n",
    "\n",
    "    model_name_or_path: str = field(\n",
    "      metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n",
    "  )\n",
    "    config_name: Optional[str] = field(\n",
    "      default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n",
    "  )\n",
    "    tokenizer_name: Optional[str] = field(\n",
    "      default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n",
    "  )\n",
    "    cache_dir: Optional[str] = field(\n",
    "      default=None, metadata={\"help\": \"Where do you want to store the pretrained models downloaded from s3\"}\n",
    "  )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class MultimodalDataTrainingArguments:\n",
    "  # \"\"\"\n",
    "  # Arguments pertaining to how we combine tabular features\n",
    "  # Using `HfArgumentParser` we can turn this class\n",
    "  # into argparse arguments to be able to specify them on\n",
    "  # the command line.\n",
    "  # \"\"\"\n",
    "\n",
    "    data_path: str = field(metadata={\n",
    "                            'help': 'the path to the csv file containing the dataset'\n",
    "                        })\n",
    "    column_info_path: str = field(\n",
    "      default=None,\n",
    "      metadata={\n",
    "          'help': 'the path to the json file detailing which columns are text, categorical, numerical, and the label'\n",
    "      })\n",
    "\n",
    "    column_info: dict = field(\n",
    "      default=None,\n",
    "      metadata={\n",
    "          'help': 'a dict referencing the text, categorical, numerical, and label columns'\n",
    "                  'its keys are text_cols, num_cols, cat_cols, and label_col'\n",
    "      })\n",
    "\n",
    "    categorical_encode_type: str = field(default='none',\n",
    "                                        metadata={\n",
    "                                            'help': 'sklearn encoder to use for categorical data',\n",
    "                                            # 'choices': ['ohe', 'binary', 'label', 'none']\n",
    "                                            'choices':['none']\n",
    "                                        })\n",
    "    numerical_transformer_method: str = field(default='yeo_johnson',\n",
    "                                            metadata={\n",
    "                                                'help': 'sklearn numerical transformer to preprocess numerical data',\n",
    "                                                'choices': ['yeo_johnson', 'box_cox', 'quantile_normal', 'none']\n",
    "                                            })\n",
    "    task: str = field(default=\"regression\",\n",
    "                    metadata={\n",
    "                        \"help\": \"The downstream training task\",\n",
    "                        \"choices\": [\"classification\", \"regression\"]\n",
    "                    })\n",
    "\n",
    "    mlp_division: int = field(default=16,\n",
    "                            metadata={\n",
    "                                'help': 'the ratio of the number of '\n",
    "                                        'hidden dims in a current layer to the next MLP layer'\n",
    "                            })\n",
    "    combine_feat_method: str = field(default='individual_mlps_on_cat_and_numerical_feats_then_concat',\n",
    "                                    metadata={\n",
    "                                        'help': 'method to combine categorical and numerical features, '\n",
    "                                                'see README for all the method'\n",
    "                                    })\n",
    "    mlp_dropout: float = field(default=0.1,\n",
    "                              metadata={\n",
    "                                'help': 'dropout ratio used for MLP layers'\n",
    "                              })\n",
    "    numerical_bn: bool = field(default=True,\n",
    "                              metadata={\n",
    "                                  'help': 'whether to use batchnorm on numerical features'\n",
    "                              })\n",
    "    use_simple_classifier: str = field(default=True,\n",
    "                                      metadata={\n",
    "                                          'help': 'whether to use single layer or MLP as final classifier'\n",
    "                                      })\n",
    "    mlp_act: str = field(default='relu',\n",
    "                        metadata={\n",
    "                            'help': 'the activation function to use for finetuning layers',\n",
    "                            'choices': ['relu', 'prelu', 'sigmoid', 'tanh', 'linear']\n",
    "                        })\n",
    "    gating_beta: float = field(default=0.2,\n",
    "                              metadata={\n",
    "                                  'help': \"the beta hyperparameters used for gating tabular data \"\n",
    "                                          \"see https://www.aclweb.org/anthology/2020.acl-main.214.pdf\"\n",
    "                              })\n",
    "\n",
    "    def __post_init__(self):\n",
    "        assert self.column_info != self.column_info_path\n",
    "        if self.column_info is None and self.column_info_path:\n",
    "            with open(self.column_info_path, 'r') as f:\n",
    "                self.column_info = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b32e435f-cad1-4bf8-bd0d-49bfa45c6e7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_cols = ['Title', 'Alltags']\n",
    "# cat_cols = ['Clothing ID', 'Division Name', 'Department Name', 'Class Name']\n",
    "cat_cols = ['Uid', 'Category', 'Latitude', 'Geoaccuracy', 'Subcategory', 'Concept', 'Mediatype', \n",
    "             'hour', 'day', 'weekday', 'week_hour','Geoaccuracy', 'ispro' , 'Ispublic', 'img_model']\n",
    "label_cols = ['label']\n",
    "useful_cols = text_cols + cat_cols + label_cols\n",
    "numerical_cols = [c for c in all_data.columns if c not in useful_cols] \n",
    "\n",
    "column_info_dict = {\n",
    "    'text_cols': text_cols,\n",
    "    'num_cols': numerical_cols,\n",
    "    'cat_cols': cat_cols,\n",
    "    'label_col': 'label'\n",
    "    # 'label_list': ['Not Recommended', 'Recommended']\n",
    "}\n",
    "\n",
    "\n",
    "model_args = ModelArguments(\n",
    "    model_name_or_path='bert-base-uncased'\n",
    ")\n",
    "\n",
    "data_args = MultimodalDataTrainingArguments(\n",
    "    data_path='.',\n",
    "    combine_feat_method='gating_on_cat_and_num_feats_then_sum',\n",
    "    column_info=column_info_dict,\n",
    "    task='regression'\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./logs/model_name\",\n",
    "    logging_dir=\"./logs/runs\",\n",
    "    overwrite_output_dir=True,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    per_device_train_batch_size=256,\n",
    "    num_train_epochs=3,\n",
    "    # evaluate_during_training=True,\n",
    "    logging_steps=25,\n",
    "    eval_steps=250\n",
    ")\n",
    "\n",
    "set_seed(training_args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cbaa076c-83e3-4773-80e6-ee6a9092137a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Uid_count',\n",
       " 'Title_len',\n",
       " 'Title_number',\n",
       " 'Alltags_len',\n",
       " 'Alltags_number',\n",
       " 'img_length',\n",
       " 'img_width',\n",
       " 'pixel',\n",
       " 'svd_mode_t_0',\n",
       " 'svd_mode_t_1',\n",
       " 'svd_mode_t_2',\n",
       " 'svd_mode_t_3',\n",
       " 'svd_mode_t_4',\n",
       " 'svd_mode_t_5',\n",
       " 'svd_mode_t_6',\n",
       " 'svd_mode_t_7',\n",
       " 'svd_mode_t_8',\n",
       " 'svd_mode_t_9',\n",
       " 'svd_mode_t_10',\n",
       " 'svd_mode_t_11',\n",
       " 'svd_mode_t_12',\n",
       " 'svd_mode_t_13',\n",
       " 'svd_mode_t_14',\n",
       " 'svd_mode_t_15',\n",
       " 'svd_mode_t_16',\n",
       " 'svd_mode_t_17',\n",
       " 'svd_mode_t_18',\n",
       " 'svd_mode_t_19',\n",
       " 'svd_mode_0',\n",
       " 'svd_mode_1',\n",
       " 'svd_mode_2',\n",
       " 'svd_mode_3',\n",
       " 'svd_mode_4',\n",
       " 'svd_mode_5',\n",
       " 'svd_mode_6',\n",
       " 'svd_mode_7',\n",
       " 'svd_mode_8',\n",
       " 'svd_mode_9',\n",
       " 'svd_mode_10',\n",
       " 'svd_mode_11',\n",
       " 'svd_mode_12',\n",
       " 'svd_mode_13',\n",
       " 'svd_mode_14',\n",
       " 'svd_mode_15',\n",
       " 'svd_mode_16',\n",
       " 'svd_mode_17',\n",
       " 'svd_mode_18',\n",
       " 'svd_mode_19',\n",
       " 'year_weekday',\n",
       " 'Longitude',\n",
       " 'photo_count',\n",
       " 'totalViews',\n",
       " 'totalTags',\n",
       " 'totalGeotagged',\n",
       " 'totalFaves',\n",
       " 'totalInGroup',\n",
       " 'photoCount',\n",
       " 'meanView',\n",
       " 'meanTags',\n",
       " 'meanFaves',\n",
       " 'followerCount',\n",
       " 'followingCount']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e318e75f-b12e-4cef-8cde-a418887441c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d8b2e2a-0320-4af2-afb5-20b7309b81c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specified tokenizer:  bert-base-uncased\n"
     ]
    }
   ],
   "source": [
    "tokenizer_path_or_name = model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path\n",
    "print('Specified tokenizer: ', tokenizer_path_or_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    tokenizer_path_or_name,\n",
    "    cache_dir=model_args.cache_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "66418b27-8380-42c3-b3c5-f5f40da8011c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelArguments(model_name_or_path='bert-base-uncased', config_name=None, tokenizer_name=None, cache_dir=None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7199a08d-cfdf-4e38-b6e1-66474547e23f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_data(\n",
    "    data_df,\n",
    "    text_cols,\n",
    "    tokenizer,\n",
    "    label_col,\n",
    "    label_list=None,\n",
    "    categorical_cols=None,\n",
    "    numerical_cols=None,\n",
    "    sep_text_token_str=\" \",\n",
    "    categorical_encode_type=\"none\",\n",
    "    numerical_transformer=None,\n",
    "    empty_text_values=None,\n",
    "    replace_empty_text=None,\n",
    "    max_token_length=None,\n",
    "    debug=False,\n",
    "    debug_dataset_size=100,\n",
    "):\n",
    "    \"\"\"Function to load a single dataset given a pandas DataFrame\n",
    "\n",
    "    Given a DataFrame, this function loads the data to a :obj:`torch_dataset.TorchTextDataset`\n",
    "    object which can be used in a :obj:`torch.utils.data.DataLoader`.\n",
    "\n",
    "    Args:\n",
    "        data_df (:obj:`pd.DataFrame`): The DataFrame to convert to a TorchTextDataset\n",
    "        text_cols (:obj:`list` of :obj:`str`): the column names in the dataset that contain text\n",
    "            from which we want to load\n",
    "        tokenizer (:obj:`transformers.tokenization_utils.PreTrainedTokenizer`):\n",
    "            HuggingFace tokenizer used to tokenize the input texts as specifed by text_cols\n",
    "        label_col (str): The column name of the label, for classification the column should have\n",
    "            int values from 0 to n_classes-1 as the label for each class.\n",
    "            For regression the column can have any numerical value\n",
    "        label_list (:obj:`list` of :obj:`str`, optional): Used for classification;\n",
    "            the names of the classes indexed by the values in label_col.\n",
    "        categorical_cols (:obj:`list` of :obj:`str`, optional): The column names in the dataset that\n",
    "            contain categorical features. The features can be already prepared numerically, or\n",
    "            could be preprocessed by the method specified by categorical_encode_type\n",
    "        numerical_cols (:obj:`list` of :obj:`str`, optional): The column names in the dataset that contain numerical features.\n",
    "            These columns should contain only numeric values.\n",
    "        sep_text_token_str (str, optional): The string token that is used to separate between the\n",
    "            different text columns for a given data example. For Bert for example,\n",
    "            this could be the [SEP] token.\n",
    "        categorical_encode_type (str, optional): Given categorical_cols, this specifies\n",
    "            what method we want to preprocess our categorical features.\n",
    "            choices: [ 'ohe', 'binary', None]\n",
    "            see encode_features.CategoricalFeatures for more details\n",
    "        numerical_transformer (:obj:`sklearn.base.TransformerMixin`): The sklearn numeric\n",
    "            transformer instance to transform our numerical features\n",
    "        empty_text_values (:obj:`list` of :obj:`str`, optional): Specifies what texts should be considered as\n",
    "            missing which would be replaced by replace_empty_text\n",
    "        replace_empty_text (str, optional): The value of the string that will replace the texts\n",
    "            that match with those in empty_text_values. If this argument is None then\n",
    "            the text that match with empty_text_values will be skipped\n",
    "        max_token_length (int, optional): The token length to pad or truncate to on the\n",
    "            input text\n",
    "        debug (bool, optional): Whether or not to load a smaller debug version of the dataset\n",
    "\n",
    "    Returns:\n",
    "        :obj:`tabular_torch_dataset.TorchTextDataset`: The converted dataset\n",
    "    \"\"\"\n",
    "    if debug:\n",
    "        data_df = data_df[:debug_dataset_size]\n",
    "    if empty_text_values is None:\n",
    "        empty_text_values = [\"nan\", \"None\"]\n",
    "\n",
    "    text_cols_func = convert_to_func(text_cols)\n",
    "    categorical_cols_func = convert_to_func(categorical_cols)\n",
    "    numerical_cols_func = convert_to_func(numerical_cols)\n",
    "\n",
    "    categorical_feats, numerical_feats = load_cat_and_num_feats(\n",
    "        data_df, categorical_cols_func, numerical_cols_func, categorical_encode_type\n",
    "    )\n",
    "    numerical_feats = normalize_numerical_feats(numerical_feats, numerical_transformer)\n",
    "    agg_func = partial(agg_text_columns_func, empty_text_values, replace_empty_text)\n",
    "    texts_cols = get_matching_cols(data_df, text_cols_func)\n",
    "    logger.info(f\"Text columns: {texts_cols}\")\n",
    "    texts_list = data_df[texts_cols].agg(agg_func, axis=1).tolist()\n",
    "    for i, text in enumerate(texts_list):\n",
    "        texts_list[i] = f\" {sep_text_token_str} \".join(text)\n",
    "    logger.info(f\"Raw text example: {texts_list[0]}\")\n",
    "    hf_model_text_input = tokenizer(\n",
    "        texts_list, padding=True, truncation=True, max_length=max_token_length\n",
    "    )\n",
    "    tokenized_text_ex = \" \".join(\n",
    "        tokenizer.convert_ids_to_tokens(hf_model_text_input[\"input_ids\"][0])\n",
    "    )\n",
    "    logger.debug(f\"Tokenized text example: {tokenized_text_ex}\")\n",
    "    labels = data_df[label_col].values\n",
    "\n",
    "    return TorchTabularTextDataset(\n",
    "        hf_model_text_input,\n",
    "        categorical_feats,\n",
    "        numerical_feats,\n",
    "        labels,\n",
    "        data_df,\n",
    "        label_list,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "99c693db-13e3-4d41-a461-ada0adc8050c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:multimodal_transformers.data.data_utils:62 numerical columns\n",
      "INFO:multimodal_transformers.data.data_utils:14 categorical columns\n",
      "INFO:multimodal_transformers.data.data_utils:62 numerical columns\n",
      "INFO:multimodal_transformers.data.data_utils:Text columns: ['Alltags', 'Title']\n",
      "INFO:multimodal_transformers.data.data_utils:Raw text example: rock punk transgender tranny electronicmusic electro glam electronica luisdrayton fusionrecords thefusionnetwork lmwcphotography [SEP] Luis Drayton - Edinburgh shoot #6\n",
      "INFO:multimodal_transformers.data.data_utils:14 categorical columns\n",
      "INFO:multimodal_transformers.data.data_utils:62 numerical columns\n",
      "INFO:multimodal_transformers.data.data_utils:Text columns: ['Alltags', 'Title']\n",
      "INFO:multimodal_transformers.data.data_utils:Raw text example: vintage toys cool nikon peace lego free retro hippie hugs minifig mai68 serie7 d3100 designholic [SEP] May 1968\n",
      "INFO:multimodal_transformers.data.data_utils:14 categorical columns\n",
      "INFO:multimodal_transformers.data.data_utils:62 numerical columns\n",
      "INFO:multimodal_transformers.data.data_utils:Text columns: ['Alltags', 'Title']\n",
      "INFO:multimodal_transformers.data.data_utils:Raw text example: family wedding friends party amigos love guests dress photos amor natureza famlia diverso fotos jardim casamento weddingparty weddingdress festa dana bruno brinde buqu vestido noiva bebida esposa fotografias marido padrinhos unio emoo encantado noivo noivos eduarda convidados madrinhas cerimnia recmcasados vestidodenoiva vestidobranco aoarlivre felipemanfroi eduardostoll objetivofotografia [SEP] OF-Casamento-EduardaBruno-1973\n"
     ]
    }
   ],
   "source": [
    "# Get Datasets\n",
    "from os.path import join, exists\n",
    "from functools import partial\n",
    "import logging\n",
    "from os.path import join, exists\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.preprocessing import PowerTransformer, QuantileTransformer\n",
    "from multimodal_transformers.data.data_utils import *\n",
    "from multimodal_transformers.data.tabular_torch_dataset import TorchTabularTextDataset\n",
    "train_dataset, val_dataset, test_dataset = load_data_from_folder(\n",
    "    data_args.data_path,\n",
    "    data_args.column_info['text_cols'],\n",
    "    tokenizer,\n",
    "    label_col=data_args.column_info['label_col'],\n",
    "    # label_list=data_args.column_info['label_list'],\n",
    "    categorical_cols=data_args.column_info['cat_cols'],\n",
    "    numerical_cols=data_args.column_info['num_cols'],\n",
    "    sep_text_token_str=tokenizer.sep_token,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cec832-2599-4d16-8a9d-65454dba6aa1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# num_labels = len(np.unique(train_dataset.labels))\n",
    "# num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f2502661-2a0f-423a-9dc3-fd0cc29490d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(\n",
    "        model_args.config_name if model_args.config_name else model_args.model_name_or_path,\n",
    "        cache_dir=model_args.cache_dir,\n",
    "    )\n",
    "tabular_config = TabularConfig(num_labels=1,\n",
    "                               cat_feat_dim=train_dataset.cat_feats.shape[1],\n",
    "                               numerical_feat_dim=train_dataset.numerical_feats.shape[1],\n",
    "                               **vars(data_args))\n",
    "config.tabular_config = tabular_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bad9f413-bf24-4f76-9eb3-206ad2ee3e48",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertWithTabular: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertWithTabular from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertWithTabular from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertWithTabular were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['tabular_combiner.num_bn.running_mean', 'tabular_combiner.h_bias', 'tabular_combiner.g_cat_layer.bias', 'classifier.bias', 'tabular_combiner.num_bn.bias', 'tabular_classifier.weight', 'tabular_combiner.h_cat_layer.weight', 'tabular_combiner.g_num_layer.weight', 'tabular_combiner.g_cat_layer.weight', 'classifier.weight', 'tabular_combiner.num_bn.running_var', 'tabular_combiner.num_bn.num_batches_tracked', 'tabular_combiner.layer_norm.bias', 'tabular_combiner.g_num_layer.bias', 'tabular_combiner.num_bn.weight', 'tabular_combiner.h_num_layer.weight', 'tabular_combiner.layer_norm.weight', 'tabular_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelWithTabular.from_pretrained(\n",
    "        model_args.config_name if model_args.config_name else model_args.model_name_or_path,\n",
    "        config=config,\n",
    "        cache_dir=model_args.cache_dir\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce342389-55d5-4a4d-b967-7fb02307e0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from scipy.special import softmax\n",
    "# from sklearn.metrics import (\n",
    "#     auc,\n",
    "#     precision_recall_curve,\n",
    "#     roc_auc_score,\n",
    "#     f1_score,\n",
    "#     confusion_matrix,\n",
    "#     matthews_corrcoef,\n",
    "# )\n",
    "import math\n",
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    auc,\n",
    "    precision_recall_curve,\n",
    "    roc_auc_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    matthews_corrcoef,\n",
    "    mean_squared_error,\n",
    "    mean_absolute_error,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "def calc_regression_metrics(preds, labels):\n",
    "    mse = mean_squared_error(labels, preds)\n",
    "    rmse = math.sqrt(mse)\n",
    "    mae = mean_absolute_error(labels, preds)\n",
    "    return {\n",
    "        \"mse\": mse,\n",
    "        \"rmse\": rmse,\n",
    "        \"mae\": mae,\n",
    "    }\n",
    "\n",
    "# def calc_classification_metrics(p: EvalPrediction):\n",
    "#     predictions = p.predictions[0]\n",
    "#     pred_labels = np.argmax(predictions, axis=1)\n",
    "#     pred_scores = softmax(predictions, axis=1)[:, 1]\n",
    "#     labels = p.label_ids\n",
    "#     if len(np.unique(labels)) == 2:  # binary classification\n",
    "#         roc_auc_pred_score = roc_auc_score(labels, pred_scores)\n",
    "#         precisions, recalls, thresholds = precision_recall_curve(labels,\n",
    "#                                                                 pred_scores)\n",
    "#         fscore = (2 * precisions * recalls) / (precisions + recalls)\n",
    "#         fscore[np.isnan(fscore)] = 0\n",
    "#         ix = np.argmax(fscore)\n",
    "#         threshold = thresholds[ix].item()\n",
    "#         pr_auc = auc(recalls, precisions)\n",
    "#         tn, fp, fn, tp = confusion_matrix(labels, pred_labels, labels=[0, 1]).ravel()\n",
    "#         result = {'roc_auc': roc_auc_pred_score,\n",
    "#                 'threshold': threshold,\n",
    "#                 'pr_auc': pr_auc,\n",
    "#                 'recall': recalls[ix].item(),\n",
    "#                 'precision': precisions[ix].item(), 'f1': fscore[ix].item(),\n",
    "#                 'tn': tn.item(), 'fp': fp.item(), 'fn': fn.item(), 'tp': tp.item()\n",
    "#                 }\n",
    "#     else:\n",
    "#         acc = (pred_labels == labels).mean()\n",
    "#         f1 = f1_score(y_true=labels, y_pred=pred_labels)\n",
    "#         result = {\n",
    "#           \"acc\": acc,\n",
    "#           \"f1\": f1,\n",
    "#           \"acc_and_f1\": (acc + f1) / 2,\n",
    "#           \"mcc\": matthews_corrcoef(labels, pred_labels)\n",
    "#       }\n",
    "\n",
    "#     return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07470d3-87f9-464b-b77c-025ff278b1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=calc_classification_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51dd6ae-33aa-4f8a-b262-1fa47be0d2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a577094-3ff4-4dd0-990d-14f332cef4fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec0b5eb-abea-42b8-bf49-d60962336a82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3feb6ee4-364c-46d8-a569-d82edbe23bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "from statistics import mean, stdev\n",
    "import sys\n",
    "from typing import Callable, Dict\n",
    "\n",
    "import numpy as np\n",
    "from pprint import pformat\n",
    "from scipy.special import softmax\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    HfArgumentParser,\n",
    "    Trainer,\n",
    "    EvalPrediction,\n",
    "    set_seed,\n",
    ")\n",
    "\n",
    "from multimodal_exp_args import (\n",
    "    MultimodalDataTrainingArguments,\n",
    "    ModelArguments,\n",
    "    OurTrainingArguments,\n",
    ")\n",
    "from evaluation import calc_classification_metrics, calc_regression_metrics\n",
    "# from multimodal_transformers.data import load_data_from_folder, load_data_into_folds\n",
    "from multimodal_transformers.model import TabularConfig\n",
    "from multimodal_transformers.model import AutoModelWithTabular\n",
    "from util import create_dir_if_not_exists, get_args_info_as_str\n",
    "\n",
    "os.environ[\"COMET_MODE\"] = \"DISABLED\"\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = HfArgumentParser(\n",
    "        (ModelArguments, MultimodalDataTrainingArguments, OurTrainingArguments)\n",
    "    )\n",
    "    if len(sys.argv) == 2 and sys.argv[1].endswith(\".json\"):\n",
    "        # If we pass only one argument to the script and it's the path to a json file,\n",
    "        # let's parse it to get our arguments.\n",
    "        model_args, data_args, training_args = parser.parse_json_file(\n",
    "            json_file=os.path.abspath(sys.argv[1])\n",
    "        )\n",
    "    else:\n",
    "        model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
    "\n",
    "    if (\n",
    "        os.path.exists(training_args.output_dir)\n",
    "        and os.listdir(training_args.output_dir)\n",
    "        and training_args.do_train\n",
    "        and not training_args.overwrite_output_dir\n",
    "    ):\n",
    "        raise ValueError(\n",
    "            f\"Output directory ({training_args.output_dir}) already exists and is not empty. Use --overwrite_output_dir to overcome.\"\n",
    "        )\n",
    "\n",
    "    # Setup logging\n",
    "    create_dir_if_not_exists(training_args.output_dir)\n",
    "    stream_handler = logging.StreamHandler(sys.stderr)\n",
    "    file_handler = logging.FileHandler(\n",
    "        filename=os.path.join(training_args.output_dir, \"train_log.txt\"), mode=\"w+\"\n",
    "    )\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        level=logging.INFO if training_args.local_rank in [-1, 0] else logging.WARN,\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        handlers=[stream_handler, file_handler],\n",
    "    )\n",
    "\n",
    "    logger.info(f\"======== Model Args ========\\n{get_args_info_as_str(model_args)}\\n\")\n",
    "    logger.info(f\"======== Data Args ========\\n{get_args_info_as_str(data_args)}\\n\")\n",
    "    logger.info(\n",
    "        f\"======== Training Args ========\\n{get_args_info_as_str(training_args)}\\n\"\n",
    "    )\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_args.tokenizer_name\n",
    "        if model_args.tokenizer_name\n",
    "        else model_args.model_name_or_path,\n",
    "        cache_dir=model_args.cache_dir,\n",
    "    )\n",
    "\n",
    "    if not data_args.create_folds:\n",
    "        train_dataset, val_dataset, test_dataset = load_data_from_folder(\n",
    "            data_args.data_path,\n",
    "            data_args.column_info[\"text_cols\"],\n",
    "            tokenizer,\n",
    "            label_col=data_args.column_info[\"label_col\"],\n",
    "            label_list=data_args.column_info[\"label_list\"],\n",
    "            categorical_cols=data_args.column_info[\"cat_cols\"],\n",
    "            numerical_cols=data_args.column_info[\"num_cols\"],\n",
    "            categorical_encode_type=data_args.categorical_encode_type,\n",
    "            numerical_transformer_method=data_args.numerical_transformer_method,\n",
    "            sep_text_token_str=tokenizer.sep_token\n",
    "            if not data_args.column_info[\"text_col_sep_token\"]\n",
    "            else data_args.column_info[\"text_col_sep_token\"],\n",
    "            max_token_length=training_args.max_token_length,\n",
    "            debug=training_args.debug_dataset,\n",
    "            debug_dataset_size=training_args.debug_dataset_size,\n",
    "        )\n",
    "        train_datasets = [train_dataset]\n",
    "        val_datasets = [val_dataset]\n",
    "        test_datasets = [test_dataset]\n",
    "    else:\n",
    "        train_datasets, val_datasets, test_datasets = load_data_into_folds(\n",
    "            data_args.data_path,\n",
    "            data_args.num_folds,\n",
    "            data_args.validation_ratio,\n",
    "            data_args.column_info[\"text_cols\"],\n",
    "            tokenizer,\n",
    "            label_col=data_args.column_info[\"label_col\"],\n",
    "            label_list=data_args.column_info[\"label_list\"],\n",
    "            categorical_cols=data_args.column_info[\"cat_cols\"],\n",
    "            numerical_cols=data_args.column_info[\"num_cols\"],\n",
    "            categorical_encode_type=data_args.categorical_encode_type,\n",
    "            numerical_transformer_method=data_args.numerical_transformer_method,\n",
    "            sep_text_token_str=tokenizer.sep_token\n",
    "            if not data_args.column_info[\"text_col_sep_token\"]\n",
    "            else data_args.column_info[\"text_col_sep_token\"],\n",
    "            max_token_length=training_args.max_token_length,\n",
    "            debug=training_args.debug_dataset,\n",
    "            debug_dataset_size=training_args.debug_dataset_size,\n",
    "        )\n",
    "    train_dataset = train_datasets[0]\n",
    "\n",
    "    set_seed(training_args.seed)\n",
    "    task = data_args.task\n",
    "    if task == \"regression\":\n",
    "        num_labels = 1\n",
    "    else:\n",
    "        num_labels = (\n",
    "            len(np.unique(train_dataset.labels))\n",
    "            if data_args.num_classes == -1\n",
    "            else data_args.num_classes\n",
    "        )\n",
    "\n",
    "    def build_compute_metrics_fn(task_name: str) -> Callable[[EvalPrediction], Dict]:\n",
    "        def compute_metrics_fn(p: EvalPrediction):\n",
    "            # p.predictions is now a list of objects\n",
    "            # The first entry is the actual predictions\n",
    "            predictions = p.predictions[0]\n",
    "            if task_name == \"classification\":\n",
    "                preds_labels = np.argmax(predictions, axis=1)\n",
    "                if predictions.shape[-1] == 2:\n",
    "                    pred_scores = softmax(predictions, axis=1)[:, 1]\n",
    "                else:\n",
    "                    pred_scores = softmax(predictions, axis=1)\n",
    "                return calc_classification_metrics(\n",
    "                    pred_scores, preds_labels, p.label_ids\n",
    "                )\n",
    "            elif task_name == \"regression\":\n",
    "                preds = np.squeeze(predictions)\n",
    "                return calc_regression_metrics(preds, p.label_ids)\n",
    "            else:\n",
    "                return {}\n",
    "\n",
    "        return compute_metrics_fn\n",
    "\n",
    "    total_results = []\n",
    "    for i, (train_dataset, val_dataset, test_dataset) in enumerate(\n",
    "        zip(train_datasets, val_datasets, test_datasets)\n",
    "    ):\n",
    "        logger.info(f\"======== Fold {i+1} ========\")\n",
    "        config = AutoConfig.from_pretrained(\n",
    "            model_args.config_name\n",
    "            if model_args.config_name\n",
    "            else model_args.model_name_or_path,\n",
    "            cache_dir=model_args.cache_dir,\n",
    "        )\n",
    "        tabular_config = TabularConfig(\n",
    "            num_labels=num_labels,\n",
    "            cat_feat_dim=train_dataset.cat_feats.shape[1]\n",
    "            if train_dataset.cat_feats is not None\n",
    "            else 0,\n",
    "            numerical_feat_dim=train_dataset.numerical_feats.shape[1]\n",
    "            if train_dataset.numerical_feats is not None\n",
    "            else 0,\n",
    "            **vars(data_args),\n",
    "        )\n",
    "        config.tabular_config = tabular_config\n",
    "\n",
    "        model = AutoModelWithTabular.from_pretrained(\n",
    "            model_args.config_name\n",
    "            if model_args.config_name\n",
    "            else model_args.model_name_or_path,\n",
    "            config=config,\n",
    "            cache_dir=model_args.cache_dir,\n",
    "        )\n",
    "        if i == 0:\n",
    "            logger.info(tabular_config)\n",
    "            logger.info(model)\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "            compute_metrics=build_compute_metrics_fn(task),\n",
    "        )\n",
    "        if training_args.do_train:\n",
    "            trainer.train(\n",
    "                model_path=model_args.model_name_or_path\n",
    "                if os.path.isdir(model_args.model_name_or_path)\n",
    "                else None\n",
    "            )\n",
    "            trainer.save_model()\n",
    "\n",
    "        # Evaluation\n",
    "        eval_results = {}\n",
    "        if training_args.do_eval:\n",
    "            logger.info(\"*** Evaluate ***\")\n",
    "            eval_result = trainer.evaluate(eval_dataset=val_dataset)\n",
    "            logger.info(pformat(eval_result, indent=4))\n",
    "\n",
    "            output_eval_file = os.path.join(\n",
    "                training_args.output_dir, f\"eval_metric_results_{task}_fold_{i+1}.txt\"\n",
    "            )\n",
    "            if trainer.is_world_process_zero():\n",
    "                with open(output_eval_file, \"w\") as writer:\n",
    "                    logger.info(\"***** Eval results {} *****\".format(task))\n",
    "                    for key, value in eval_result.items():\n",
    "                        logger.info(\"  %s = %s\", key, value)\n",
    "                        writer.write(\"%s = %s\\n\" % (key, value))\n",
    "\n",
    "            eval_results.update(eval_result)\n",
    "\n",
    "        if training_args.do_predict:\n",
    "            logging.info(\"*** Test ***\")\n",
    "\n",
    "            predictions = trainer.predict(test_dataset=test_dataset).predictions[0]\n",
    "            output_test_file = os.path.join(\n",
    "                training_args.output_dir, f\"test_results_{task}_fold_{i+1}.txt\"\n",
    "            )\n",
    "            eval_result = trainer.evaluate(eval_dataset=test_dataset)\n",
    "            logger.info(pformat(eval_result, indent=4))\n",
    "            if trainer.is_world_process_zero():\n",
    "                with open(output_test_file, \"w\") as writer:\n",
    "                    logger.info(\"***** Test results {} *****\".format(task))\n",
    "                    writer.write(\"index\\tprediction\\n\")\n",
    "                    if task == \"classification\":\n",
    "                        predictions = np.argmax(predictions, axis=1)\n",
    "                    for index, item in enumerate(predictions):\n",
    "                        if task == \"regression\":\n",
    "                            writer.write(\n",
    "                                \"%d\\t%3.3f\\t%d\\n\"\n",
    "                                % (index, item, test_dataset.labels[index])\n",
    "                            )\n",
    "                        else:\n",
    "                            item = test_dataset.get_labels()[item]\n",
    "                            writer.write(\"%d\\t%s\\n\" % (index, item))\n",
    "                output_test_file = os.path.join(\n",
    "                    training_args.output_dir,\n",
    "                    f\"test_metric_results_{task}_fold_{i+1}.txt\",\n",
    "                )\n",
    "                with open(output_test_file, \"w\") as writer:\n",
    "                    logger.info(\"***** Test results {} *****\".format(task))\n",
    "                    for key, value in eval_result.items():\n",
    "                        logger.info(\"  %s = %s\", key, value)\n",
    "                        writer.write(\"%s = %s\\n\" % (key, value))\n",
    "                eval_results.update(eval_result)\n",
    "        del model\n",
    "        del config\n",
    "        del tabular_config\n",
    "        del trainer\n",
    "        torch.cuda.empty_cache()\n",
    "        total_results.append(eval_results)\n",
    "    aggr_res = aggregate_results(total_results)\n",
    "    logger.info(\"========= Aggr Results ========\")\n",
    "    logger.info(pformat(aggr_res, indent=4))\n",
    "\n",
    "    output_aggre_test_file = os.path.join(\n",
    "        training_args.output_dir, f\"all_test_metric_results_{task}.txt\"\n",
    "    )\n",
    "    with open(output_aggre_test_file, \"w\") as writer:\n",
    "        logger.info(\"***** Aggr results {} *****\".format(task))\n",
    "        for key, value in aggr_res.items():\n",
    "            logger.info(\"  %s = %s\", key, value)\n",
    "            writer.write(\"%s = %s\\n\" % (key, value))\n",
    "\n",
    "\n",
    "def aggregate_results(total_test_results):\n",
    "    metric_keys = list(total_test_results[0].keys())\n",
    "    aggr_results = dict()\n",
    "\n",
    "    for metric_name in metric_keys:\n",
    "        if type(total_test_results[0][metric_name]) is str:\n",
    "            continue\n",
    "        res_list = []\n",
    "        for results in total_test_results:\n",
    "            res_list.append(results[metric_name])\n",
    "        if len(res_list) == 1:\n",
    "            metric_avg = res_list[0]\n",
    "            metric_stdev = 0\n",
    "        else:\n",
    "            metric_avg = mean(res_list)\n",
    "            metric_stdev = stdev(res_list)\n",
    "\n",
    "        aggr_results[metric_name + \"_mean\"] = metric_avg\n",
    "        aggr_results[metric_name + \"_stdev\"] = metric_stdev\n",
    "    return aggr_results\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb99e28-e837-4a60-9aa1-47922e243cce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da595e46-4557-4806-93b8-36e58878a230",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038f9c70-494e-49e2-908a-83d7262960a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ebbb64-af73-45f3-b5c9-de46a6ec0ea7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

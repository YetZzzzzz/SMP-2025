{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d65fcf4b-20b7-4f09-967f-7e0b486c86c6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "## This script is borrowed from https://github.com/ryancheunggit/Denoise-Transformer-AutoEncoder\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import json\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '4,5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d2e7c6b-6646-4b34-830c-c7de62bc2644",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_data():\n",
    "    all_data = pd.read_csv('../data/feature_data_530.csv')\n",
    "    # glove\n",
    "    # glove_tags = pd.read_csv('../data/alltags_feature.csv')\n",
    "    # glove_title = pd.read_csv('../data/title_feature.csv')\n",
    "    # glove_title = pd.read_csv('../data/title_feature.csv')\n",
    "    # all_data = pd.concat([all_data, glove_tags, glove_title], axis=1)\n",
    "    columns = ['Title_len', 'Title_number', 'Alltags_len', 'Alltags_number', 'photo_count', 'totalTags', 'totalGeotagged', 'totalFaves',\n",
    "              'totalInGroup','photoCount','meanView', 'meanTags', 'meanFaves', 'followerCount','followingCount']\n",
    "    skew_features = all_data[columns].apply(lambda x: skew(x)).sort_values(ascending=False)\n",
    "    high_skew = skew_features[abs(skew_features) > 0.75]\n",
    "    skew_index = high_skew.index\n",
    "    for i in skew_index:\n",
    "        all_data[i] = np.log1p(all_data[i])\n",
    "        \n",
    "    useless_columns = ['Pid','mean_label','train_type', 'Uid_count'] \n",
    "    useless_columns += ['user_fe_{}'.format(i) for i in range(399)]\n",
    "    useless_columns += ['loc_fe_{}'.format(i) for i in range(400)]\n",
    "    all_data = all_data.drop(useless_columns, axis=1)\n",
    "    \n",
    "    train_data = all_data[:-180581]\n",
    "    test_data = all_data[-180581:]\n",
    "    # train_all_data = all_data[all_data['train_type'] != -1]\n",
    "    # submit_all_data = all_data[all_data['train_type'] == -1]\n",
    "    # feature_columns = ['label']\n",
    "    # all_data = all_data.drop(feature_columns, axis=1)\n",
    "    \n",
    "    train_data = train_data.reset_index(drop=True)\n",
    "    test_data = test_data.reset_index(drop=True)\n",
    "    CAT_COLS = ['Uid', 'Category', 'Subcategory', 'Concept', 'Mediatype', 'hour', 'day', 'weekday', 'week_hour', 'year_weekday','Geoaccuracy', 'ispro' , 'Ispublic']\n",
    "    NUM_COLS = [c for c in train_data.columns if c not in CAT_COLS]\n",
    "    \n",
    "    X_nums = np.vstack([\n",
    "        train_data[NUM_COLS].to_numpy(),\n",
    "        test_data[NUM_COLS].to_numpy()\n",
    "    ])\n",
    "    X_nums = (X_nums - X_nums.mean(0)) / X_nums.std(0)\n",
    "\n",
    "    X_cat = np.vstack([\n",
    "        train_data[CAT_COLS].to_numpy(),\n",
    "        test_data[CAT_COLS].to_numpy()\n",
    "    ])\n",
    "    encoder = OneHotEncoder(sparse=False)\n",
    "    X_cat = encoder.fit_transform(X_cat)\n",
    "\n",
    "    X = np.hstack([X_cat, X_nums])\n",
    "    y = train_data['label'].to_numpy().reshape(-1, 1)\n",
    "    return X, y, X_cat.shape[1], X_nums.shape[1]\n",
    "\n",
    "\n",
    "class SingleDataset(Dataset):\n",
    "    def __init__(self, x, is_sparse=False):\n",
    "        self.x = x.astype('float32')\n",
    "        self.is_sparse = is_sparse\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.x[index]\n",
    "        if self.is_sparse: x = x.toarray().squeeze()\n",
    "        return x   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4aeebba5-65cb-455d-8724-430316ccb250",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val, self.avg, self.sum, self.count = 0, 0, 0, 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a177f3b-7840-48e8-818a-824204809bd7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "specified :  [0.2, 0.5, 0.8]  - actual :  tensor([0.1921, 0.4843, 0.7784])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "bce_logits = torch.nn.functional.binary_cross_entropy_with_logits\n",
    "mse = torch.nn.functional.mse_loss\n",
    "\n",
    "\n",
    "class TransformerEncoder(torch.nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, dropout, feedforward_dim):\n",
    "        super().__init__()\n",
    "        self.attn = torch.nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout)\n",
    "        self.linear_1 = torch.nn.Linear(embed_dim, feedforward_dim)\n",
    "        self.linear_2 = torch.nn.Linear(feedforward_dim, embed_dim)\n",
    "        self.layernorm_1 = torch.nn.LayerNorm(embed_dim)\n",
    "        self.layernorm_2 = torch.nn.LayerNorm(embed_dim)\n",
    "    \n",
    "    def forward(self, x_in):\n",
    "        attn_out, _ = self.attn(x_in, x_in, x_in)\n",
    "        x = self.layernorm_1(x_in + attn_out)\n",
    "        ff_out = self.linear_2(torch.nn.functional.relu(self.linear_1(x)))\n",
    "        x = self.layernorm_2(x + ff_out)\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerAutoEncoder(torch.nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            num_inputs, \n",
    "            n_cats, \n",
    "            n_nums, \n",
    "            hidden_size=1024, \n",
    "            num_subspaces=8,\n",
    "            embed_dim=128, \n",
    "            num_heads=8, \n",
    "            dropout=0, \n",
    "            feedforward_dim=512, \n",
    "            emphasis=.75, \n",
    "            task_weights=[10, 14],\n",
    "            mask_loss_weight=2,\n",
    "        ):\n",
    "        super().__init__()\n",
    "        assert hidden_size == embed_dim * num_subspaces\n",
    "        self.n_cats = n_cats\n",
    "        self.n_nums = n_nums\n",
    "        self.num_subspaces = num_subspaces\n",
    "        self.num_heads = num_heads\n",
    "        self.embed_dim = embed_dim\n",
    "        self.emphasis = emphasis\n",
    "        self.task_weights = np.array(task_weights) / sum(task_weights)\n",
    "        self.mask_loss_weight = mask_loss_weight\n",
    "\n",
    "        self.excite = torch.nn.Linear(in_features=num_inputs, out_features=hidden_size)\n",
    "        self.encoder_1 = TransformerEncoder(embed_dim, num_heads, dropout, feedforward_dim)\n",
    "        self.encoder_2 = TransformerEncoder(embed_dim, num_heads, dropout, feedforward_dim)\n",
    "        self.encoder_3 = TransformerEncoder(embed_dim, num_heads, dropout, feedforward_dim)\n",
    "        \n",
    "        self.mask_predictor = torch.nn.Linear(in_features=hidden_size, out_features=num_inputs)\n",
    "        self.reconstructor = torch.nn.Linear(in_features=hidden_size + num_inputs, out_features=num_inputs)\n",
    "\n",
    "    def divide(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        x = x.reshape((batch_size, self.num_subspaces, self.embed_dim)).permute((1, 0, 2))\n",
    "        return x\n",
    "\n",
    "    def combine(self, x):\n",
    "        batch_size = x.shape[1]\n",
    "        x = x.permute((1, 0, 2)).reshape((batch_size, -1))\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.nn.functional.relu(self.excite(x))\n",
    "        \n",
    "        x = self.divide(x)\n",
    "        x1 = self.encoder_1(x)\n",
    "        x2 = self.encoder_2(x1)\n",
    "        x3 = self.encoder_3(x2)\n",
    "        x = self.combine(x3)\n",
    "        \n",
    "        predicted_mask = self.mask_predictor(x)\n",
    "        reconstruction = self.reconstructor(torch.cat([x, predicted_mask], dim=1))\n",
    "        return (x1, x2, x3), (reconstruction, predicted_mask)\n",
    "\n",
    "    def split(self, t):\n",
    "        return torch.split(t, [self.n_cats, self.n_nums], dim=1)\n",
    "\n",
    "    def feature(self, x):\n",
    "        attn_outs, _ = self.forward(x)\n",
    "        return torch.cat([self.combine(x) for x in attn_outs], dim=1)\n",
    "\n",
    "    def loss(self, x, y, mask, reduction='mean'):\n",
    "        _, (reconstruction, predicted_mask) = self.forward(x)\n",
    "        x_cats, x_nums = self.split(reconstruction)\n",
    "        y_cats, y_nums = self.split(y)\n",
    "        w_cats, w_nums = self.split(mask * self.emphasis + (1 - mask) * (1 - self.emphasis))\n",
    "\n",
    "        cat_loss = self.task_weights[0] * torch.mul(w_cats, bce_logits(x_cats, y_cats, reduction='none'))\n",
    "        num_loss = self.task_weights[1] * torch.mul(w_nums, mse(x_nums, y_nums, reduction='none'))\n",
    "\n",
    "        reconstruction_loss = torch.cat([cat_loss, num_loss], dim=1) if reduction == 'none' else cat_loss.mean() + num_loss.mean()\n",
    "        mask_loss = self.mask_loss_weight * bce_logits(predicted_mask, mask, reduction=reduction)\n",
    "\n",
    "        return reconstruction_loss + mask_loss if reduction == 'mean' else [reconstruction_loss, mask_loss]\n",
    "\n",
    "\n",
    "class SwapNoiseMasker(object):\n",
    "    def __init__(self, probas):\n",
    "        self.probas = torch.from_numpy(np.array(probas))\n",
    "\n",
    "    def apply(self, X):\n",
    "        should_swap = torch.bernoulli(self.probas.to(X.device) * torch.ones((X.shape)).to(X.device))\n",
    "        corrupted_X = torch.where(should_swap == 1, X[torch.randperm(X.shape[0])], X)\n",
    "        mask = (corrupted_X != X).float()\n",
    "        return corrupted_X, mask\n",
    "\n",
    "\n",
    "def test_tf_encoder():\n",
    "    m = TransformerEncoder(4, 2, .1, 16)\n",
    "    x = torch.rand((32, 8))\n",
    "    x = x.reshape((32, 2, 4)).permute((1, 0, 2))\n",
    "    o = m(x)\n",
    "    assert o.shape == torch.Size([2, 32, 4])\n",
    "\n",
    "\n",
    "def test_dae_model():\n",
    "    m = TransformerAutoEncoder(5, 2, 3, 16, 4, 4, 2, .1, 4, .75)\n",
    "    x = torch.cat([torch.randint(0, 2, (5, 2)), torch.rand((5, 3))], dim=1)\n",
    "    f = m.feature(x)\n",
    "    assert f.shape == torch.Size([5, 16 * 3])\n",
    "    loss = m.loss(x, x, (x > .2).float())\n",
    "\n",
    "\n",
    "def test_swap_noise():\n",
    "    probas = [.2, .5, .8]\n",
    "    m = SwapNoiseMasker(probas)\n",
    "    diffs = []\n",
    "    for i in range(1000):\n",
    "        x = torch.rand((32, 3))\n",
    "        noisy_x, _ = m.apply(x)\n",
    "        diffs.append((x != noisy_x).float().mean(0).unsqueeze(0)) \n",
    "\n",
    "    print('specified : ', probas, ' - actual : ', torch.cat(diffs, 0).mean(0))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test_tf_encoder()\n",
    "    test_dae_model()\n",
    "    test_swap_noise()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc1379a-fa0e-4cc2-ba8b-ba368022f2e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "# from util import AverageMeter\n",
    "# from model import SwapNoiseMasker, TransformerAutoEncoder\n",
    "# from data import get_data, SingleDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.metrics import f1_score, mean_absolute_error, mean_squared_error\n",
    "from scipy import stats\n",
    "from scipy.stats import norm, skew\n",
    "\n",
    "# Hyper-params\n",
    "model_params = dict(\n",
    "    hidden_size=1024,\n",
    "    num_subspaces=8,\n",
    "    embed_dim=128,\n",
    "    num_heads=8,\n",
    "    dropout=0,\n",
    "    feedforward_dim=512,\n",
    "    emphasis=.75,\n",
    "    mask_loss_weight=2\n",
    ")\n",
    "batch_size = 128\n",
    "init_lr = 3e-4\n",
    "lr_decay = .998\n",
    "max_epochs = 2001\n",
    "\n",
    "repeats = [  2,  2,  2,  4,  4,  4,  8,  8,  7, 15,  14]\n",
    "probas =  [.95, .4, .7, .9, .9, .9, .9, .9, .9, .9, .25]\n",
    "swap_probas = sum([[p] * r for p, r in zip(probas, repeats)], [])\n",
    "\n",
    "#  get data\n",
    "X, Y, n_cats, n_nums = get_data()\n",
    "\n",
    "train_dl = DataLoader(\n",
    "    dataset=SingleDataset(X),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "# setup model\n",
    "model = TransformerAutoEncoder(\n",
    "    num_inputs=X.shape[1],\n",
    "    n_cats=n_cats,\n",
    "    n_nums=n_nums,\n",
    "    **model_params\n",
    ").cuda()\n",
    "model_checkpoint = 'model_checkpoint.pth'\n",
    "\n",
    "print(model)\n",
    "\n",
    "noise_maker = SwapNoiseMasker(swap_probas)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=init_lr)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=lr_decay)\n",
    "\n",
    "# train model\n",
    "for epoch in range(max_epochs):\n",
    "    t0 = datetime.now()\n",
    "    model.train()\n",
    "    meter = AverageMeter()\n",
    "    for i, x in enumerate(train_dl):\n",
    "        x = x.cuda()\n",
    "        x_corrputed, mask = noise_maker.apply(x)\n",
    "        optimizer.zero_grad()\n",
    "        loss = model.loss(x_corrputed, x, mask)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        meter.update(loss.detach().cpu().numpy())\n",
    "\n",
    "    delta = (datetime.now() - t0).seconds\n",
    "    scheduler.step()\n",
    "    print('\\r epoch {:5d} - loss {:.6f} - {:4.6f} sec per epoch'.format(epoch, meter.avg, delta), end='')\n",
    "\n",
    "torch.save({\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "        \"scheduler\": scheduler.state_dict(),\n",
    "        \"model\": model.state_dict()\n",
    "    }, model_checkpoint\n",
    ")\n",
    "model_state = torch.load(model_checkpoint)\n",
    "model.load_state_dict(model_state['model'])\n",
    "\n",
    "# extract features\n",
    "dl = DataLoader(dataset=SingleDataset(X), batch_size=1024, shuffle=False, pin_memory=True, drop_last=False)\n",
    "features = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for x in dl:\n",
    "        features.append(model.feature(x.cuda()).detach().cpu().numpy())\n",
    "features = np.vstack(features)\n",
    "\n",
    "# downstream supervised regressor\n",
    "alpha = 1250 # 1000\n",
    "X = features[:300_000, :]\n",
    "scores = []\n",
    "for train_idx, valid_idx in KFold().split(X, Y):\n",
    "    scores.append(mean_squared_error(Y[valid_idx], Ridge(alpha=1250).fit(X[train_idx], Y[train_idx]).predict(X[valid_idx]), squared=False))\n",
    "print(np.mean(scores))\n",
    "\n",
    "np.save('dae_features.npy', features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0db3fa9-bed7-4547-84f0-4c57a180703b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b97d75-9ebb-46dd-ba7a-caf2417dc4d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

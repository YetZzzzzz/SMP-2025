{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4161c1dc-8b49-4036-b510-f7e0b91a3339",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>excerpt</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rock punk transgender tranny electronicmusic e...</td>\n",
       "      <td>11.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>brazil rio brasil riodejaneiro by maria fifa m...</td>\n",
       "      <td>15.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>old cinema beauty marilyn photoshop movie joke...</td>\n",
       "      <td>10.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pictures old family scans brothers sister 1958...</td>\n",
       "      <td>8.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hot sahara animal animals desert bottles drink...</td>\n",
       "      <td>11.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486189</th>\n",
       "      <td>natur nature frog sea lake fish outdoor macro ...</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486190</th>\n",
       "      <td>11 arisk okruh veternov 2016 10 jn preov veter...</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486191</th>\n",
       "      <td>holland netherlands amsterdam cat nl ramses gu...</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486192</th>\n",
       "      <td>street people bw white black netherlands monoc...</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486193</th>\n",
       "      <td>flower nature fleur extrieure</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>486194 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  excerpt  target\n",
       "0       rock punk transgender tranny electronicmusic e...   11.18\n",
       "1       brazil rio brasil riodejaneiro by maria fifa m...   15.15\n",
       "2       old cinema beauty marilyn photoshop movie joke...   10.99\n",
       "3       pictures old family scans brothers sister 1958...    8.63\n",
       "4       hot sahara animal animals desert bottles drink...   11.16\n",
       "...                                                   ...     ...\n",
       "486189  natur nature frog sea lake fish outdoor macro ...    0.00\n",
       "486190  11 arisk okruh veternov 2016 10 jn preov veter...    0.00\n",
       "486191  holland netherlands amsterdam cat nl ramses gu...    0.00\n",
       "486192  street people bw white black netherlands monoc...    0.00\n",
       "486193                      flower nature fleur extrieure    0.00\n",
       "\n",
       "[486194 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "all_data = pd.read_csv('../data/forauto.csv')\n",
    "alltags = all_data[['Alltags','label']]\n",
    "alltags.columns = ['excerpt', 'target']\n",
    "train = alltags[:-180581]\n",
    "test = alltags[-180581:]\n",
    "alltags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11a7f26a-d693-4810-9219-a94c83279d1c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8246/3370286601.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[\"kfold\"] = -1\n"
     ]
    }
   ],
   "source": [
    "from sklearn import model_selection\n",
    "import os \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '4,5'\n",
    "def create_folds(data, num_splits):\n",
    "    data[\"kfold\"] = -1\n",
    "    kf = model_selection.KFold(n_splits=num_splits, shuffle=True, random_state=2020)\n",
    "    for f, (t_, v_) in enumerate(kf.split(X=data)):\n",
    "        data.loc[v_, 'kfold'] = f\n",
    "    return data\n",
    "train = create_folds(train, num_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67ed8495-9a3c-4038-b95e-4cca21439561",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from glob import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import gc\n",
    "gc.enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8befaba-d414-4b58-8cd3-5e3cf1d18881",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.optimizer import Optimizer\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.data import (\n",
    "    Dataset, DataLoader, \n",
    "    SequentialSampler, RandomSampler\n",
    ")\n",
    "from transformers import AutoConfig\n",
    "from transformers import (\n",
    "    get_cosine_schedule_with_warmup, \n",
    "    get_cosine_with_hard_restarts_schedule_with_warmup,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from transformers import AdamW\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModel\n",
    "from transformers import MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING\n",
    "from IPython.display import clear_output\n",
    "from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2469037c-9769-4ab2-93bf-2ecda6518bc4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_examples_to_features(data, tokenizer, max_len, is_test=False):\n",
    "    data = data.replace('\\n', '')\n",
    "    tok = tokenizer.encode_plus(\n",
    "        data, \n",
    "        max_length=max_len, \n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_token_type_ids=True\n",
    "    )\n",
    "    curr_sent = {}\n",
    "    padding_length = max_len - len(tok['input_ids'])\n",
    "    curr_sent['input_ids'] = tok['input_ids'] + ([tokenizer.pad_token_id] * padding_length)\n",
    "    curr_sent['token_type_ids'] = tok['token_type_ids'] + \\\n",
    "        ([0] * padding_length)\n",
    "    curr_sent['attention_mask'] = tok['attention_mask'] + \\\n",
    "        ([0] * padding_length)\n",
    "    return curr_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "525d0498-567b-44ba-8424-62312c52edc6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DatasetRetriever(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_len, is_test=False):\n",
    "        self.data = data\n",
    "        if 'excerpt' in self.data.columns:\n",
    "            self.excerpts = self.data.excerpt.values.tolist()\n",
    "        else:\n",
    "            self.excerpts = self.data.text.values.tolist()\n",
    "        self.targets = self.data.target.values.tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.is_test = is_test\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        excerpt, label = self.excerpts[item], self.targets[item]\n",
    "        features = convert_examples_to_features(\n",
    "            excerpt, self.tokenizer, \n",
    "            self.max_len, self.is_test\n",
    "        )\n",
    "        return {\n",
    "            'input_ids':torch.tensor(features['input_ids'], dtype=torch.long),\n",
    "            'token_type_ids':torch.tensor(features['token_type_ids'], dtype=torch.long),\n",
    "            'attention_mask':torch.tensor(features['attention_mask'], dtype=torch.long),\n",
    "            'label':torch.tensor(label, dtype=torch.double),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49701510-c872-4d33-9ff2-a6155bdc8614",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CommonLitModel(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        model_name, \n",
    "        config,  \n",
    "        multisample_dropout=False,\n",
    "        output_hidden_states=False\n",
    "    ):\n",
    "        super(CommonLitModel, self).__init__()\n",
    "        self.config = config\n",
    "        self.roberta = AutoModel.from_pretrained(\n",
    "            model_name, \n",
    "            output_hidden_states=output_hidden_states\n",
    "        )\n",
    "        self.layer_norm = nn.LayerNorm(config.hidden_size)\n",
    "        if multisample_dropout:\n",
    "            self.dropouts = nn.ModuleList([\n",
    "                nn.Dropout(0.5) for _ in range(5)\n",
    "            ])\n",
    "        else:\n",
    "            self.dropouts = nn.ModuleList([nn.Dropout(0.3)])\n",
    "        #self.regressor = nn.Linear(config.hidden_size*2, 1)\n",
    "        self.regressor = nn.Linear(config.hidden_size, 1)\n",
    "        self._init_weights(self.layer_norm)\n",
    "        self._init_weights(self.regressor)\n",
    " \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    " \n",
    "    def forward(\n",
    "        self, \n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        labels=None\n",
    "    ):\n",
    "        outputs = self.roberta(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "        )\n",
    "        sequence_output = outputs[1]\n",
    "        sequence_output = self.layer_norm(sequence_output)\n",
    " \n",
    "        # max-avg head\n",
    "        # average_pool = torch.mean(sequence_output, 1)\n",
    "        # max_pool, _ = torch.max(sequence_output, 1)\n",
    "        # concat_sequence_output = torch.cat((average_pool, max_pool), 1)\n",
    " \n",
    "        # multi-sample dropout\n",
    "        for i, dropout in enumerate(self.dropouts):\n",
    "            if i == 0:\n",
    "                logits = self.regressor(dropout(sequence_output))\n",
    "            else:\n",
    "                logits += self.regressor(dropout(sequence_output))\n",
    "        \n",
    "        logits /= len(self.dropouts)\n",
    " \n",
    "        # calculate loss\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # regression task\n",
    "            loss_fn = torch.nn.MSELoss()\n",
    "            logits = logits.view(-1).to(labels.dtype)\n",
    "            loss = torch.sqrt(loss_fn(logits, labels.view(-1)))\n",
    "        \n",
    "        output = (logits,) + outputs[2:]\n",
    "        return ((loss,) + output) if loss is not None else output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b98deaaa-8a26-4ccc-88d2-4183f52baa26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Lamb(Optimizer):\n",
    "    # Reference code: https://github.com/cybertronai/pytorch-lamb\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        params,\n",
    "        lr: float = 1e-3,\n",
    "        betas = (0.9, 0.999),\n",
    "        eps: float = 1e-6,\n",
    "        weight_decay: float = 0,\n",
    "        clamp_value: float = 10,\n",
    "        adam: bool = False,\n",
    "        debias: bool = False,\n",
    "    ):\n",
    "        if lr <= 0.0:\n",
    "            raise ValueError('Invalid learning rate: {}'.format(lr))\n",
    "        if eps < 0.0:\n",
    "            raise ValueError('Invalid epsilon value: {}'.format(eps))\n",
    "        if not 0.0 <= betas[0] < 1.0:\n",
    "            raise ValueError(\n",
    "                'Invalid beta parameter at index 0: {}'.format(betas[0])\n",
    "            )\n",
    "        if not 0.0 <= betas[1] < 1.0:\n",
    "            raise ValueError(\n",
    "                'Invalid beta parameter at index 1: {}'.format(betas[1])\n",
    "            )\n",
    "        if weight_decay < 0:\n",
    "            raise ValueError(\n",
    "                'Invalid weight_decay value: {}'.format(weight_decay)\n",
    "            )\n",
    "        if clamp_value < 0.0:\n",
    "            raise ValueError('Invalid clamp value: {}'.format(clamp_value))\n",
    "\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
    "        self.clamp_value = clamp_value\n",
    "        self.adam = adam\n",
    "        self.debias = debias\n",
    "\n",
    "        super(Lamb, self).__init__(params, defaults)\n",
    "\n",
    "    def step(self, closure = None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data\n",
    "                if grad.is_sparse:\n",
    "                    msg = (\n",
    "                        'Lamb does not support sparse gradients, '\n",
    "                        'please consider SparseAdam instead'\n",
    "                    )\n",
    "                    raise RuntimeError(msg)\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                # State initialization\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    # Exponential moving average of gradient values\n",
    "                    state['exp_avg'] = torch.zeros_like(\n",
    "                        p, memory_format=torch.preserve_format\n",
    "                    )\n",
    "                    # Exponential moving average of squared gradient values\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(\n",
    "                        p, memory_format=torch.preserve_format\n",
    "                    )\n",
    "\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "\n",
    "                state['step'] += 1\n",
    "\n",
    "                # Decay the first and second moment running average coefficient\n",
    "                # m_t\n",
    "                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n",
    "                # v_t\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n",
    "\n",
    "                # Paper v3 does not use debiasing.\n",
    "                if self.debias:\n",
    "                    bias_correction = math.sqrt(1 - beta2 ** state['step'])\n",
    "                    bias_correction /= 1 - beta1 ** state['step']\n",
    "                else:\n",
    "                    bias_correction = 1\n",
    "\n",
    "                # Apply bias to lr to avoid broadcast.\n",
    "                step_size = group['lr'] * bias_correction\n",
    "\n",
    "                weight_norm = torch.norm(p.data).clamp(0, self.clamp_value)\n",
    "\n",
    "                adam_step = exp_avg / exp_avg_sq.sqrt().add(group['eps'])\n",
    "                if group['weight_decay'] != 0:\n",
    "                    adam_step.add_(p.data, alpha=group['weight_decay'])\n",
    "\n",
    "                adam_norm = torch.norm(adam_step)\n",
    "                if weight_norm == 0 or adam_norm == 0:\n",
    "                    trust_ratio = 1\n",
    "                else:\n",
    "                    trust_ratio = weight_norm / adam_norm\n",
    "                state['weight_norm'] = weight_norm\n",
    "                state['adam_norm'] = adam_norm\n",
    "                state['trust_ratio'] = trust_ratio\n",
    "                if self.adam:\n",
    "                    trust_ratio = 1\n",
    "\n",
    "                p.data.add_(adam_step, alpha=-step_size * trust_ratio)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e25ba36-6a85-431c-af13-2b36c88d1867",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_optimizer_params(model):\n",
    "    # differential learning rate and weight decay\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    learning_rate = 5e-5\n",
    "    no_decay = ['bias', 'gamma', 'beta']\n",
    "    group1=['layer.0.','layer.1.','layer.2.','layer.3.']\n",
    "    group2=['layer.4.','layer.5.','layer.6.','layer.7.']    \n",
    "    group3=['layer.8.','layer.9.','layer.10.','layer.11.']\n",
    "    group_all=['layer.0.','layer.1.','layer.2.','layer.3.','layer.4.','layer.5.','layer.6.','layer.7.','layer.8.','layer.9.','layer.10.','layer.11.']\n",
    "    optimizer_parameters = [\n",
    "        {'params': [p for n, p in model.roberta.named_parameters() if not any(nd in n for nd in no_decay) and not any(nd in n for nd in group_all)],'weight_decay': 0.01},\n",
    "        {'params': [p for n, p in model.roberta.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group1)],'weight_decay': 0.01, 'lr': learning_rate/2.6},\n",
    "        {'params': [p for n, p in model.roberta.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group2)],'weight_decay': 0.01, 'lr': learning_rate},\n",
    "        {'params': [p for n, p in model.roberta.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group3)],'weight_decay': 0.01, 'lr': learning_rate*2.6},\n",
    "        {'params': [p for n, p in model.roberta.named_parameters() if any(nd in n for nd in no_decay) and not any(nd in n for nd in group_all)],'weight_decay': 0.0},\n",
    "        {'params': [p for n, p in model.roberta.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group1)],'weight_decay': 0.0, 'lr': learning_rate/2.6},\n",
    "        {'params': [p for n, p in model.roberta.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group2)],'weight_decay': 0.0, 'lr': learning_rate},\n",
    "        {'params': [p for n, p in model.roberta.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group3)],'weight_decay': 0.0, 'lr': learning_rate*2.6},\n",
    "        {'params': [p for n, p in model.named_parameters() if \"roberta\" not in n], 'lr':1e-3, \"momentum\" : 0.99},\n",
    "    ]\n",
    "    return optimizer_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef194bc0-3075-43d8-9fff-fa113cc93e2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_model(model_name='../output/', num_labels=1):\n",
    "    tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
    "    config = AutoConfig.from_pretrained(model_name)\n",
    "    config.update({'num_labels':num_labels})\n",
    "    model = CommonLitModel(model_name, config=config)\n",
    "    return model, tokenizer\n",
    "\n",
    "def make_optimizer(model, optimizer_name=\"AdamW\"):\n",
    "    optimizer_grouped_parameters = get_optimizer_params(model)\n",
    "    kwargs = {\n",
    "            'lr':5e-5,\n",
    "            'weight_decay':0.01,\n",
    "            # 'betas': (0.9, 0.98),\n",
    "            # 'eps': 1e-06\n",
    "    }\n",
    "    if optimizer_name == \"LAMB\":\n",
    "        optimizer = Lamb(optimizer_grouped_parameters, **kwargs)\n",
    "        return optimizer\n",
    "    elif optimizer_name == \"Adam\":\n",
    "        from torch.optim import Adam\n",
    "        optimizer = Adam(optimizer_grouped_parameters, **kwargs)\n",
    "        return optimizer\n",
    "    elif optimizer_name == \"AdamW\":\n",
    "        optimizer = AdamW(optimizer_grouped_parameters, **kwargs)\n",
    "        return optimizer\n",
    "    else:\n",
    "        raise Exception('Unknown optimizer: {}'.format(optimizer_name))\n",
    "\n",
    "def make_scheduler(optimizer, decay_name='linear', t_max=None, warmup_steps=None):\n",
    "    if decay_name == 'step':\n",
    "        scheduler = optim.lr_scheduler.MultiStepLR(\n",
    "            optimizer,\n",
    "            milestones=[30, 60, 90],\n",
    "            gamma=0.1\n",
    "        )\n",
    "    elif decay_name == 'cosine':\n",
    "        scheduler = lrs.CosineAnnealingLR(\n",
    "            optimizer,\n",
    "            T_max=t_max\n",
    "        )\n",
    "    elif decay_name == \"cosine_warmup\":\n",
    "        scheduler = get_cosine_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=warmup_steps,\n",
    "            num_training_steps=t_max\n",
    "        )\n",
    "    elif decay_name == \"linear\":\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer, \n",
    "            num_warmup_steps=warmup_steps, \n",
    "            num_training_steps=t_max\n",
    "        )\n",
    "    else:\n",
    "        raise Exception('Unknown lr scheduler: {}'.format(decay_type))    \n",
    "    return scheduler    \n",
    "\n",
    "def make_loader(\n",
    "    data, \n",
    "    tokenizer, \n",
    "    max_len,\n",
    "    batch_size,\n",
    "    fold=0\n",
    "):\n",
    "    train_set, valid_set = data[data['kfold']!=fold], data[data['kfold']==fold]\n",
    "    train_dataset = DatasetRetriever(train_set, tokenizer, max_len)\n",
    "    valid_dataset = DatasetRetriever(valid_set, tokenizer, max_len)\n",
    "\n",
    "    train_sampler = RandomSampler(train_dataset)\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size, \n",
    "        sampler=train_sampler, \n",
    "        pin_memory=True, \n",
    "        drop_last=False, \n",
    "        num_workers=4\n",
    "    )\n",
    "\n",
    "    valid_sampler = SequentialSampler(valid_dataset)\n",
    "    valid_loader = DataLoader(\n",
    "        valid_dataset, \n",
    "        batch_size=batch_size // 2, \n",
    "        sampler=valid_sampler, \n",
    "        pin_memory=True, \n",
    "        drop_last=False, \n",
    "        num_workers=4\n",
    "    )\n",
    "\n",
    "    return train_loader, valid_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "600f5da8-264c-4cd8-842c-6996e2e2e025",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "        self.max = 0\n",
    "        self.min = 1e5\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "        if val > self.max:\n",
    "            self.max = val\n",
    "        if val < self.min:\n",
    "            self.min = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "27cf47f8-d436-4a64-a337-1fe9f3c9d60c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, optimizer, scheduler, scalar=None, log_interval=1, evaluate_interval=1):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.scalar = scalar\n",
    "        self.log_interval = log_interval\n",
    "        self.evaluate_interval = evaluate_interval\n",
    "        self.evaluator = Evaluator(self.model, self.scalar)\n",
    "\n",
    "    def train(self, train_loader, valid_loader, epoch, \n",
    "              result_dict, tokenizer, fold):\n",
    "        count = 0\n",
    "        losses = AverageMeter()\n",
    "        self.model.train()\n",
    "        \n",
    "        for batch_idx, batch_data in enumerate(train_loader):\n",
    "            input_ids, attention_mask, token_type_ids, labels = batch_data['input_ids'], \\\n",
    "                batch_data['attention_mask'], batch_data['token_type_ids'], batch_data['label']\n",
    "            input_ids, attention_mask, token_type_ids, labels = \\\n",
    "                input_ids.cuda(), attention_mask.cuda(), token_type_ids.cuda(), labels.cuda()\n",
    "            \n",
    "            if self.scalar is not None:\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    outputs = self.model(\n",
    "                        input_ids=input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        token_type_ids=token_type_ids,\n",
    "                        labels=labels\n",
    "                    )\n",
    "            else:\n",
    "                outputs = self.model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    token_type_ids=token_type_ids,\n",
    "                    labels=labels\n",
    "                )\n",
    "\n",
    "            loss, logits = outputs[:2]\n",
    "            count += labels.size(0)\n",
    "            losses.update(loss.item(), input_ids.size(0))\n",
    "            \n",
    "            if self.scalar is not None:\n",
    "                self.scalar.scale(loss).backward()\n",
    "                self.scalar.step(self.optimizer)\n",
    "                self.scalar.update()\n",
    "            else:\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "            self.scheduler.step()\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            if batch_idx % self.log_interval == 0:\n",
    "                _s = str(len(str(len(train_loader.sampler))))\n",
    "                ret = [\n",
    "                    ('epoch: {:0>3} [{: >' + _s + '}/{} ({: >3.0f}%)]').format(epoch, count, len(train_loader.sampler), 100 * count / len(train_loader.sampler)),\n",
    "                    'train_loss: {: >4.5f}'.format(losses.avg),\n",
    "                ]\n",
    "                print(', '.join(ret))\n",
    "            \n",
    "            if batch_idx % self.evaluate_interval == 0:\n",
    "                result_dict = self.evaluator.evaluate(\n",
    "                    valid_loader, \n",
    "                    epoch, \n",
    "                    result_dict, \n",
    "                    tokenizer\n",
    "                )\n",
    "                if result_dict['val_loss'][-1] < result_dict['best_val_loss']:\n",
    "                    print(\"{} epoch, best epoch was updated! valid_loss: {: >4.5f}\".format(epoch, result_dict['val_loss'][-1]))\n",
    "                    result_dict[\"best_val_loss\"] = result_dict['val_loss'][-1]\n",
    "                    torch.save(self.model.state_dict(), f\"model{fold}.bin\")\n",
    "\n",
    "        result_dict['train_loss'].append(losses.avg)\n",
    "        return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4d4b80dd-b690-45be-b846-9dde693fad25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Evaluator:\n",
    "    def __init__(self, model, scalar=None):\n",
    "        self.model = model\n",
    "        self.scalar = scalar\n",
    "    \n",
    "    def worst_result(self):\n",
    "        ret = {\n",
    "            'loss':float('inf'),\n",
    "            'accuracy':0.0\n",
    "        }\n",
    "        return ret\n",
    "\n",
    "    def result_to_str(self, result):\n",
    "        ret = [\n",
    "            'epoch: {epoch:0>3}',\n",
    "            'loss: {loss: >4.2e}'\n",
    "        ]\n",
    "        for metric in self.evaluation_metrics:\n",
    "            ret.append('{}: {}'.format(metric.name, metric.fmtstr))\n",
    "        return ', '.join(ret).format(**result)\n",
    "\n",
    "    def save(self, result):\n",
    "        with open('result_dict.json', 'w') as f:\n",
    "            f.write(json.dumps(result, sort_keys=True, indent=4, ensure_ascii=False))\n",
    "    \n",
    "    def load(self):\n",
    "        result = self.worst_result\n",
    "        if os.path.exists('result_dict.json'):\n",
    "            with open('result_dict.json', 'r') as f:\n",
    "                try:\n",
    "                    result = json.loads(f.read())\n",
    "                except:\n",
    "                    pass\n",
    "        return result\n",
    "\n",
    "    def evaluate(self, data_loader, epoch, result_dict, tokenizer):\n",
    "        losses = AverageMeter()\n",
    "\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, batch_data in enumerate(data_loader):\n",
    "                input_ids, attention_mask, token_type_ids, labels = batch_data['input_ids'], \\\n",
    "                    batch_data['attention_mask'], batch_data['token_type_ids'], batch_data['label']\n",
    "                input_ids, attention_mask, token_type_ids, labels = input_ids.cuda(), \\\n",
    "                    attention_mask.cuda(), token_type_ids.cuda(), labels.cuda()\n",
    "                \n",
    "                if self.scalar is not None:\n",
    "                    with torch.cuda.amp.autocast():\n",
    "                        outputs = self.model(\n",
    "                            input_ids=input_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                            token_type_ids=token_type_ids,\n",
    "                            labels=labels\n",
    "                        )\n",
    "                else:\n",
    "                    outputs = self.model(\n",
    "                        input_ids=input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        token_type_ids=token_type_ids,\n",
    "                        labels=labels\n",
    "                    )\n",
    "                \n",
    "                loss, logits = outputs[:2]\n",
    "                losses.update(loss.item(), input_ids.size(0))\n",
    "\n",
    "        print('----Validation Results Summary----')\n",
    "        print('Epoch: [{}] valid_loss: {: >4.5f}'.format(epoch, losses.avg))\n",
    "\n",
    "        result_dict['val_loss'].append(losses.avg)        \n",
    "        return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e081c665-4cb3-4462-9d94-1ce279091f08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def config(fold=0):\n",
    "    torch.manual_seed(2021)\n",
    "    torch.cuda.manual_seed(2021)\n",
    "    torch.cuda.manual_seed_all(2021)\n",
    "    epochs = 8\n",
    "    max_len = 250\n",
    "    batch_size = 16\n",
    "\n",
    "    model, tokenizer = make_model(model_name='./output/', num_labels=1)\n",
    "    train_loader, valid_loader = make_loader(\n",
    "        train, tokenizer, max_len=max_len,\n",
    "        batch_size=batch_size, fold=fold\n",
    "    )\n",
    "\n",
    "    import math\n",
    "    num_update_steps_per_epoch = len(train_loader)\n",
    "    max_train_steps = epochs * num_update_steps_per_epoch\n",
    "    warmup_proportion = 0\n",
    "    if warmup_proportion != 0:\n",
    "        warmup_steps = math.ceil((max_train_steps * 2) / 100)\n",
    "    else:\n",
    "        warmup_steps = 0\n",
    "\n",
    "    optimizer = make_optimizer(model, \"AdamW\")\n",
    "    scheduler = make_scheduler(\n",
    "        optimizer, decay_name='cosine_warmup', \n",
    "        t_max=max_train_steps, \n",
    "        warmup_steps=warmup_steps\n",
    "    )    \n",
    "\n",
    "    if torch.cuda.device_count() >= 1:\n",
    "        print('Model pushed to {} GPU(s), type {}.'.format(\n",
    "            torch.cuda.device_count(), \n",
    "            torch.cuda.get_device_name(0))\n",
    "        )\n",
    "        model = model.cuda() \n",
    "    else:\n",
    "        raise ValueError('CPU training is not supported')\n",
    "\n",
    "    # scaler = torch.cuda.amp.GradScaler()\n",
    "    scaler = None\n",
    "\n",
    "    result_dict = {\n",
    "        'epoch':[], \n",
    "        'train_loss': [], \n",
    "        'val_loss' : [], \n",
    "        'best_val_loss': np.inf\n",
    "    }\n",
    "    return (\n",
    "        model, tokenizer, \n",
    "        optimizer, scheduler, \n",
    "        scaler, train_loader, \n",
    "        valid_loader, result_dict, \n",
    "        epochs\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "98128bde-254d-45b0-8874-7aef7376b770",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run(fold=0):\n",
    "    model, tokenizer, optimizer, scheduler, scaler, \\\n",
    "        train_loader, valid_loader, result_dict, epochs = config(fold)\n",
    "    \n",
    "    import time\n",
    "    trainer = Trainer(model, optimizer, scheduler, scaler)\n",
    "    train_time_list = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        result_dict['epoch'] = epoch\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        tic1 = time.time()\n",
    "\n",
    "        result_dict = trainer.train(train_loader, valid_loader, epoch, \n",
    "                                    result_dict, tokenizer, fold)\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        tic2 = time.time() \n",
    "        train_time_list.append(tic2 - tic1)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    del model, tokenizer, optimizer, scheduler, \\\n",
    "        scaler, train_loader, valid_loader,\n",
    "    gc.collect()\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2297e1-0e51-4e7d-a107-18b5c58e3603",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "FOLD: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./output/ were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at ./output/ and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model pushed to 2 GPU(s), type NVIDIA GeForce RTX 3090.\n",
      "epoch: 000 [    16/244490 (  0%)], train_loss: 8.42921\n",
      "----Validation Results Summary----\n",
      "Epoch: [0] valid_loss: 4.61140\n",
      "0 epoch, best epoch was updated! valid_loss: 4.61140\n",
      "epoch: 000 [    32/244490 (  0%)], train_loss: 6.42651\n",
      "----Validation Results Summary----\n",
      "Epoch: [0] valid_loss: 2.55178\n",
      "0 epoch, best epoch was updated! valid_loss: 2.55178\n",
      "epoch: 000 [    48/244490 (  0%)], train_loss: 5.30979\n",
      "----Validation Results Summary----\n",
      "Epoch: [0] valid_loss: 2.56805\n",
      "epoch: 000 [    64/244490 (  0%)], train_loss: 4.72330\n",
      "----Validation Results Summary----\n",
      "Epoch: [0] valid_loss: 2.37287\n",
      "0 epoch, best epoch was updated! valid_loss: 2.37287\n",
      "epoch: 000 [    80/244490 (  0%)], train_loss: 4.36144\n",
      "----Validation Results Summary----\n",
      "Epoch: [0] valid_loss: 2.30339\n",
      "0 epoch, best epoch was updated! valid_loss: 2.30339\n",
      "epoch: 000 [    96/244490 (  0%)], train_loss: 4.01909\n",
      "----Validation Results Summary----\n",
      "Epoch: [0] valid_loss: 2.46685\n",
      "epoch: 000 [   112/244490 (  0%)], train_loss: 4.05687\n",
      "----Validation Results Summary----\n",
      "Epoch: [0] valid_loss: 2.46049\n",
      "epoch: 000 [   128/244490 (  0%)], train_loss: 3.84158\n",
      "----Validation Results Summary----\n",
      "Epoch: [0] valid_loss: 2.39433\n",
      "epoch: 000 [   144/244490 (  0%)], train_loss: 3.62787\n",
      "----Validation Results Summary----\n",
      "Epoch: [0] valid_loss: 2.25673\n",
      "0 epoch, best epoch was updated! valid_loss: 2.25673\n",
      "epoch: 000 [   160/244490 (  0%)], train_loss: 3.48748\n",
      "----Validation Results Summary----\n",
      "Epoch: [0] valid_loss: 2.26434\n",
      "epoch: 000 [   176/244490 (  0%)], train_loss: 3.31417\n",
      "----Validation Results Summary----\n",
      "Epoch: [0] valid_loss: 2.26186\n",
      "epoch: 000 [   192/244490 (  0%)], train_loss: 3.24987\n",
      "----Validation Results Summary----\n",
      "Epoch: [0] valid_loss: 2.21307\n",
      "0 epoch, best epoch was updated! valid_loss: 2.21307\n",
      "epoch: 000 [   208/244490 (  0%)], train_loss: 3.12934\n",
      "----Validation Results Summary----\n",
      "Epoch: [0] valid_loss: 2.29361\n",
      "epoch: 000 [   224/244490 (  0%)], train_loss: 3.06689\n",
      "----Validation Results Summary----\n",
      "Epoch: [0] valid_loss: 2.33347\n",
      "epoch: 000 [   240/244490 (  0%)], train_loss: 2.95162\n",
      "----Validation Results Summary----\n",
      "Epoch: [0] valid_loss: 2.22003\n",
      "epoch: 000 [   256/244490 (  0%)], train_loss: 2.88257\n",
      "----Validation Results Summary----\n",
      "Epoch: [0] valid_loss: 2.18202\n",
      "0 epoch, best epoch was updated! valid_loss: 2.18202\n",
      "epoch: 000 [   272/244490 (  0%)], train_loss: 2.87652\n",
      "----Validation Results Summary----\n",
      "Epoch: [0] valid_loss: 2.23872\n",
      "epoch: 000 [   288/244490 (  0%)], train_loss: 2.87436\n",
      "----Validation Results Summary----\n",
      "Epoch: [0] valid_loss: 2.28259\n",
      "epoch: 000 [   304/244490 (  0%)], train_loss: 2.85126\n",
      "----Validation Results Summary----\n",
      "Epoch: [0] valid_loss: 2.21304\n",
      "epoch: 000 [   320/244490 (  0%)], train_loss: 2.84295\n",
      "----Validation Results Summary----\n",
      "Epoch: [0] valid_loss: 2.16285\n",
      "0 epoch, best epoch was updated! valid_loss: 2.16285\n",
      "epoch: 000 [   336/244490 (  0%)], train_loss: 2.79118\n",
      "----Validation Results Summary----\n",
      "Epoch: [0] valid_loss: 2.13430\n",
      "0 epoch, best epoch was updated! valid_loss: 2.13430\n",
      "epoch: 000 [   352/244490 (  0%)], train_loss: 2.79572\n",
      "----Validation Results Summary----\n",
      "Epoch: [0] valid_loss: 2.14223\n",
      "epoch: 000 [   368/244490 (  0%)], train_loss: 2.74565\n",
      "----Validation Results Summary----\n",
      "Epoch: [0] valid_loss: 2.43180\n",
      "epoch: 000 [   384/244490 (  0%)], train_loss: 2.73069\n",
      "----Validation Results Summary----\n",
      "Epoch: [0] valid_loss: 2.92618\n",
      "epoch: 000 [   400/244490 (  0%)], train_loss: 2.72408\n",
      "----Validation Results Summary----\n",
      "Epoch: [0] valid_loss: 2.44832\n",
      "epoch: 000 [   416/244490 (  0%)], train_loss: 2.74523\n",
      "----Validation Results Summary----\n",
      "Epoch: [0] valid_loss: 2.11344\n",
      "0 epoch, best epoch was updated! valid_loss: 2.11344\n",
      "epoch: 000 [   432/244490 (  0%)], train_loss: 2.71909\n",
      "----Validation Results Summary----\n",
      "Epoch: [0] valid_loss: 2.13006\n",
      "epoch: 000 [   448/244490 (  0%)], train_loss: 2.71095\n",
      "----Validation Results Summary----\n",
      "Epoch: [0] valid_loss: 2.25231\n",
      "epoch: 000 [   464/244490 (  0%)], train_loss: 2.69913\n",
      "----Validation Results Summary----\n",
      "Epoch: [0] valid_loss: 2.30541\n",
      "epoch: 000 [   480/244490 (  0%)], train_loss: 2.68163\n",
      "----Validation Results Summary----\n",
      "Epoch: [0] valid_loss: 2.29017\n",
      "epoch: 000 [   496/244490 (  0%)], train_loss: 2.66629\n",
      "----Validation Results Summary----\n",
      "Epoch: [0] valid_loss: 2.26416\n",
      "epoch: 000 [   512/244490 (  0%)], train_loss: 2.65833\n",
      "----Validation Results Summary----\n",
      "Epoch: [0] valid_loss: 2.13513\n",
      "epoch: 000 [   528/244490 (  0%)], train_loss: 2.64595\n",
      "----Validation Results Summary----\n",
      "Epoch: [0] valid_loss: 2.07075\n",
      "0 epoch, best epoch was updated! valid_loss: 2.07075\n",
      "epoch: 000 [   544/244490 (  0%)], train_loss: 2.61829\n",
      "----Validation Results Summary----\n",
      "Epoch: [0] valid_loss: 2.34249\n",
      "epoch: 000 [   560/244490 (  0%)], train_loss: 2.61204\n",
      "----Validation Results Summary----\n",
      "Epoch: [0] valid_loss: 2.47706\n",
      "epoch: 000 [   576/244490 (  0%)], train_loss: 2.63766\n",
      "----Validation Results Summary----\n",
      "Epoch: [0] valid_loss: 2.45008\n",
      "epoch: 000 [   592/244490 (  0%)], train_loss: 2.65348\n",
      "----Validation Results Summary----\n",
      "Epoch: [0] valid_loss: 2.24712\n",
      "epoch: 000 [   608/244490 (  0%)], train_loss: 2.64292\n",
      "----Validation Results Summary----\n",
      "Epoch: [0] valid_loss: 2.11271\n",
      "epoch: 000 [   624/244490 (  0%)], train_loss: 2.61346\n",
      "----Validation Results Summary----\n",
      "Epoch: [0] valid_loss: 2.07114\n",
      "epoch: 000 [   640/244490 (  0%)], train_loss: 2.60501\n",
      "----Validation Results Summary----\n",
      "Epoch: [0] valid_loss: 2.11915\n",
      "epoch: 000 [   656/244490 (  0%)], train_loss: 2.58408\n",
      "----Validation Results Summary----\n",
      "Epoch: [0] valid_loss: 2.14417\n",
      "epoch: 000 [   672/244490 (  0%)], train_loss: 2.58986\n",
      "----Validation Results Summary----\n",
      "Epoch: [0] valid_loss: 2.17321\n",
      "epoch: 000 [   688/244490 (  0%)], train_loss: 2.58048\n",
      "----Validation Results Summary----\n",
      "Epoch: [0] valid_loss: 2.19686\n",
      "epoch: 000 [   704/244490 (  0%)], train_loss: 2.57966\n",
      "----Validation Results Summary----\n",
      "Epoch: [0] valid_loss: 2.21046\n",
      "epoch: 000 [   720/244490 (  0%)], train_loss: 2.56606\n",
      "----Validation Results Summary----\n",
      "Epoch: [0] valid_loss: 2.21021\n",
      "epoch: 000 [   736/244490 (  0%)], train_loss: 2.54812\n",
      "----Validation Results Summary----\n",
      "Epoch: [0] valid_loss: 2.22549\n",
      "epoch: 000 [   752/244490 (  0%)], train_loss: 2.54874\n",
      "----Validation Results Summary----\n",
      "Epoch: [0] valid_loss: 2.22724\n",
      "epoch: 000 [   768/244490 (  0%)], train_loss: 2.54238\n",
      "----Validation Results Summary----\n",
      "Epoch: [0] valid_loss: 2.14003\n",
      "epoch: 000 [   784/244490 (  0%)], train_loss: 2.53932\n",
      "----Validation Results Summary----\n",
      "Epoch: [0] valid_loss: 2.08909\n",
      "epoch: 000 [   800/244490 (  0%)], train_loss: 2.54325\n",
      "----Validation Results Summary----\n",
      "Epoch: [0] valid_loss: 2.08054\n",
      "epoch: 000 [   816/244490 (  0%)], train_loss: 2.54034\n"
     ]
    }
   ],
   "source": [
    "result_list = []\n",
    "for fold in range(5):\n",
    "    print('----')\n",
    "    print(f'FOLD: {fold}')\n",
    "    result_dict = run(fold)\n",
    "    result_list.append(result_dict)\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807191e8-b639-4c95-9391-da6003fea3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "[print(\"FOLD::\", i, \"Loss:: \", fold['best_val_loss']) for i, fold in enumerate(result_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac5645a-53f2-44ba-b616-8145404969c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "oof = np.zeros(len(train))\n",
    "for fold in tqdm(range(5), total=5):\n",
    "    model, tokenizer = make_model()\n",
    "    model.load_state_dict(\n",
    "        torch.load(f'model{fold}.bin')\n",
    "    )\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    val_index = train[train.kfold==fold].index.tolist()\n",
    "    train_loader, val_loader = make_loader(train, tokenizer, 250, 16, fold=fold)\n",
    "    # scalar = torch.cuda.amp.GradScaler()\n",
    "    scalar = None\n",
    "    preds = []\n",
    "    for index, data in enumerate(val_loader):\n",
    "        input_ids, attention_mask, token_type_ids, labels = data['input_ids'], \\\n",
    "            data['attention_mask'], data['token_type_ids'], data['label']\n",
    "        input_ids, attention_mask, token_type_ids, labels = input_ids.cuda(), \\\n",
    "            attention_mask.cuda(), token_type_ids.cuda(), labels.cuda()\n",
    "        if scalar is not None:\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    token_type_ids=token_type_ids,\n",
    "                    labels=labels\n",
    "                )\n",
    "        else:\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids,\n",
    "                labels=labels\n",
    "            )\n",
    "        \n",
    "        loss, logits = outputs[:2]\n",
    "        preds += logits.cpu().detach().numpy().tolist()\n",
    "    oof[val_index] = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c7f691-f6e4-48f9-a2f1-922b891567f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "round(np.sqrt(mean_squared_error(train.target.values, oof)), 4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
